[
  {
    "objectID": "howto/index.html",
    "href": "howto/index.html",
    "title": "How-to",
    "section": "",
    "text": "How-to\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LaBB-CAT Documentation",
    "section": "",
    "text": "LaBB-CAT Documentation\nThis is the documentation for the LaBB-CAT corpus management system.\nLaBB-CAT is a browser-based linguistics research tool that stores audio or video recordings, text transcripts, and other annotations.\nAnnotations of various types can be automatically generated or manually added.\nThe transcripts and annotations can be searched for particular text or regular expressions. The search results, or entire transcripts, can be viewed or saved in a variety of formats, and the related parts of the recordings can be played or opened in acoustic analysis software, all directly through the web-browser.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/demo/3-celex.html",
    "href": "worksheets/demo/3-celex.html",
    "title": "3 - CELEX",
    "section": "",
    "text": "In some circumstances it can be useful to group together different forms of the same word; e.g. treat “damage”, “damaged” and “damaging” as variants of the same thing for the purposes of frequency-counting and other analyses.\nThe demo database has been configured to tag each token with it’s root form or ‘Lemma’. To do this, LaBB-CAT has been integrated with the CELEX lexicon, which can be purchased from the Linguistic Data Consortium (LDC) and includes lemma, part of speech, morphological,  phonological, and frequency information for English, German, and Dutch.\nThere is a lemma layer configured, which looks up each word token in the CELEX lexicon, and tags it with its lemma.\nFor words that are missing from CELEX, LaBB-CAT is configured to instead tag the word with it’s ‘stem’ according to the Porter Algorithm (Porter, 1980, An algorithm for suffix stripping, Program, Vol. 14, no. 3, pp 130-137, or http://www.tartarus.org/~martin/PorterStemmer).\n\nIn the transcript you have open, under the “projects” heading, tick the celex project.\nA number of extra layer options will appear in the layer list.\nTick the lemma layer.\nWhen the transcript re-loads, you’ll see that each word is tagged with its lemma; in some cases the lemma is the same as the word-form, and in other cases, the lemma has suffixes stripped off, etc.\nSearch for the word “damage” on the page (in most browsers, Ctrl + F or some similar keyboard combination allows you to search for text on the current page).\nYou should see that variants like “damage”, “damaged” and “damaging” are all tagged with the same lemma: “damage”.\nNow tick the frequency project.\nThree layer options appear; word frequency, lemma frequency, and liwc. You have already seen (and searched) the word frequency layer.\nThe lemma frequency layer is similarly generated by the Frequency Layer Manager, but instead of counting up raw word forms from the orthography layer, it counts based on the lemma layer.\nTick both word frequency and lemma frequency\nFind the word “damaging” in the text.\nYou’ll see that, although the word-form is very low frequency, the lemma frequency is somewhat higher (as you’d expect in a corpus of earthquake stories!).\n\nThe Frequency Layer Manager also keeps a straight word-list with word counts for each corpus…\n\nClick the home menu option at the top.\nClick the Frequency Layer Manager icon.\nYou will see a drop-down box with each frequency layer in it.\nSelect Lemma Frequency and press Select.\nPress the Export button at the bottom.\nSave and open the resulting CSV file.\nYou will see an alphabetical list of all the distinct lemmas in the database, and next to each, a count of the number of tokens of that type.\n\n\n\n\nThere is other information in CELEX that can be used to tag words. Let’s say you’re interested in the morphological suffix ~ing. If we search for .*ing on the orthoghraphy layer, we’ll get a number of false positives ...\n\nSelect the search menu option.\nSearch for .*ing on the orthography layer.\nYou will see that the results include words like like “thing” and “everything” whose “ing” is part of the base word, not a morphological affix.\nLeave the results tab open, so you can compare these results with the next search …\nSwap back to the search matrix page, tick the ‘celex’ project, and add the morphology layer to the search matrix.\nNow do a search on the morphology layer of words ending in +ing, and compare the results with the orthography-based search.\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that in regular expressions the ‘plus’ character + has a special meaning - it means “one or more of the previous thing”. But we are now searching for actual literal + characters.\nIn order to search for a literal “+” in the annotation, you have to ‘escape’ the +. Consult the regular expressions help page to figure out how to do that.\n\n\nThe results should now contain only words for which the ~ing is a morphological suffix.\n\n\n\nThe CELEX lexicon includes phonological information, so we can tag each word with its phonemic transcription, and view/search the pronunciations of words.\n\nClick the transcripts link on the menu.\nClick the name of the first transcript listed, to display the transcript text.\nTick the celex project.\nTick the phonemes layer.\n\nYou will see that each word is tagged with its phonemic transcription using the International Phonetic Alphabet (IPA). However, CELEX doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme.\nThe IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ‘ASCII’ option on the layer in the transcript.\n\nSelect ‘ASCII’ on the phonemes layer, to see what CELEX is actually producing.\nYou may find that this is somewhat harder to read. Diphthongs are generally represented by digits, schwa is “@”, and various other characters are used to represent affricates, etc.\n\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in the table below), because they are what we have to use when searching on the phonemes layer, which we are going to try now.\n\n\n\nIPA\nDISC\n \n \nIPA\nDISC\n \n\n\np\np\npat\n \nɪ\nI\nKIT\n\n\nb\nb\nbad\n \nε\nE\nDRESS\n\n\nt\nt\ntack\n \næ\n{\nTRAP\n\n\nd\nd\ndad\n \nʌ\nV\nSTRUT\n\n\nk\nk\ncad\n \nɒ\nQ\nLOT\n\n\ng\ng\ngame\n \nʊ\nU\nFOOT\n\n\nŋ\nN\nbang\n \nə\n@\nanother\n\n\nm\nm\nmat\n \ni:\ni\nFLEECE\n\n\nn\nn\nnat\n \nα: \n#\nfather\n\n\nl\nl\nlad\n \nɔ:\n$\nTHOUGHT\n\n\nr\nr\nrat\n \nu:\nu\nGOOSE\n\n\nf\nf\nfat\n \nɜ:\n3\nNURSE\n\n\nv\nv\nvat\n \neɪ\n1\nFACE\n\n\nθ\nT\nthin\n \nαɪ\n2\nPRICE\n\n\nð\nD\nthen\n \nɔɪ\n4\nCHOICE\n\n\ns\ns\nsap\n \nəʊ\n5\nGOAT\n\n\nz\nz\nzap\n \nαʊ\n6\nMOUTH\n\n\n∫\nS\nsheep\n \nɪə\n7\nNEAR\n\n\nʒ\nZ\nmeasure\n \nεə\n8\nSQUARE\n\n\nj\nj\nyank\n \nʊə\n9\nCURE\n\n\nx\nx\nloch\n \næ\nc\ntimbre\n\n\nh\nh\nhad\n \nɑ̃ː\nq\ndétente\n\n\nw\nw\nwet\n \næ̃ː\n0\nlingerie\n\n\nʧ\nJ\ncheap\n \nɒ̃ː\n~\nbouillon\n\n\nʤ\n_\njeep\n \n \n \n \n\n\nŋ̩\nC\nbacon\n \n \n \n \n\n\nm̩\nF\nidealism\n \n \n \n \n\n\nn̩\nH\nburden\n \n \n \n \n\n\nl̩\nP\ndangle\n \n \n \n \n\n\n\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\n\nNow we’re going to do a search for the word “the” followed by a word that starts with schwa.\n\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little « button to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it.\nFind the schwa symbol ə and click it.\nYou will see that a @* symbol appears in the box.\n@* is the DISC symbol for ə, so in order to search for schwa, we have to use it in our search pattern.\nWe want words that start with schwa, so type .* after the @ symbol.\nClick Search.\nYou will see that some of the words being matched are words that you might not normally think start with a schwa. LaBB-CAT is matching words against all their possible phonemic transcriptions, so if CELEX has multiple possible pronunciations for a word, and one of them starts with schwa, it will be matched.\n\nWith the phonemic transcriptions, we can do a better job of the search we tried in an earlier exercise - “the” followed by a word starting with a vowel…\n\nChange your search so that, instead of just @ at the beginning of the word, it matches any vowel.\n\n\n\n\n\n\n\nNote\n\n\n\nYou could use the square-brackets [] at the start of your pattern, and type all vowel symbols inside them - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the Phoneme Symbol Selector, including all the diphthongs.\nAlternatively, you can simply click the VOWEL link in the Phoneme Symbol Selector, which will add all the DISC vowels for you, already enclosed in square-brackets.\n\n\n\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve seen a few different layers, and how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nWords which have the vowel in DRESS as the second phoneme\nThe word “the” followed by a word beginning with the phoneme /k/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/\n\n\nIn this worksheet you have seen that:\n\nThe CELEX Layer Manager tags words with information from the CELEX lexicon.\nPhonemic transcription layers can be used to search on the basis of pronunciation.\nAlthough pronunciations can be displayed with IPA symbols, CELEX uses DISC to encode phonemes, so DISC must be used for searches."
  },
  {
    "objectID": "worksheets/demo/3-celex.html#lemma",
    "href": "worksheets/demo/3-celex.html#lemma",
    "title": "3 - CELEX",
    "section": "",
    "text": "In some circumstances it can be useful to group together different forms of the same word; e.g. treat “damage”, “damaged” and “damaging” as variants of the same thing for the purposes of frequency-counting and other analyses.\nThe demo database has been configured to tag each token with it’s root form or ‘Lemma’. To do this, LaBB-CAT has been integrated with the CELEX lexicon, which can be purchased from the Linguistic Data Consortium (LDC) and includes lemma, part of speech, morphological,  phonological, and frequency information for English, German, and Dutch.\nThere is a lemma layer configured, which looks up each word token in the CELEX lexicon, and tags it with its lemma.\nFor words that are missing from CELEX, LaBB-CAT is configured to instead tag the word with it’s ‘stem’ according to the Porter Algorithm (Porter, 1980, An algorithm for suffix stripping, Program, Vol. 14, no. 3, pp 130-137, or http://www.tartarus.org/~martin/PorterStemmer).\n\nIn the transcript you have open, under the “projects” heading, tick the celex project.\nA number of extra layer options will appear in the layer list.\nTick the lemma layer.\nWhen the transcript re-loads, you’ll see that each word is tagged with its lemma; in some cases the lemma is the same as the word-form, and in other cases, the lemma has suffixes stripped off, etc.\nSearch for the word “damage” on the page (in most browsers, Ctrl + F or some similar keyboard combination allows you to search for text on the current page).\nYou should see that variants like “damage”, “damaged” and “damaging” are all tagged with the same lemma: “damage”.\nNow tick the frequency project.\nThree layer options appear; word frequency, lemma frequency, and liwc. You have already seen (and searched) the word frequency layer.\nThe lemma frequency layer is similarly generated by the Frequency Layer Manager, but instead of counting up raw word forms from the orthography layer, it counts based on the lemma layer.\nTick both word frequency and lemma frequency\nFind the word “damaging” in the text.\nYou’ll see that, although the word-form is very low frequency, the lemma frequency is somewhat higher (as you’d expect in a corpus of earthquake stories!).\n\nThe Frequency Layer Manager also keeps a straight word-list with word counts for each corpus…\n\nClick the home menu option at the top.\nClick the Frequency Layer Manager icon.\nYou will see a drop-down box with each frequency layer in it.\nSelect Lemma Frequency and press Select.\nPress the Export button at the bottom.\nSave and open the resulting CSV file.\nYou will see an alphabetical list of all the distinct lemmas in the database, and next to each, a count of the number of tokens of that type."
  },
  {
    "objectID": "worksheets/demo/3-celex.html#morphology",
    "href": "worksheets/demo/3-celex.html#morphology",
    "title": "3 - CELEX",
    "section": "",
    "text": "There is other information in CELEX that can be used to tag words. Let’s say you’re interested in the morphological suffix ~ing. If we search for .*ing on the orthoghraphy layer, we’ll get a number of false positives ...\n\nSelect the search menu option.\nSearch for .*ing on the orthography layer.\nYou will see that the results include words like like “thing” and “everything” whose “ing” is part of the base word, not a morphological affix.\nLeave the results tab open, so you can compare these results with the next search …\nSwap back to the search matrix page, tick the ‘celex’ project, and add the morphology layer to the search matrix.\nNow do a search on the morphology layer of words ending in +ing, and compare the results with the orthography-based search.\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that in regular expressions the ‘plus’ character + has a special meaning - it means “one or more of the previous thing”. But we are now searching for actual literal + characters.\nIn order to search for a literal “+” in the annotation, you have to ‘escape’ the +. Consult the regular expressions help page to figure out how to do that.\n\n\nThe results should now contain only words for which the ~ing is a morphological suffix."
  },
  {
    "objectID": "worksheets/demo/3-celex.html#phonology",
    "href": "worksheets/demo/3-celex.html#phonology",
    "title": "3 - CELEX",
    "section": "",
    "text": "The CELEX lexicon includes phonological information, so we can tag each word with its phonemic transcription, and view/search the pronunciations of words.\n\nClick the transcripts link on the menu.\nClick the name of the first transcript listed, to display the transcript text.\nTick the celex project.\nTick the phonemes layer.\n\nYou will see that each word is tagged with its phonemic transcription using the International Phonetic Alphabet (IPA). However, CELEX doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme.\nThe IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ‘ASCII’ option on the layer in the transcript.\n\nSelect ‘ASCII’ on the phonemes layer, to see what CELEX is actually producing.\nYou may find that this is somewhat harder to read. Diphthongs are generally represented by digits, schwa is “@”, and various other characters are used to represent affricates, etc.\n\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in the table below), because they are what we have to use when searching on the phonemes layer, which we are going to try now.\n\n\n\nIPA\nDISC\n \n \nIPA\nDISC\n \n\n\np\np\npat\n \nɪ\nI\nKIT\n\n\nb\nb\nbad\n \nε\nE\nDRESS\n\n\nt\nt\ntack\n \næ\n{\nTRAP\n\n\nd\nd\ndad\n \nʌ\nV\nSTRUT\n\n\nk\nk\ncad\n \nɒ\nQ\nLOT\n\n\ng\ng\ngame\n \nʊ\nU\nFOOT\n\n\nŋ\nN\nbang\n \nə\n@\nanother\n\n\nm\nm\nmat\n \ni:\ni\nFLEECE\n\n\nn\nn\nnat\n \nα: \n#\nfather\n\n\nl\nl\nlad\n \nɔ:\n$\nTHOUGHT\n\n\nr\nr\nrat\n \nu:\nu\nGOOSE\n\n\nf\nf\nfat\n \nɜ:\n3\nNURSE\n\n\nv\nv\nvat\n \neɪ\n1\nFACE\n\n\nθ\nT\nthin\n \nαɪ\n2\nPRICE\n\n\nð\nD\nthen\n \nɔɪ\n4\nCHOICE\n\n\ns\ns\nsap\n \nəʊ\n5\nGOAT\n\n\nz\nz\nzap\n \nαʊ\n6\nMOUTH\n\n\n∫\nS\nsheep\n \nɪə\n7\nNEAR\n\n\nʒ\nZ\nmeasure\n \nεə\n8\nSQUARE\n\n\nj\nj\nyank\n \nʊə\n9\nCURE\n\n\nx\nx\nloch\n \næ\nc\ntimbre\n\n\nh\nh\nhad\n \nɑ̃ː\nq\ndétente\n\n\nw\nw\nwet\n \næ̃ː\n0\nlingerie\n\n\nʧ\nJ\ncheap\n \nɒ̃ː\n~\nbouillon\n\n\nʤ\n_\njeep\n \n \n \n \n\n\nŋ̩\nC\nbacon\n \n \n \n \n\n\nm̩\nF\nidealism\n \n \n \n \n\n\nn̩\nH\nburden\n \n \n \n \n\n\nl̩\nP\ndangle\n \n \n \n \n\n\n\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\n\nNow we’re going to do a search for the word “the” followed by a word that starts with schwa.\n\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little « button to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it.\nFind the schwa symbol ə and click it.\nYou will see that a @* symbol appears in the box.\n@* is the DISC symbol for ə, so in order to search for schwa, we have to use it in our search pattern.\nWe want words that start with schwa, so type .* after the @ symbol.\nClick Search.\nYou will see that some of the words being matched are words that you might not normally think start with a schwa. LaBB-CAT is matching words against all their possible phonemic transcriptions, so if CELEX has multiple possible pronunciations for a word, and one of them starts with schwa, it will be matched.\n\nWith the phonemic transcriptions, we can do a better job of the search we tried in an earlier exercise - “the” followed by a word starting with a vowel…\n\nChange your search so that, instead of just @ at the beginning of the word, it matches any vowel.\n\n\n\n\n\n\n\nNote\n\n\n\nYou could use the square-brackets [] at the start of your pattern, and type all vowel symbols inside them - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the Phoneme Symbol Selector, including all the diphthongs.\nAlternatively, you can simply click the VOWEL link in the Phoneme Symbol Selector, which will add all the DISC vowels for you, already enclosed in square-brackets.\n\n\n\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve seen a few different layers, and how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nWords which have the vowel in DRESS as the second phoneme\nThe word “the” followed by a word beginning with the phoneme /k/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/\n\n\nIn this worksheet you have seen that:\n\nThe CELEX Layer Manager tags words with information from the CELEX lexicon.\nPhonemic transcription layers can be used to search on the basis of pronunciation.\nAlthough pronunciations can be displayed with IPA symbols, CELEX uses DISC to encode phonemes, so DISC must be used for searches."
  },
  {
    "objectID": "worksheets/demo/1-exploration.html",
    "href": "worksheets/demo/1-exploration.html",
    "title": "1 - Exploration",
    "section": "",
    "text": "LaBB-CAT is a speech/language corpus management system that:\n\nstores transcripts with audio/video\n\nsupporting a variety of formats\nand the definition of speech elicitation tasks;\n\nallows the addition of different layers of annotation, which can\n\nbe manual or automatic, and\nhave different granularities, from topic tagging to individual phones;\n\nsupports forced alignment to phone level using a speech recognition toolkit called “HTK”, or the Montreal Forced Aligner, or the WebMAUS service provided by BAS Web Services;\nallows cross-layer regular-expression search;\nsearch results are exportable to CSV for further analysis;\nbatch acoustic measurement of segments using Praat is also supported, and\ntranscripts and fragments of them are exportable in a variety of formats.\n\nIn this worksheet you will start exploring a demo LaBB-CAT corpus, to get a general idea of how to find your way around LaBB-CAT and how the language data is presented.\nThe demo corpus contains a collection of videos of people telling stories about their experiences during the earthquakes that struck Canterbury during 2010 and 2011. They have been orthographically transcribed using a tool called ELAN, so they have been time aligned to the utterance level; i.e. the start and end time of each line in the transcript has been manually synchronized with the recording. The ELAN transcripts, and their video and audio files, have been uploaded into LaBB-CAT.\nLaBB-CAT is a browser-based system so the first thing to do is access it with your web browser. Generally, any modern browser should be fine (although some features you’ll see in later worksheets are only supported by Mozilla Firefox or Google Chrome).\n\nIn your web browser, type in the following URL:\nhttps://labbcat.canterbury.ac.nz/demo\nYou will be asked for a username and password.\n\n\n\n\n\n\n\nImportant\n\n\n\nIf typing this out manually, ensure you enter ‘https’ not ‘http’\n\n\n\nThe username is demo and the password is demo\nThe very first time you access LaBB-CAT, you will see its licence agreement.\nScroll to the bottom of the page and click I Agree to continue.\nYou will see a page called “LaBB-CAT Demo” which has a menu of links along the top and a number of icons. Below the icons is some information about the corpus. This is the LaBB-CAT home page.\nClick the where do I start? icon on the left.\nThe help page that pops up includes a brief description of LaBB-CAT and some tips for navigation and getting more information.\nRead through the page, and then close the browser tab to return to the home page.\n\nThere are two main ways to use LaBB-CAT:\n\neasy exploration and plain text search (this worksheet)\nlayered filtering and search (the following worksheets)\n\n\n\nUsing the ‘easy’ method for exploring LaBB-CAT is simple, as annotation layers are largely ignored; each transcript is essentially treated as a plain text that you can search and display based on ordinary orthographic spelling.\n\nOn the LaBB-CAT home page, click the explore icon to access the easy exploration pages.\nYou will see a similar home page, with Browse icon, Easy Search, and Layered Search icons.\nClick the Browse icon.\nYou will see a page that lists the collections of recordings (or ‘corpora’) in the LaBB-CAT database. Each corpus contains a number of recordings.\nClick the first corpus listed.\nYou will see a page that lists the first 20 recordings in the corpus. The recording names are on the left, followed by some meta data (called ‘participant attributes’) about the participant in the recording. At the bottom is a list of pages, so you can access further recordings in the corpus.\nClick the name of the first recording.\nYou will see a page with transcript text, and the video appears in the top right corner of the page.\nHover the mouse over the video.\nThe video pane grows larger.\nPress the play button.\nAs the video plays, you will see the current utterance highlighted in the transcript. You will also see that the current utterance appears as closed captions in the video. You can use the video controls as normal, including the full-screen button in the bottom right, to make the video occupy the whole screen.\nPause the recording.\nMove the mouse over one of the utterances further down the transcript.\nYou will notice that the video pane shrinks again, and that the mouse pointer becomes a play button.\nClick the utterance.\nYou will see that playback starts at that utterance. Playback will stop when the participant finishes the utterance.\nAt the top of the page, you will see a tab button labelled General; click it.\nYou will see some meta-data about the transcript and recording.\nLaBB-CAT attaches meta-data both to transcripts (called ‘transcript attributes’), and also to participants (‘participant attributes’).\nBelow the transcript attributes is the name of the participant. Click their name.\nYou will see a page with the participant attributes, and a list of the recordings they appear in. In this case, they appear in only one recording; if you were to click the name of the recording, you would be taken back to the transcript page you’ve just seen.\n\n\n\n\n\nOn the menu at the top of the page, there’s a Search option. Click it.\nYou will see a search form with a “Text” search box at the top, and options for meta data below.\nIn the “Text” box enter the word quake and press the Search button at the bottom.\nYou will see a list of hits, with the name of the transcript on the left and the matched word on the right, highlighted within its immediate context.\nClick the Search again link (or the Search link on the menu at the top)\nYou’ll see that the search form remembers the last search text.\nSelect ‘Female’ from the Gender drop-down box, leave the word quake in the “Text” box, and click Search.\nThis time the results are narrowed down to only female participants.\nClick the first result.\nYou will see the transcript page, as we saw earlier, but with each match from the search highlighted.\n\n\n\n\nYou can also search across multiple words, and search for patterns as well as exact spellings.\nFor example, let’s say you want to investigate how the pronunciation of the word ‘the’ changes when the following word starts with a vowel. You can search for this pattern using the search form:\n\nClick the Search option on the menu.\nSearch for the word the\nYou will see that there are lots of results, including many where ‘the’ is followed by a word that starts with a consonant.\nGo back to the Search page.\nNow search for: the [aeiou].*\nThis is a ‘regular expression’ that allows you to identify a pattern, with the following parts:\n\nthe word ‘the’\nfollowed by a space\nfollowed by any vowel ([aeiou])\nfollowed anything at all - . in a regular expression means ‘any character’, and * means ‘zero or more of the previous thing’, so .* means ‘zero or more characters’\n\nYou will see that the results include only instances where the word that follows ‘the’ starts with a vowel.\n\nSee if you can create a search for all words ending in ‘ing’\n\n\nIn this worksheet you have seen that:\n\nLaBB-CAT is a repository for recordings and their transcripts;\nTranscripts are grouped together into corpora;\nMeta-data can be attached to transcripts (transcript attributes) and to participants (participant attributes);\nYou can search the texts of the transcripts;\nYou can filter the search results on the basis of meta-data;\nYou can search for patterns as well as exact spelling, by using regular expressions."
  },
  {
    "objectID": "worksheets/demo/1-exploration.html#easy-exploration",
    "href": "worksheets/demo/1-exploration.html#easy-exploration",
    "title": "1 - Exploration",
    "section": "",
    "text": "Using the ‘easy’ method for exploring LaBB-CAT is simple, as annotation layers are largely ignored; each transcript is essentially treated as a plain text that you can search and display based on ordinary orthographic spelling.\n\nOn the LaBB-CAT home page, click the explore icon to access the easy exploration pages.\nYou will see a similar home page, with Browse icon, Easy Search, and Layered Search icons.\nClick the Browse icon.\nYou will see a page that lists the collections of recordings (or ‘corpora’) in the LaBB-CAT database. Each corpus contains a number of recordings.\nClick the first corpus listed.\nYou will see a page that lists the first 20 recordings in the corpus. The recording names are on the left, followed by some meta data (called ‘participant attributes’) about the participant in the recording. At the bottom is a list of pages, so you can access further recordings in the corpus.\nClick the name of the first recording.\nYou will see a page with transcript text, and the video appears in the top right corner of the page.\nHover the mouse over the video.\nThe video pane grows larger.\nPress the play button.\nAs the video plays, you will see the current utterance highlighted in the transcript. You will also see that the current utterance appears as closed captions in the video. You can use the video controls as normal, including the full-screen button in the bottom right, to make the video occupy the whole screen.\nPause the recording.\nMove the mouse over one of the utterances further down the transcript.\nYou will notice that the video pane shrinks again, and that the mouse pointer becomes a play button.\nClick the utterance.\nYou will see that playback starts at that utterance. Playback will stop when the participant finishes the utterance.\nAt the top of the page, you will see a tab button labelled General; click it.\nYou will see some meta-data about the transcript and recording.\nLaBB-CAT attaches meta-data both to transcripts (called ‘transcript attributes’), and also to participants (‘participant attributes’).\nBelow the transcript attributes is the name of the participant. Click their name.\nYou will see a page with the participant attributes, and a list of the recordings they appear in. In this case, they appear in only one recording; if you were to click the name of the recording, you would be taken back to the transcript page you’ve just seen."
  },
  {
    "objectID": "worksheets/demo/1-exploration.html#basic-search",
    "href": "worksheets/demo/1-exploration.html#basic-search",
    "title": "1 - Exploration",
    "section": "",
    "text": "On the menu at the top of the page, there’s a Search option. Click it.\nYou will see a search form with a “Text” search box at the top, and options for meta data below.\nIn the “Text” box enter the word quake and press the Search button at the bottom.\nYou will see a list of hits, with the name of the transcript on the left and the matched word on the right, highlighted within its immediate context.\nClick the Search again link (or the Search link on the menu at the top)\nYou’ll see that the search form remembers the last search text.\nSelect ‘Female’ from the Gender drop-down box, leave the word quake in the “Text” box, and click Search.\nThis time the results are narrowed down to only female participants.\nClick the first result.\nYou will see the transcript page, as we saw earlier, but with each match from the search highlighted."
  },
  {
    "objectID": "worksheets/demo/1-exploration.html#regular-expressions",
    "href": "worksheets/demo/1-exploration.html#regular-expressions",
    "title": "1 - Exploration",
    "section": "",
    "text": "You can also search across multiple words, and search for patterns as well as exact spellings.\nFor example, let’s say you want to investigate how the pronunciation of the word ‘the’ changes when the following word starts with a vowel. You can search for this pattern using the search form:\n\nClick the Search option on the menu.\nSearch for the word the\nYou will see that there are lots of results, including many where ‘the’ is followed by a word that starts with a consonant.\nGo back to the Search page.\nNow search for: the [aeiou].*\nThis is a ‘regular expression’ that allows you to identify a pattern, with the following parts:\n\nthe word ‘the’\nfollowed by a space\nfollowed by any vowel ([aeiou])\nfollowed anything at all - . in a regular expression means ‘any character’, and * means ‘zero or more of the previous thing’, so .* means ‘zero or more characters’\n\nYou will see that the results include only instances where the word that follows ‘the’ starts with a vowel.\n\nSee if you can create a search for all words ending in ‘ing’\n\n\nIn this worksheet you have seen that:\n\nLaBB-CAT is a repository for recordings and their transcripts;\nTranscripts are grouped together into corpora;\nMeta-data can be attached to transcripts (transcript attributes) and to participants (participant attributes);\nYou can search the texts of the transcripts;\nYou can filter the search results on the basis of meta-data;\nYou can search for patterns as well as exact spelling, by using regular expressions."
  },
  {
    "objectID": "worksheets/demo/4-alignment.html",
    "href": "worksheets/demo/4-alignment.html",
    "title": "4 - Alignment",
    "section": "",
    "text": "The Hidden Markov Model Toolkit (HTK) is a speech recognition toolkit developed at Cambridge University. It is a set of programs that can be used to build speech recognition systems. Part of the process of building such systems involves force-aligning training data - i.e. automatically lining up phonemic-transcriptions of known words with the audio signal in the training recordings. LaBB-CAT takes advantage of this capability to facilitate forced-alignment for your transcripts.\nIn order to do this, HTK needs the following ingredients:\n\na set of recordings broken up into short utterances\northographic transcriptions of each utterance\nphonemic transcriptions of each of the words in each utterance\n\nIn the demo database you have all of these three ingredients, and the data has be force-aligned using HTK.\nThis means that, in addition to the manually alignment of utterance start/end times, HTK has automatically provided start and end times for words, and also for the speech sounds (‘phones’) within each word.\n\nSelect the transcripts option on the menu, and open a transcript in the list.\nTick the segment layer; this is the layer that contains the phone that HTK has aligned.\n\nThe segment layer looks similar to the phonemes layer on the transcript page, but there are several important differences:\n\nEach of the phonemes layer annotations has the transcription for the whole word, e.g.\n\n/dɪfrənt/\n…but the segment layer has, for each word, several annotations, one for each phone.\n/d/\n/ɪ/\n/f/\n/r/\n/ə/\n/n/\n/t/\n\nThe phonemes layer annotation are word tags that are not aligned, but the segment layer annotations have a start and end time specified.\nThe phonemes layer can include more than one phonemic transcription for a word - all possible pronunciations found in CELEX are tagged on each token, e.g.\n\n\n/dɪfrənt/\n\n/dɪfrn̩t/\n\n/dɪfərənt/\n\n/dɪfərn̩t/\n…but the segment layer annotations represent only one pronunciation; the pronunciation that HTK determined to be the one that best matched the audio.\n\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using the \"EMU webApp\" that is integrated into LaBB-CAT.\n(For more information about EMU, see: http://ips-lmu.github.io/EMU.html)\n\nClick on a line that has been aligned (i.e. that has segments under the words).\nSelect the ‘View in EMU webApp’ option on the menu.\nA new window will appear, and after a short delay, you will see the wave form of the utterance audio, with a spectrogram, and below this, segment annotations which are aligned with the audio above and represent individual sounds within each word.\nYou can check the alignments by clicking on a segment to selected it, and then clicking the Play Selected button below.\n\n\n\n\nLaBB-CAT also integrates directly with Praat, if you have it installed on your computer. With Praat integration installed, you can similarly inspect alignments, but you can also correct them by moving the alignments in Praat and then saving them back to LaBB-CAT.\n\n\n\n\n\n\nNote\n\n\n\nIf you don’t use Praat, or don’t have it installed on your computer, you can skip this section.\n\n\nAlthough you can’t actually correct the Demo LaBB-CAT alignments, because you have read-only access to the data, you may like to install the Praat integration to get an idea of how it works:\nFirst, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the transcript page, above the playback controls, there’s a Praat icon - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\n\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\nClick on a line that has been aligned, and select the ‘Open Text Grid in Praat’ option on the menu.\n\nYou may also be prompted to download and run a program called “install-jsendpraat.jar”. If so, click the link, save the resulting file, run the program, and then do this step again.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nIf in doubt, check the  online help on the transcript page; it has a section explaining how to set up Praat integration on various browsers and operating systems.\n\nAfter a short delay, Praat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\nIf you click on a word, and hit the &lt;tab&gt; key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\nIf you had ‘edit’ rather than ‘read-only’ permissions in LaBB-CAT, then each time you opened an utterance in Praat, a button would appear in the transcript to the left of the line, labelled Import Changes. This button would allow you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments.\nThis mechanism can also be used to add other annotations from Praat into LaBB-CAT annotation layers.\n\n\nOnce the words and phone have been aligned with HTK there are a number of annotation possibilities that arise.\nFor example, word syllabification information can be retrieved from CELEX and combined with the aligned phones to construct aligned syllable annotations.\n\nTick the alignment project at the top of the transcript.\nThis reveals several layers.\nTick the syllables layer\nOnce the transcript has re-loaded, open an utterance with the EMU webApp or with Praat.\nYou will see that, in addition to aligned words and phones, the syllables are also aligned, and labelled with their phonemic transcription, and stressed syllables are prepended with an apostrophe.\n\nAlso, with exact word durations (i.e. excluding pauses in speech) and syllable counts, the speaker’s articulation rate, in syllables per minute, can be computed. The Statistics Layer Manager is a module that can be configured to compute sums, counts, and rates of various kinds over different scopes, including syllables per minute.\n\nTick the syllables per minute layer.\nYou will see that each utterance has a spanning annotation across the top of it, labelled with a number; that number is the articulation rate for that particular utterance.\nBoth local and global articulation rate can be calculated …\nClick on the name of the speaker at the top of the transcript.\nThis will open the participant attributes page for that speaker.\nYou will see that one of the attributes is Syllables per Minute; this is the speakers overall articulation rate, across all their utterances.\n\nArticulation rate is calculated by excluding the durations of inter-word pauses. These pauses themselves can be annotated, for search or analysis purposes.\n\nGo back to the transcript page.\nTick the previous pause layer.\nYou will see that many of the word tokens in the transcript are tagged with a number. These words are preceded by a pause in speech, and the number is the length of that pause in seconds.\nOpen an utterance in the EMU webApp or in Praat to confirm these pauses are correct.\n\nYou may notice that pauses in the middle of utterances are always right, but the pause before the first word in the utterance seems wrong. See if you can figure out why.\n\n\n\n\nGiven that HTK has created individually aligned phones in the database, those speech sounds can be searched and exported.\nLet’s say you’re particularly interested in the vowel in the word ‘KIT’. You can now identify and extract instances of that phoneme.\n\nClick the search link on the menu.\nTick the segment layer.\n\nThe segments layer contains annotations at the sub-word level - i.e. there are potentially multiple annotations per word, each annotation representing a single phone of the word. You will see that, as with other layers, there is a box on the segments layer for a regular expression.\nAs with other patterns in the search matrix, the pattern that you enter in the box is matched against individual annotations. So if you enter I (i.e. capital I) in the in the box, it will match each ‘KIT’ vowel segment in each word in the database.\n\n\n\n\n\n\nImportant\n\n\n\nIf you enter a pattern that would match more than a single character on this layer (i.e. more than a single phoneme) then no search results will be returned, because each annotation on this layer is only a single character long (remember the DISC encoding uses one character per phoneme).\nFor example, if you enter .*IN for your search, intending to match all words ending in “…ing”, then no results will be returned, because no single segment will ever match that pattern.\n\n\n\nWe want to search for all instances of the ‘KIT’ vowel, so enter I in the segments pattern box.\nClick Search\nAfter a short delay, you should see a list of results.\n\nYou will see that the results list words that have the ‘KIT’, but in many cases it’s not the main stressed vowel. What if we’re only interested in stressed ‘KIT’ vowels?\nThat’s ok, because we also have stress-marked syllable annotations, so we can add that layer to the search matrix, and identify only stressed vowels …\n\nNote down the number of results returned by your last search.\nBack on the search form, add the syllables layer to the search matrix.\nAs we have seen, stressed syllables are labelled with an apostrophe at the start. Enter a regular expression that will identify all syllables that start with apostrophe.\nIn this way, the results will give use all KIT vowels that are within a stressed syllable.\nClick Search again.\nThis time you will see fewer results returned, because we’ve filtered out the un-stressed version of the vowel.\n\n\nYou can export all these vowel tokens to a CSV file for analysis or further processing. The CSV file can include all kinds of other information, including participant and transcript attributes and other annotations.\n\nOn the results page, next to the CSV Export button there’s a ▼ button. Press it.\nYou will see several columns of checkboxes appear.\nTick the following checkboxes:\n\n\nUnder Participant tick gender, age_category, and syllables per minute\nUnder Span tick topic\nUnder Phrase tick syllables per minute\n\nNow press the CSV Export button above.\nSave and open the resulting file.\nYou will see that the file includes extra columns for the attributes and layers that you ticked (e.g. the topic marked in the original ELAN transcript, the speaker’s articulation rate, the local articulation rate, etc.).\n\nThe CSV file includes whatever annotation you might be interested, so you can go on to do qualitative or statistical analysis with other tools like Microsoft Excel or R. You can even add your own annotations to the CSV file and import them back into LaBB-CAT.\n\n\n\nThe CSV file also includes the columns “Target segments start” and “Target segments end”; these columns have the start and end time of the matching ‘KIT’ vowel token. Given this information, LaBB-CAT can extract acoustic measurements on the speech sounds using Praat.\n\n\n\n\n\n\nNote\n\n\n\nThe following steps work even if you don’t have Praat installed on your own computer, because Praat is used on the LaBB-CAT server …\n\n\n\nIn LaBB-CAT, click the upload menu option.\nClick the process with praat option.\nClick Browse and select the CSV results that you saved above.\nYou will see a form to fill in, and the first couple of settings (Transcript Name column and Participant column should be already filled in).\nFor the Start Time column, ensure that the Target segment start option is selected.\nFor the End Time column, ensure the Target segment end option is selected.\n\nThese two settings define the start/end times of the phone. For some measurements you might extract from Praat, processing signal that includes surrounding context is usually a good idea. You’ll see there’s a setting for that (which you can leave at the default of 0.025s), and you will see options for various measurements.\nThe default options are for F1 and F2 only, but if you feel like getting other measurements, feel free to tick those options too. You can expand each section with the ⯈ button to reveal more settings, which allow you to specify more detail about how Praat should do its computations. Again, feel free to look at those and try different settings.\n\nClick Process.\nYou will see a progress bar while LaBB-CAT generates Praat scripts and runs them.\nOnce Praat has finished processing the intervals, you will get a CSV file (you might have to click the CSV file with measurements link) - save and open it.\n\nYou will see that it’s a copy of the CSV file you uploaded, with some extra columns added on the right.\nDepending on your settings, this will include at least one column per measurement you selected (the formant columns also include on that contains the time at which the measurements were taken), and a final column called Error which is hopefully blank, but which might contain errors reported back by Praat (e.g. if it couldn’t find the audio file or ran into any other problem during processing).\n\nIn this worksheet you have seen that:\n\nHTK can be used to compute word and phone alignments automatically from your data.\nThe resulting alignments can be inspected and corrected directly from the transcript page.\nArticulation rate can be computed, excluding inter-word pauses.\nInter-word pauses can also be tagged.\nIndividual phone tokens can be searched for and extracted.\nAcoustic measurements for matching phones can also be made."
  },
  {
    "objectID": "worksheets/demo/4-alignment.html#htk",
    "href": "worksheets/demo/4-alignment.html#htk",
    "title": "4 - Alignment",
    "section": "",
    "text": "The Hidden Markov Model Toolkit (HTK) is a speech recognition toolkit developed at Cambridge University. It is a set of programs that can be used to build speech recognition systems. Part of the process of building such systems involves force-aligning training data - i.e. automatically lining up phonemic-transcriptions of known words with the audio signal in the training recordings. LaBB-CAT takes advantage of this capability to facilitate forced-alignment for your transcripts.\nIn order to do this, HTK needs the following ingredients:\n\na set of recordings broken up into short utterances\northographic transcriptions of each utterance\nphonemic transcriptions of each of the words in each utterance\n\nIn the demo database you have all of these three ingredients, and the data has be force-aligned using HTK.\nThis means that, in addition to the manually alignment of utterance start/end times, HTK has automatically provided start and end times for words, and also for the speech sounds (‘phones’) within each word.\n\nSelect the transcripts option on the menu, and open a transcript in the list.\nTick the segment layer; this is the layer that contains the phone that HTK has aligned.\n\nThe segment layer looks similar to the phonemes layer on the transcript page, but there are several important differences:\n\nEach of the phonemes layer annotations has the transcription for the whole word, e.g.\n\n/dɪfrənt/\n…but the segment layer has, for each word, several annotations, one for each phone.\n/d/\n/ɪ/\n/f/\n/r/\n/ə/\n/n/\n/t/\n\nThe phonemes layer annotation are word tags that are not aligned, but the segment layer annotations have a start and end time specified.\nThe phonemes layer can include more than one phonemic transcription for a word - all possible pronunciations found in CELEX are tagged on each token, e.g.\n\n\n/dɪfrənt/\n\n/dɪfrn̩t/\n\n/dɪfərənt/\n\n/dɪfərn̩t/\n…but the segment layer annotations represent only one pronunciation; the pronunciation that HTK determined to be the one that best matched the audio.\n\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using the \"EMU webApp\" that is integrated into LaBB-CAT.\n(For more information about EMU, see: http://ips-lmu.github.io/EMU.html)\n\nClick on a line that has been aligned (i.e. that has segments under the words).\nSelect the ‘View in EMU webApp’ option on the menu.\nA new window will appear, and after a short delay, you will see the wave form of the utterance audio, with a spectrogram, and below this, segment annotations which are aligned with the audio above and represent individual sounds within each word.\nYou can check the alignments by clicking on a segment to selected it, and then clicking the Play Selected button below."
  },
  {
    "objectID": "worksheets/demo/4-alignment.html#praat-browser-integration",
    "href": "worksheets/demo/4-alignment.html#praat-browser-integration",
    "title": "4 - Alignment",
    "section": "",
    "text": "LaBB-CAT also integrates directly with Praat, if you have it installed on your computer. With Praat integration installed, you can similarly inspect alignments, but you can also correct them by moving the alignments in Praat and then saving them back to LaBB-CAT.\n\n\n\n\n\n\nNote\n\n\n\nIf you don’t use Praat, or don’t have it installed on your computer, you can skip this section.\n\n\nAlthough you can’t actually correct the Demo LaBB-CAT alignments, because you have read-only access to the data, you may like to install the Praat integration to get an idea of how it works:\nFirst, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the transcript page, above the playback controls, there’s a Praat icon - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\n\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\nClick on a line that has been aligned, and select the ‘Open Text Grid in Praat’ option on the menu.\n\nYou may also be prompted to download and run a program called “install-jsendpraat.jar”. If so, click the link, save the resulting file, run the program, and then do this step again.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nIf in doubt, check the  online help on the transcript page; it has a section explaining how to set up Praat integration on various browsers and operating systems.\n\nAfter a short delay, Praat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\nIf you click on a word, and hit the &lt;tab&gt; key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\nIf you had ‘edit’ rather than ‘read-only’ permissions in LaBB-CAT, then each time you opened an utterance in Praat, a button would appear in the transcript to the left of the line, labelled Import Changes. This button would allow you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments.\nThis mechanism can also be used to add other annotations from Praat into LaBB-CAT annotation layers.\n\n\nOnce the words and phone have been aligned with HTK there are a number of annotation possibilities that arise.\nFor example, word syllabification information can be retrieved from CELEX and combined with the aligned phones to construct aligned syllable annotations.\n\nTick the alignment project at the top of the transcript.\nThis reveals several layers.\nTick the syllables layer\nOnce the transcript has re-loaded, open an utterance with the EMU webApp or with Praat.\nYou will see that, in addition to aligned words and phones, the syllables are also aligned, and labelled with their phonemic transcription, and stressed syllables are prepended with an apostrophe.\n\nAlso, with exact word durations (i.e. excluding pauses in speech) and syllable counts, the speaker’s articulation rate, in syllables per minute, can be computed. The Statistics Layer Manager is a module that can be configured to compute sums, counts, and rates of various kinds over different scopes, including syllables per minute.\n\nTick the syllables per minute layer.\nYou will see that each utterance has a spanning annotation across the top of it, labelled with a number; that number is the articulation rate for that particular utterance.\nBoth local and global articulation rate can be calculated …\nClick on the name of the speaker at the top of the transcript.\nThis will open the participant attributes page for that speaker.\nYou will see that one of the attributes is Syllables per Minute; this is the speakers overall articulation rate, across all their utterances.\n\nArticulation rate is calculated by excluding the durations of inter-word pauses. These pauses themselves can be annotated, for search or analysis purposes.\n\nGo back to the transcript page.\nTick the previous pause layer.\nYou will see that many of the word tokens in the transcript are tagged with a number. These words are preceded by a pause in speech, and the number is the length of that pause in seconds.\nOpen an utterance in the EMU webApp or in Praat to confirm these pauses are correct.\n\nYou may notice that pauses in the middle of utterances are always right, but the pause before the first word in the utterance seems wrong. See if you can figure out why."
  },
  {
    "objectID": "worksheets/demo/4-alignment.html#searching",
    "href": "worksheets/demo/4-alignment.html#searching",
    "title": "4 - Alignment",
    "section": "",
    "text": "Given that HTK has created individually aligned phones in the database, those speech sounds can be searched and exported.\nLet’s say you’re particularly interested in the vowel in the word ‘KIT’. You can now identify and extract instances of that phoneme.\n\nClick the search link on the menu.\nTick the segment layer.\n\nThe segments layer contains annotations at the sub-word level - i.e. there are potentially multiple annotations per word, each annotation representing a single phone of the word. You will see that, as with other layers, there is a box on the segments layer for a regular expression.\nAs with other patterns in the search matrix, the pattern that you enter in the box is matched against individual annotations. So if you enter I (i.e. capital I) in the in the box, it will match each ‘KIT’ vowel segment in each word in the database.\n\n\n\n\n\n\nImportant\n\n\n\nIf you enter a pattern that would match more than a single character on this layer (i.e. more than a single phoneme) then no search results will be returned, because each annotation on this layer is only a single character long (remember the DISC encoding uses one character per phoneme).\nFor example, if you enter .*IN for your search, intending to match all words ending in “…ing”, then no results will be returned, because no single segment will ever match that pattern.\n\n\n\nWe want to search for all instances of the ‘KIT’ vowel, so enter I in the segments pattern box.\nClick Search\nAfter a short delay, you should see a list of results.\n\nYou will see that the results list words that have the ‘KIT’, but in many cases it’s not the main stressed vowel. What if we’re only interested in stressed ‘KIT’ vowels?\nThat’s ok, because we also have stress-marked syllable annotations, so we can add that layer to the search matrix, and identify only stressed vowels …\n\nNote down the number of results returned by your last search.\nBack on the search form, add the syllables layer to the search matrix.\nAs we have seen, stressed syllables are labelled with an apostrophe at the start. Enter a regular expression that will identify all syllables that start with apostrophe.\nIn this way, the results will give use all KIT vowels that are within a stressed syllable.\nClick Search again.\nThis time you will see fewer results returned, because we’ve filtered out the un-stressed version of the vowel.\n\n\nYou can export all these vowel tokens to a CSV file for analysis or further processing. The CSV file can include all kinds of other information, including participant and transcript attributes and other annotations.\n\nOn the results page, next to the CSV Export button there’s a ▼ button. Press it.\nYou will see several columns of checkboxes appear.\nTick the following checkboxes:\n\n\nUnder Participant tick gender, age_category, and syllables per minute\nUnder Span tick topic\nUnder Phrase tick syllables per minute\n\nNow press the CSV Export button above.\nSave and open the resulting file.\nYou will see that the file includes extra columns for the attributes and layers that you ticked (e.g. the topic marked in the original ELAN transcript, the speaker’s articulation rate, the local articulation rate, etc.).\n\nThe CSV file includes whatever annotation you might be interested, so you can go on to do qualitative or statistical analysis with other tools like Microsoft Excel or R. You can even add your own annotations to the CSV file and import them back into LaBB-CAT."
  },
  {
    "objectID": "worksheets/demo/4-alignment.html#acoustic-measurement",
    "href": "worksheets/demo/4-alignment.html#acoustic-measurement",
    "title": "4 - Alignment",
    "section": "",
    "text": "The CSV file also includes the columns “Target segments start” and “Target segments end”; these columns have the start and end time of the matching ‘KIT’ vowel token. Given this information, LaBB-CAT can extract acoustic measurements on the speech sounds using Praat.\n\n\n\n\n\n\nNote\n\n\n\nThe following steps work even if you don’t have Praat installed on your own computer, because Praat is used on the LaBB-CAT server …\n\n\n\nIn LaBB-CAT, click the upload menu option.\nClick the process with praat option.\nClick Browse and select the CSV results that you saved above.\nYou will see a form to fill in, and the first couple of settings (Transcript Name column and Participant column should be already filled in).\nFor the Start Time column, ensure that the Target segment start option is selected.\nFor the End Time column, ensure the Target segment end option is selected.\n\nThese two settings define the start/end times of the phone. For some measurements you might extract from Praat, processing signal that includes surrounding context is usually a good idea. You’ll see there’s a setting for that (which you can leave at the default of 0.025s), and you will see options for various measurements.\nThe default options are for F1 and F2 only, but if you feel like getting other measurements, feel free to tick those options too. You can expand each section with the ⯈ button to reveal more settings, which allow you to specify more detail about how Praat should do its computations. Again, feel free to look at those and try different settings.\n\nClick Process.\nYou will see a progress bar while LaBB-CAT generates Praat scripts and runs them.\nOnce Praat has finished processing the intervals, you will get a CSV file (you might have to click the CSV file with measurements link) - save and open it.\n\nYou will see that it’s a copy of the CSV file you uploaded, with some extra columns added on the right.\nDepending on your settings, this will include at least one column per measurement you selected (the formant columns also include on that contains the time at which the measurements were taken), and a final column called Error which is hopefully blank, but which might contain errors reported back by Praat (e.g. if it couldn’t find the audio file or ran into any other problem during processing).\n\nIn this worksheet you have seen that:\n\nHTK can be used to compute word and phone alignments automatically from your data.\nThe resulting alignments can be inspected and corrected directly from the transcript page.\nArticulation rate can be computed, excluding inter-word pauses.\nInter-word pauses can also be tagged.\nIndividual phone tokens can be searched for and extracted.\nAcoustic measurements for matching phones can also be made."
  },
  {
    "objectID": "worksheets/demo/2-search.html",
    "href": "worksheets/demo/2-search.html",
    "title": "2 - Search",
    "section": "",
    "text": "In addition to storing recordings and orthographic transcripts, the data can also be annotated in various ways with different information. Each type of annotation is stored on its own ‘layer’, so you can display and search on the basis of different aspects of the transcripts, including:\n\nfrequency\nlemma\npart of speech\npronunciation\nspeech rate\npause duration\n…and more.\n\nAnnotations can be made manually, and LaBB-CAT includes modules (called ‘Layer Managers’) for doing certain annotations automatically.\nVarious automatically generated annotation layers have been configured in the demo instance of LaBB-CAT, and we will start to explore some of them in this worksheet.\n\n\nLayered search is usually a two-step process: first you select which participants you want to search, using their participant attributes. And then you specify the pattern you want to search for.\nIf we were interested only in monolingual speakers, for example, we would filter out those that speak various language by setting the attribute values appropriately on the filter page.\n\nFirstly, return to LaBB-CAT’s home page by clicking the Home link on the menu, and then click the Layered Search icon.\nYou will see a page called “Participants”.\nSelect ‘M’ in the Gender box.\nYou will see a list of the male participants only.\nNotice that each participant has a check-box; if we wanted to, we could select specific participants from the list by checking/unchecking the boxes. (But in this case, let’s search all of them, so leave all the boxes un-ticked.)\nPress the Layered Search button at the top of the list.\nYou will see a page that lists the speakers at the top, a number of tickable annotation layers in the middle, a ‘Search Matrix’ below. (It doesn’t look much like a matrix yet, as it only includes the ‘orthography’ layer, but we will be adding rows and columns later on.)\nIn the box labelled “orthography” type the regular expression th[aeiou].+\nAs you saw earlier, [aeiou] means ‘any vowel’, and a full-stop/period means ‘any character’\nThe plus-sign means ‘one or more of the previous thing’, so .+ means ‘at least one character’.\nNow press the Search button at the bottom (or hit Enter).\nA progress bar will appear, and then shortly after that, a new window will open, which has a list of search results in it. Your browser’s popup-blocker might prevent the results page from opening - you can fix that either by allowing the popups in your browser, or by clicking the Display results link that appears after the search finishes.\n\nYou will see that the results include words like “that”, “there”, “then”, etc. - i.e. words that start with “th”, followed by a vowel, followed by at least one more letter.\n\n\n\n\n\n\nTip\n\n\n\n You can get more information about regular expressions by using the online help back on the search page, and also by clicking the the regular expressions link above the tickable layers.\n\n\nAs we previously saw with the ‘easy search’, each match is highlighted and shown within a few words context. However, this results page has a few more options available.\n\nIn the Context drop-down box at the top, select the 5 words option, to show more context in the llist of results.\nEach result line has a ticked checkbox next to it. At the bottom of the list, you’ll see that there are various buttons, which perform operations on the ticked results, including CSV Export, Utterance Export, and Audio Export.\nUntick the “Select all result” checkbox, and then tick a handful of results in the list.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can select a group of matches by ticking the first one, and then holding down the Shift key while ticking the last one.\n\n\n\nClick the Audio Export button.\nSave and open the resulting zip file. \nYou’ll see that extracted wav files are systematically named to include:\n\nthe name of the transcript\nthe start and end time of the extracted utterance\n\nIf you also have Praat installed on your computer, go back to the results page and click Utterance Export button. Save and open the resulting zip file.\nYou’ll see that the TextGrid names match the audio file names in the previous zip file.\nIf you open a TextGrid in Praat, you’ll see it includes a tier for the whole utterance transcript, a tier with an interval for each word, and a target… tier which tags the word that matched the regular expression you searched for.\nBack on the results page, click the CSV Export button.\nSave the resulting file, and open it.\nYou may have to specify some import options, in which case it may be handy to know that the field separator is comma, and the fields are quoted by speech marks.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using Microsoft Excel and you find it doesn’t open all the columns correctly:\n\nCreate a new workbook in Excel.\nClick the ‘Data’ tab.\nOn the “Get External Data” ribbon click ‘From Text’.\nSelect the CSV file you downloaded.\nSelect ‘Delimited’ and click Next.\nEnsure ‘Comma’ is the only delimiter ticked and click Next.\nClick Finish and then OK.\n\n\n\nYou will see a spreadsheet with one line per selected result, and various columns containing information about the speaker, the corpus, the match line and word, and a URL to the interactive transcript for the match.\nWith this spreadsheet, you can work ‘offline’ with the results, tagging them, computing statistics in Excel, R, or any other program that can work with CSV files. We’ll look at a few more uses for the CSV results files later…\n\nClose the CSV file, and got back to the results page.\n\nUp until now, we’ve only been matching against one word at a time. Now we’re going to include patterns for a chain of words. Unlike the simple search, adding a space in the regular won’t work, because each column in the search matrix only matches a single word. To match a chain of two words, we need to have two columns in the search matrix.\n\nOn the search page, next to the orthography box where you entered the regular expression, there’s a ➕ button for adding a column to the matrix. Click it.\n\nNow you will see that our search matrix is one layer high by two words wide.\nChange the entries on the orthography layer so that it will match the word “the” followed immediately by a word that starts with a vowel, and click Search.\n\nCheck the search results are giving you what you expected. You may note that some of the following words start with a vowel in the spelling, even though they are not pronounced with a vowel sound. We will see how to search on the basis of pronunciation in another worksheet.\nNow search for “the” followed, within two words, by a word that starts with a vowel.\n\n\n\n\n\n\n\nTip\n\n\n\n If in doubt about a search option, try the online help page.\n\n\n\n\n\nSo far we have only searched the orthography layer - i.e. the ordinary spellings of words. But LaBB-CAT has been configured to generate a number of other annotation layers.\nLet’s say we’re interested in how rare or common words are in our data.\nLaBB-CAT’s ‘Frequency Layer Manager’ is a module that counts up the number of times each word type appears in the database. It generates a frequency list, and also annotates each word token with its frequency.\nWe’ll now search for tokens of words that appear only once in the database.\nThe annotation layers are grouped into a number of ‘projects’ to avoid clutter. We will initially be interested in the layers related to frequency.\n\nOn the search page, in the box labelled Tick layers to include, there’s a Projects column. Tick the frequency’ project.\nSome addition layers will appear in the layer list on the right.\nTick the word frequency layer.\nSet the word matrix to be 1 word wide again by clicking the ➖ button to the right.\nYou will see that the search matrix now has two layers in it. \n\nUnlike the orthography layer, which has one box for a regular expression, the word frequency layer has two boxes, marked “≥” and “&lt;”. This is because the annotation values are numbers. \nWe want all the words that appeared only once in the database. Enter a number or numbers in the appropriate box (you can leave either box blank) and click Search.\n\n\n\n\n\n\n\nImportant\n\n\n\nEnsure the orthography box is empty, otherwise it will be trying to find in instances of the word “the” that appear only once in the corpus; there are lots of instances of the word “the”, so the search will return no results, as the frequency is greater than 1.\n\n\n\nClick on the first result in the list.\n\n\n\n\nThis displays the ‘layered transcript’ page for the recording. This is similar to the previous ‘easy’ transcript page, but has a number of extra options and functions.\nThe most obvious difference is that each word token has a number above it. This is the frequency of that word, which is displayed because the word frequency layer is selected; there’s a list of layers at the top of the transcript, and you can see that both word frequency and word are ticked.\n\nUntick the word frequency layer.\nAfter a short delay, the transcript will be displayed again, with only the transcript text visible.\n\nThe transcript also includes any noises (e.g. “tuts”), comments, and other events that were put in the transcript in ELAN.\n\nThe video is the top right corner as before; click the play button. Again you will see a shaded rectangle following the participant’s speech.\nTry clicking the magnifying-glass icon  below the video, to see what it does.\nNow click on any word in the transcript.\nYou will see a menu appear.\nClick the play option in the menu to see what it does.\nClick on the formats link under the title.\nYou will see a menu, which includes various formats for exporting the transcript.\nSelect Plain Text Document\nSave the resulting file on your desktop, and then open it.\nYou will see the transcript in plain-text form.\nIf you have Praat installed on your computer, click the formats link, and select the Praat Text Grid option. Save the resulting file on your desktop, and then open it with Praat.\n\nYou will see that the TextGrid has various tiers, one for whole utterances (or two if there are two speakers), and one for individual words (or two if there are two speakers). \n\n(You will see that each individual word has a ‘default’ alignment - i.e. the words are evenly spread out during the duration of the line they’re in. In a later exercise we will look at ways to make these word alignments actually line up with the words in the audio signal)\n\nUsing frequencies of full wordforms can be useful, but in some circumstances it may be more informative to group together different forms of the same word; e.g. treat “damage”, “damaged” and “damaging” as variants of the same thing for the purposes of frequency-counting.\nWe’ll see a way to do that in the next worksheet.\n\nIn this worksheet you have seen that:\n\nAnnotations can be automatically added to transcripts in layers, using Layer Managers.\nThe Frequency Layer Manager can tag words with their frequencies, and maintains a frequency list.\nAnnotation layers can be searched using the search matrix, using numeric value or regular expressions.\nLayers can be optionally displayed in transcripts."
  },
  {
    "objectID": "worksheets/demo/2-search.html#layered-search-matrix",
    "href": "worksheets/demo/2-search.html#layered-search-matrix",
    "title": "2 - Search",
    "section": "",
    "text": "Layered search is usually a two-step process: first you select which participants you want to search, using their participant attributes. And then you specify the pattern you want to search for.\nIf we were interested only in monolingual speakers, for example, we would filter out those that speak various language by setting the attribute values appropriately on the filter page.\n\nFirstly, return to LaBB-CAT’s home page by clicking the Home link on the menu, and then click the Layered Search icon.\nYou will see a page called “Participants”.\nSelect ‘M’ in the Gender box.\nYou will see a list of the male participants only.\nNotice that each participant has a check-box; if we wanted to, we could select specific participants from the list by checking/unchecking the boxes. (But in this case, let’s search all of them, so leave all the boxes un-ticked.)\nPress the Layered Search button at the top of the list.\nYou will see a page that lists the speakers at the top, a number of tickable annotation layers in the middle, a ‘Search Matrix’ below. (It doesn’t look much like a matrix yet, as it only includes the ‘orthography’ layer, but we will be adding rows and columns later on.)\nIn the box labelled “orthography” type the regular expression th[aeiou].+\nAs you saw earlier, [aeiou] means ‘any vowel’, and a full-stop/period means ‘any character’\nThe plus-sign means ‘one or more of the previous thing’, so .+ means ‘at least one character’.\nNow press the Search button at the bottom (or hit Enter).\nA progress bar will appear, and then shortly after that, a new window will open, which has a list of search results in it. Your browser’s popup-blocker might prevent the results page from opening - you can fix that either by allowing the popups in your browser, or by clicking the Display results link that appears after the search finishes.\n\nYou will see that the results include words like “that”, “there”, “then”, etc. - i.e. words that start with “th”, followed by a vowel, followed by at least one more letter.\n\n\n\n\n\n\nTip\n\n\n\n You can get more information about regular expressions by using the online help back on the search page, and also by clicking the the regular expressions link above the tickable layers.\n\n\nAs we previously saw with the ‘easy search’, each match is highlighted and shown within a few words context. However, this results page has a few more options available.\n\nIn the Context drop-down box at the top, select the 5 words option, to show more context in the llist of results.\nEach result line has a ticked checkbox next to it. At the bottom of the list, you’ll see that there are various buttons, which perform operations on the ticked results, including CSV Export, Utterance Export, and Audio Export.\nUntick the “Select all result” checkbox, and then tick a handful of results in the list.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can select a group of matches by ticking the first one, and then holding down the Shift key while ticking the last one.\n\n\n\nClick the Audio Export button.\nSave and open the resulting zip file. \nYou’ll see that extracted wav files are systematically named to include:\n\nthe name of the transcript\nthe start and end time of the extracted utterance\n\nIf you also have Praat installed on your computer, go back to the results page and click Utterance Export button. Save and open the resulting zip file.\nYou’ll see that the TextGrid names match the audio file names in the previous zip file.\nIf you open a TextGrid in Praat, you’ll see it includes a tier for the whole utterance transcript, a tier with an interval for each word, and a target… tier which tags the word that matched the regular expression you searched for.\nBack on the results page, click the CSV Export button.\nSave the resulting file, and open it.\nYou may have to specify some import options, in which case it may be handy to know that the field separator is comma, and the fields are quoted by speech marks.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using Microsoft Excel and you find it doesn’t open all the columns correctly:\n\nCreate a new workbook in Excel.\nClick the ‘Data’ tab.\nOn the “Get External Data” ribbon click ‘From Text’.\nSelect the CSV file you downloaded.\nSelect ‘Delimited’ and click Next.\nEnsure ‘Comma’ is the only delimiter ticked and click Next.\nClick Finish and then OK.\n\n\n\nYou will see a spreadsheet with one line per selected result, and various columns containing information about the speaker, the corpus, the match line and word, and a URL to the interactive transcript for the match.\nWith this spreadsheet, you can work ‘offline’ with the results, tagging them, computing statistics in Excel, R, or any other program that can work with CSV files. We’ll look at a few more uses for the CSV results files later…\n\nClose the CSV file, and got back to the results page.\n\nUp until now, we’ve only been matching against one word at a time. Now we’re going to include patterns for a chain of words. Unlike the simple search, adding a space in the regular won’t work, because each column in the search matrix only matches a single word. To match a chain of two words, we need to have two columns in the search matrix.\n\nOn the search page, next to the orthography box where you entered the regular expression, there’s a ➕ button for adding a column to the matrix. Click it.\n\nNow you will see that our search matrix is one layer high by two words wide.\nChange the entries on the orthography layer so that it will match the word “the” followed immediately by a word that starts with a vowel, and click Search.\n\nCheck the search results are giving you what you expected. You may note that some of the following words start with a vowel in the spelling, even though they are not pronounced with a vowel sound. We will see how to search on the basis of pronunciation in another worksheet.\nNow search for “the” followed, within two words, by a word that starts with a vowel.\n\n\n\n\n\n\n\nTip\n\n\n\n If in doubt about a search option, try the online help page."
  },
  {
    "objectID": "worksheets/demo/2-search.html#searching-other-layers",
    "href": "worksheets/demo/2-search.html#searching-other-layers",
    "title": "2 - Search",
    "section": "",
    "text": "So far we have only searched the orthography layer - i.e. the ordinary spellings of words. But LaBB-CAT has been configured to generate a number of other annotation layers.\nLet’s say we’re interested in how rare or common words are in our data.\nLaBB-CAT’s ‘Frequency Layer Manager’ is a module that counts up the number of times each word type appears in the database. It generates a frequency list, and also annotates each word token with its frequency.\nWe’ll now search for tokens of words that appear only once in the database.\nThe annotation layers are grouped into a number of ‘projects’ to avoid clutter. We will initially be interested in the layers related to frequency.\n\nOn the search page, in the box labelled Tick layers to include, there’s a Projects column. Tick the frequency’ project.\nSome addition layers will appear in the layer list on the right.\nTick the word frequency layer.\nSet the word matrix to be 1 word wide again by clicking the ➖ button to the right.\nYou will see that the search matrix now has two layers in it. \n\nUnlike the orthography layer, which has one box for a regular expression, the word frequency layer has two boxes, marked “≥” and “&lt;”. This is because the annotation values are numbers. \nWe want all the words that appeared only once in the database. Enter a number or numbers in the appropriate box (you can leave either box blank) and click Search.\n\n\n\n\n\n\n\nImportant\n\n\n\nEnsure the orthography box is empty, otherwise it will be trying to find in instances of the word “the” that appear only once in the corpus; there are lots of instances of the word “the”, so the search will return no results, as the frequency is greater than 1.\n\n\n\nClick on the first result in the list."
  },
  {
    "objectID": "worksheets/demo/2-search.html#layered-transcript",
    "href": "worksheets/demo/2-search.html#layered-transcript",
    "title": "2 - Search",
    "section": "",
    "text": "This displays the ‘layered transcript’ page for the recording. This is similar to the previous ‘easy’ transcript page, but has a number of extra options and functions.\nThe most obvious difference is that each word token has a number above it. This is the frequency of that word, which is displayed because the word frequency layer is selected; there’s a list of layers at the top of the transcript, and you can see that both word frequency and word are ticked.\n\nUntick the word frequency layer.\nAfter a short delay, the transcript will be displayed again, with only the transcript text visible.\n\nThe transcript also includes any noises (e.g. “tuts”), comments, and other events that were put in the transcript in ELAN.\n\nThe video is the top right corner as before; click the play button. Again you will see a shaded rectangle following the participant’s speech.\nTry clicking the magnifying-glass icon  below the video, to see what it does.\nNow click on any word in the transcript.\nYou will see a menu appear.\nClick the play option in the menu to see what it does.\nClick on the formats link under the title.\nYou will see a menu, which includes various formats for exporting the transcript.\nSelect Plain Text Document\nSave the resulting file on your desktop, and then open it.\nYou will see the transcript in plain-text form.\nIf you have Praat installed on your computer, click the formats link, and select the Praat Text Grid option. Save the resulting file on your desktop, and then open it with Praat.\n\nYou will see that the TextGrid has various tiers, one for whole utterances (or two if there are two speakers), and one for individual words (or two if there are two speakers). \n\n(You will see that each individual word has a ‘default’ alignment - i.e. the words are evenly spread out during the duration of the line they’re in. In a later exercise we will look at ways to make these word alignments actually line up with the words in the audio signal)\n\nUsing frequencies of full wordforms can be useful, but in some circumstances it may be more informative to group together different forms of the same word; e.g. treat “damage”, “damaged” and “damaging” as variants of the same thing for the purposes of frequency-counting.\nWe’ll see a way to do that in the next worksheet.\n\nIn this worksheet you have seen that:\n\nAnnotations can be automatically added to transcripts in layers, using Layer Managers.\nThe Frequency Layer Manager can tag words with their frequencies, and maintains a frequency list.\nAnnotation layers can be searched using the search matrix, using numeric value or regular expressions.\nLayers can be optionally displayed in transcripts."
  },
  {
    "objectID": "worksheets/demo/5-syntax.html",
    "href": "worksheets/demo/5-syntax.html",
    "title": "5 - Syntax",
    "section": "",
    "text": "5 - Syntax\nThe Stanford Parser is an open-source PCFG parser that can use grammars for a variety of languages including English.\nLaBB-CAT includes a Layer Manager that handles integration with the parser. The Stanford Parser Layer Manager:\n\nextracts chunks of transcripts (ideally sentences or clauses),\ngives them to the Stanford Parser for processing, which produces a ‘best parse’ for the utterance provided, and\nsaves the parse on a ‘tree’ layer, and optionally saves the resulting part-of-speech tags on a word layer.\n\nOne of the problems with parsing speech is that speakers often don’t speak in complete, well-formed sentences. In addition, the demo corpus you are using was not generally transcribed with parsing in mind, and so grammatically complete units have not been marked with full-stops, commas, etc. (Instead, full-stop has been used to mark short pauses in speech).\nFor these reasons, the parses you will see in this data may not be perfect. However, it’s possible to get a sense of the kinds of things that could be achieved with well-formed written texts, or speech that has been transcribed with grammatical punctuation included.\n\nClick the transcripts link on the menu.\n\nOne transcript in the database has delimiters inserted which divide the transcript into more or less grammatical units. As the ‘full-stop’ symbol is already being used to mark pauses, the ‘vertical bar’ symbol | has been used as a grammatical delimiter.\nThe transcript is called BR2044_OllyOhlson.eaf\n\nIn the Transcript box, type olly.\nAfter a short pause, BR2044_OllyOhlson.eaf will be the only transcript in the filtered list.\nClick BR2044_OllyOhlson.eaf\nUntick all except the word layer, to avoid clutter.\nWhen the transcript re-loads, tick the syntax project.\nThis reveals three layers.\nTick the parseable layer.\n\nWhen the transcript re-loads, you will see that almost all words in the transcript have been tagged with their own orthography. However, some words have not been tagged:\n\nfilled pauses like “um”, “ah , etc. and\nincomplete words like “re~”, “na~”, etc. - i.e. cases where the participant started saying something but changed their mind.\n\nThese have been identified by the Pattern Matcher Layer Manager, which can pick out words by regular expression, and has been configured to tag as ‘parseable’ all tokens except those that matching the following patterns:\n\na+h+ - e.g. “ah”, “aah”, “ahh”, …\nm+h*m+ - e.g. “mm”, “mmm”, “mhmm”, …\ne+r+ - e.g. “er”, “err”, “eeerr”, …\nu+m+ - e.g. “um”, “uum”, “ummm”, …\n.+~ - e.g. “re~”, “na~”, “w~”, ...\n\nThe result is that the parseable layer includes all words except filled pauses and incomplete words. This is the layer that is passed to the Stanford Parser for syntactic parsing.\nOne of the results of parsing is that each word token is tagged with its part-of-speech.\n\nUntick the parseable layer and tick the pos layer.\nYou will see that most of the words have been tagged with a syntactic category:\n\n“CC” Coordinating conjunction\n“DT” Determiner\n“JJ” Adjective\n“NN” Noun\n“NNS” Plural noun\n“NNP” Proper noun\n“PRP” Personal pronoun\n“PRP$” Possessive pronoun\n“VB” Verb\n“VBD” Past tense verb\n“VBG” Gerund\n“VBZ” 3rd person singular present verb\n…etc.\n\n\nThese, like any other word tags you have seen, can be included in searches or exported to CSV results files.\n\nThe other result from parsing is, of course, a ‘parse tree’ of each utterance.\n\nUntick the pos layer  and tick the parse layer.\nYou will see that above the words, there are bracketing annotations that are labelled with parts-of-speech or phrase labels.\nEach of these brackets represents a syntactic constituent constructed by the parser, smaller constituents at the bottom, building into larger constituents going up.\nClick on any constituent label (e.g. “NP” or “S”).\nA new window will open, which shows the selected utterance, using the familiar ‘upside-down tree’ representation.\nIf the tree appears far off and small, you can make it larger by widening the window, or ‘zooming in’ with you mouse wheel.\n\nYou can also search the parses themselves.\nClose the parse tree window.\nClick the search option on the menu.\nTick the syntax project.\nTick the parse and pos layers.\n\nOn the parse layer you’ll see that you can enter a search expression for annotations on that layer, just like any other.\nHowever, it also has a checkbox before and after the pattern. If you tick the checkbox before the pattern, it will anchor the search to the first word in the matching constituent. Similarly the checkbox after the pattern anchors to the last word in the constituent.\nLet’s say you want all the noun phrases that don’t start with a determiner like “the”, “this”, “a”, etc.\n\nEnter a pattern that matches NP (noun phrase) on the parse layer…\n...and tick the checkbox to anchor to the first word in the consituent.\nEnter a pattern that would match DT (determiner) on the pos layer…\n...and in the dropdown box before the pattern, select doesn’t match.\n\nThis will match words who have an annotation that doesn’t match “DT” on the *pos* layer.\n\n\nPress Search.\n\nWhen the search finishes, you should see that lots of nouns are returned, but nothing preceded by “the”, “a”, “that”, or any other determiner.\n\nIn this worksheet you have seen that:\n\nthe Stanford Parser can we used to annotate transcripts with part-of-speech tags and constituent annotations, and\nthe resulting annotations can be included in syntax-based searches.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/demo/index.html",
    "href": "worksheets/demo/index.html",
    "title": "LaBB-CAT Demo Session",
    "section": "",
    "text": "LaBB-CAT Demo Session\n1 - Exploration\n2 - Search\n3 - CELEX\n4 - Alignment\n5 - Syntax\n6 - Other Processing\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html",
    "href": "worksheets/demo/6-other-processing.html",
    "title": "6 - Other Processing",
    "section": "",
    "text": "You have seen in a previous worksheet that articulation rate can be calculated over the words in individual utterances, and also over all the words uttered by each participant. There are other useful computations that can be computed over different scopes.\n\nSelect the transcripts link on the menu.\nYou may have previously noticed that the top of the page includes a form with various transcript attributes. This form allows you to both filter and sort the list of transcripts by transcript attribute values.\nFor the Word Count attribute, there are two boxes which you can use to specify a range of values.\nIn the right-hand To box, enter 1000.\nWhen the list reloads, you will see a list of all transcripts that have up to 1000 words.\nThis word count was computed by the Statistics Layer Manager, which has also been configured to compute speech duration in seconds and save the result in the “Duration” transcript attribute.\n\nAnother simply aggregate calculation is type/token ratio. The Demo LaBB-CAT has been configured to compute the type/token ratio for each participant.\n\nSelect the participants link on the menu.\nFor the corpus attribute, tick the QB option to list only participants recorded in the “Quake Box” portable recording studio.\nAt the top of the participant list, press the Export Attributes button.\nTick the Gender, Age and type/token ratio attributes.\nPress the Participant Data button.\nSave and open the resulting CSV file.\nYou will see that you have a list of their participants, with gender and age, and also a column for type/token ratio; this is the ratio expressed as a percentage.\n\n\n\n\nThe transcripts in this database each have a video and an audio file.\nHowever, some of the recordings have also been processed with a facial feature location algorithm. One of the results of this process was an annotated video; a copy of the original video, with the participant’s face located, along with various facial landmarks (position of the eyes,\nshape of mouth, etc.).\nLaBB-CAT supports having multiple media ‘track’ files for the same transcript, and for some of the transcripts, the annotated video has been uploaded as well as the original video.\n\nOn the transcripts page, list the transcripts with the Quake Face attribute set to 1 - true.\nOpen one of the listed transcripts.\n\nOn the top right of the page, by default, the original video is selected for display, but all the other media files available for the transcript are listed below the video, with a checkbox next to each.\nTick the checkbox next to the media file that ends with “…_face”. The transcript will reload and display the annotated video.\nPress Play to see the annotations (marked in green and red) change with the facial features.\nIt may be easier to see the annotations if you put the video in ‘full screen’ mode.\n\n\n\n\nIn addition to calculating word frequencies for direct analysis, frequencies can be compared to a reference corpus to calculated their ‘keyness’; a measure of whether the word is unusually frequent (a high positive keyness) or unusually infrequent (a low negative keyness).\nThe Demo LaBB-CAT has been configured to compute keyness compared to the frequencies available in the CELEX lexicon, which come from the Cobuild corpus.\n\nSelect the home link on the menu.\nClick the ‘Keyness’ icon.\nYou will see a form that allows you to search for particular spelling patterns, or export a list.\nPress the Search button without filling in the Pattern box, to list all words above the default Keyness threshold.\nA list of words will be displayed, each word with its keyness metric. The high-positive words (which are unusually frequent) are listed first, with the low-negative words (unusually infrequent) below.\n\nUnsurprisingly for this speech corpus, as compared to the mostly-written Cobuild corpus, words with high keyness include filled pauses like “um” and “ahh”, other words more likely in informal speech like “gonna” and “yeah”, topic-specific words like “earthquake” and “aftershocks”, and Canterbury place-names like “Christchurch” and “Brooklands”.\nThe Frequency Layer Manager can be configured to compute keyness of the data compared to any corpus for which you have word frequency data, or if you have several corpora within one LaBB-CAT database, each corpus can be compared to all the rest.\n\n\n\nLinguistic Inquiry and Word Count (LIWC) text analysis can be done with the LIWC Layer Manager and categorised word lists.\nSee: https://liwc.wpengine.com/how-it-works/\nOr: Tausczik & Pennebaker (2010) “The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods” Journal of Language and Social Psychology 29 (1) 24-54\nLIWC involves calculating the percentage of words in different categories. Categorised word lists can be purchased from liwc.wpengine.com, or can be compiled by hand.\nLIWC text analysis has been done on the Demo LaBB-CAT database, and also on the Cobuild corpus as a comparison corpus.\n\nClick the home link on the menu.\nClick the ‘LIWC’ icon.\nYou will see a horizontal bar graph: each bar represents category of words, with the bar length representing the percentage of that category’s usage in the database.\nTick the Cobuild checkbox.\nBars representing the percentages for the Cobuild corpus will be added to the graph, for comparison.\nPress the Export button.\nSave and open the resulting CSV file.\nYou will see that the file contains the list of categories, with two percentages for each category, first the percentage for the LaBB-CAT data, and then the percentage for the Cobuild corpus.\n\n\n\n\nLaBB-CAT can also integrate with the IBM Watson Personality Insights service.\n(https://www.ibm.com/watson/services/personality-insights/)\nThis is a web service that, given texts, provides personality metrics on the author (or speaker) of the text.\nThe transcripts in the Demo LaBB-CAT have been processed by the Personality Insights service. The results can be listed and visualised per speaker.\n\nSelect the home link on the menu.\nClick the ‘Personality’ icon.\nYou will see a list of participants that have been analysed.\nClick on the name of a participant.\nYou will see a ‘sunburst’ style visualisation of the participant’s personality metrics.\nBelow the visualisation, the categorised metrics are listed in a table.\n\n\nIn this worksheet you have seen that:\n\nthe Statistics Layer Manager can provide a variety of summary computations,\nthe transcript list can be filtered and sorted by transcript attributes,\ntranscripts can be linked to multiple media files,\nunusually frequent or infrequent words can be identified,\nLIWC text analysis can be automatically performed, and\npersonality metrics can be obtained for participants, based on their utterances."
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html#aggregate-measures",
    "href": "worksheets/demo/6-other-processing.html#aggregate-measures",
    "title": "6 - Other Processing",
    "section": "",
    "text": "You have seen in a previous worksheet that articulation rate can be calculated over the words in individual utterances, and also over all the words uttered by each participant. There are other useful computations that can be computed over different scopes.\n\nSelect the transcripts link on the menu.\nYou may have previously noticed that the top of the page includes a form with various transcript attributes. This form allows you to both filter and sort the list of transcripts by transcript attribute values.\nFor the Word Count attribute, there are two boxes which you can use to specify a range of values.\nIn the right-hand To box, enter 1000.\nWhen the list reloads, you will see a list of all transcripts that have up to 1000 words.\nThis word count was computed by the Statistics Layer Manager, which has also been configured to compute speech duration in seconds and save the result in the “Duration” transcript attribute.\n\nAnother simply aggregate calculation is type/token ratio. The Demo LaBB-CAT has been configured to compute the type/token ratio for each participant.\n\nSelect the participants link on the menu.\nFor the corpus attribute, tick the QB option to list only participants recorded in the “Quake Box” portable recording studio.\nAt the top of the participant list, press the Export Attributes button.\nTick the Gender, Age and type/token ratio attributes.\nPress the Participant Data button.\nSave and open the resulting CSV file.\nYou will see that you have a list of their participants, with gender and age, and also a column for type/token ratio; this is the ratio expressed as a percentage."
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html#other-media",
    "href": "worksheets/demo/6-other-processing.html#other-media",
    "title": "6 - Other Processing",
    "section": "",
    "text": "The transcripts in this database each have a video and an audio file.\nHowever, some of the recordings have also been processed with a facial feature location algorithm. One of the results of this process was an annotated video; a copy of the original video, with the participant’s face located, along with various facial landmarks (position of the eyes,\nshape of mouth, etc.).\nLaBB-CAT supports having multiple media ‘track’ files for the same transcript, and for some of the transcripts, the annotated video has been uploaded as well as the original video.\n\nOn the transcripts page, list the transcripts with the Quake Face attribute set to 1 - true.\nOpen one of the listed transcripts.\n\nOn the top right of the page, by default, the original video is selected for display, but all the other media files available for the transcript are listed below the video, with a checkbox next to each.\nTick the checkbox next to the media file that ends with “…_face”. The transcript will reload and display the annotated video.\nPress Play to see the annotations (marked in green and red) change with the facial features.\nIt may be easier to see the annotations if you put the video in ‘full screen’ mode."
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html#keyness",
    "href": "worksheets/demo/6-other-processing.html#keyness",
    "title": "6 - Other Processing",
    "section": "",
    "text": "In addition to calculating word frequencies for direct analysis, frequencies can be compared to a reference corpus to calculated their ‘keyness’; a measure of whether the word is unusually frequent (a high positive keyness) or unusually infrequent (a low negative keyness).\nThe Demo LaBB-CAT has been configured to compute keyness compared to the frequencies available in the CELEX lexicon, which come from the Cobuild corpus.\n\nSelect the home link on the menu.\nClick the ‘Keyness’ icon.\nYou will see a form that allows you to search for particular spelling patterns, or export a list.\nPress the Search button without filling in the Pattern box, to list all words above the default Keyness threshold.\nA list of words will be displayed, each word with its keyness metric. The high-positive words (which are unusually frequent) are listed first, with the low-negative words (unusually infrequent) below.\n\nUnsurprisingly for this speech corpus, as compared to the mostly-written Cobuild corpus, words with high keyness include filled pauses like “um” and “ahh”, other words more likely in informal speech like “gonna” and “yeah”, topic-specific words like “earthquake” and “aftershocks”, and Canterbury place-names like “Christchurch” and “Brooklands”.\nThe Frequency Layer Manager can be configured to compute keyness of the data compared to any corpus for which you have word frequency data, or if you have several corpora within one LaBB-CAT database, each corpus can be compared to all the rest."
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html#linguistic-inquiry-and-word-count",
    "href": "worksheets/demo/6-other-processing.html#linguistic-inquiry-and-word-count",
    "title": "6 - Other Processing",
    "section": "",
    "text": "Linguistic Inquiry and Word Count (LIWC) text analysis can be done with the LIWC Layer Manager and categorised word lists.\nSee: https://liwc.wpengine.com/how-it-works/\nOr: Tausczik & Pennebaker (2010) “The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods” Journal of Language and Social Psychology 29 (1) 24-54\nLIWC involves calculating the percentage of words in different categories. Categorised word lists can be purchased from liwc.wpengine.com, or can be compiled by hand.\nLIWC text analysis has been done on the Demo LaBB-CAT database, and also on the Cobuild corpus as a comparison corpus.\n\nClick the home link on the menu.\nClick the ‘LIWC’ icon.\nYou will see a horizontal bar graph: each bar represents category of words, with the bar length representing the percentage of that category’s usage in the database.\nTick the Cobuild checkbox.\nBars representing the percentages for the Cobuild corpus will be added to the graph, for comparison.\nPress the Export button.\nSave and open the resulting CSV file.\nYou will see that the file contains the list of categories, with two percentages for each category, first the percentage for the LaBB-CAT data, and then the percentage for the Cobuild corpus."
  },
  {
    "objectID": "worksheets/demo/6-other-processing.html#personality",
    "href": "worksheets/demo/6-other-processing.html#personality",
    "title": "6 - Other Processing",
    "section": "",
    "text": "LaBB-CAT can also integrate with the IBM Watson Personality Insights service.\n(https://www.ibm.com/watson/services/personality-insights/)\nThis is a web service that, given texts, provides personality metrics on the author (or speaker) of the text.\nThe transcripts in the Demo LaBB-CAT have been processed by the Personality Insights service. The results can be listed and visualised per speaker.\n\nSelect the home link on the menu.\nClick the ‘Personality’ icon.\nYou will see a list of participants that have been analysed.\nClick on the name of a participant.\nYou will see a ‘sunburst’ style visualisation of the participant’s personality metrics.\nBelow the visualisation, the categorised metrics are listed in a table.\n\n\nIn this worksheet you have seen that:\n\nthe Statistics Layer Manager can provide a variety of summary computations,\nthe transcript list can be filtered and sorted by transcript attributes,\ntranscripts can be linked to multiple media files,\nunusually frequent or infrequent words can be identified,\nLIWC text analysis can be automatically performed, and\npersonality metrics can be obtained for participants, based on their utterances."
  },
  {
    "objectID": "worksheets/index.html",
    "href": "worksheets/index.html",
    "title": "Worksheets",
    "section": "",
    "text": "Worksheets\nHere you will find various learning resources for LaBB-CAT, including:\n\na course for learning how to use LaBB-CAT, from scratch (3x2-hour sessions)\na series of worksheets for exploring the capabilities of LaBB-CAT using a demo corpus (one 2-hour session)\na brief hands-on introduction setting up a LaBB-CAT corpus from scratch, then annotating, and exploring it (one 1-hour session)\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/course/8b-forced-alignment-mfa.html",
    "href": "worksheets/course/8b-forced-alignment-mfa.html",
    "title": "8b. MFA",
    "section": "",
    "text": "The Montreal Forced Aligner (MFA) is a 3rd-party tool developed by Michael McAuliffe and others that can use words with phonemic transcriptions, and the corresponding audio, to force-align words and phones; i.e. determine the start and end time of each speech sound within each word, and thus the start/end times of the words.\nThe annotator can work in two modes:\n\nTrain and Align - acoustic models are trained on the data you want to align, which can be in any language as long as you have a pronunciation dictionary for it.\nPre-trained Models/Dictionaries - pre-trained models and pronunciation dictionaries are supplied by the Montreal Forced Aligner and used for forced alignment. Languages for which dictionaries are available include:\n\nEnglish\nFrench\nGerman\nBrazilian Portuguese\nSpanish\nCatalan\n\n\nAs the data we have is in English, we will use the Pre-trained Models/Dictionaries approach in this exercise.\nIn this exercise you will\n\ninstall the MFA Layer Manager,\nforce-align the speech of all of the participants in your database, and\ncheck and manually correct the alignments.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this exercise, you will set up Praat Integration in your web browser. There is currently no Praat integration support for Microsoft’s Edge’ browser, so if you normally use ‘Edge’ on Windows, you may need to swap to another browser for this exercise - e.g. Google Chrome, or Mozilla Firefox.\n\n\n\n\n\n\nMFA is a 3rd-party tool (https://montrealcorpustools.github.io/Montreal-Forced-Aligner/) that LaBB-CAT integrates with via a Layer Manager module. MFA is not included as part of LaBB-CAT, and so it must be installed on the server you have installed LaBB-CAT on before you can integrate LaBB-CAT with it.\nIf MFA has not been installed already, please follow the following steps, depending on the operatings system of your LaBB-CAT server:\n\n\n\n\n\n\nLinux\n\n\n\n\n\nTo install the Montreal Forced Aligner on Linux systems for all users, so that your web server can access it if required:\n\nDownload Miniconda:\nwget https://repo.anaconda.com/miniconda/Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nStart the installer:\nsudo bash Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nWhen asked the location to install Miniconda, use:\n/opt/conda\nWhen asked whether the installer should initialize Miniconda, this is unnecessary so you can respond no\nChange ownership of the conda files):\nsudo chown -R $USERNAME:$USERNAME /opt/conda\nMake conda accessible to all users (so you web server can access MFA):\nchmod -R go-w /opt/conda\nchmod -R go+rX /opt/conda\nInstall the Montreal Forced Aligner\n/opt/conda/bin/conda create -n aligner -c conda-forge montreal-forced-aligner\n\n\n\n\n\n\n\n\n\n\nWindows\n\n\n\n\n\nTo install the Montreal Forced Aligner on Windows systems for all users, so that your web server can access it if required:\n\nDownload the Miniconda installer:   \nhttps://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\nStart the installer by double-clicking it.\nWhen asked, select the “Install for all users” option. This will install conda somewhere like\nC:\\ProgramData\\Miniconda3\nWhen asked, tick the add to PATH option.\nInstall the Montreal Forced Aligner by specifying a path to the environment\nconda create -c conda-forge -p C:\\ProgramData\\Miniconda3\\envs\\aligner montreal-forced-aligner\n\n\n\n\n\n\nIf your LaBB-CAT server is installed in a Docker Container, it can download and install Miniconda and MFA itself, as part of the process of installing the MFA Manager (below)\n\n\n\n\nOnce MFA has been installed, you have to install the MFA Manager, which is the LaBB-CAT module that provides MFA with all the data it needs, and then saves to alignments MFA produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link at the bottom.\nFind MFA Manager in the list, and press its Install button and then press Install again.\n\nAs long as MFA has been installed for all users, you should see a box that’s already filled in with the location that MFA was installed to in the Path to MFA box.\n\nIf the Path to MFA box is empty and there’s an Attempt to Install MFA button, click the button, and LaBB-CAT will try to install MFA on the server for you. This process can take a few minutes, and the Configure button will be disabled until it’s finished.\nPress Configure to continue the layer manager installation.\nYou will see a window open with some information about integrating with MFA, including the information you’ve already read above.\nNow you need to add a phrase layer for the MFA configuration:\n\nLayer ID: mfa\nType: Text\nAlignment: Intervals\nManager: MFA Manager\nGenerate: always\nDescription: MFA alignment time\n\n\n\n\n\n\n\n\nNote\n\n\n\nAfter saving the layer, the MFA configuraion form will appear.\nInitially, there will be a ‘spinner’ and the form will be disabled while LaBB-CAT is making internet requests in order to retrieve lists of available dictionaries and acoustic models.\nThis process may take a few seconds, depending on LaBB-CAT’s connection to the internet.\n\n\n\nWhen you configure the layer, set the following options:\n\nDictionary Name: english_mfa\nPretrained Acoustic Models: english_mfa\nThe rest of the options can be left as their default values.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a ‘tool tip’ that describes what the option is for.\n\n\n\nPress Set Parameters We will not click Regenerate to force-align the whole corpus just yet. We need to tweak a few other settings, and then align a single participant’s speech.\n\nWhen forced-alignment is done, the resulting aligned phones will be saved on the segment layer. By default, this has it’s type set to Phonological, which assumes that the phoneme symbols will be those defined by the CELEX DISC encoding. However, MFA uses its own phoneme symbols, which are different from the CELEX DISC ones.\nTo make later processing easier, we’re going to change the type of the segment layer to Text, and let LaBB-CAT know what the possible phoneme labels are.\n\nSelect the segment layers menu option.\nYou will see a single layer listed, called segment.\nChange the Type of the segment layer to Text.\nPress Save.\nA new icon appears with a tag  icon - if you hover the mouse over this button, you’ll see it’s for setting Valid labels.\nClick the Valid labels icon\nYou will see a page that allows you to add a list of possible labels.\nBut we don’t know what the valid labels are yet. The labels used by MFA depend on the dictionary/models selected when defining the layer. The details for all dictionaries are in MFA Models documentation.\nOpen the MFA Models documentation for english_mfa.\nYou will see a page that includes information about the dictionary including the Phones used by it.\nSelect and copy the list of phones used by the english_mfa dictionary - i.e. all of these:\na aj aw aː b bʲ c cʰ d dʒ dʲ e ej f fʲ h i iː j k kʰ l m mʲ m̩ n n̩ o ow p pʰ pʲ s t tʃ tʰ tʲ u uː v vʲ w z æ ç ð ŋ ɐ ɑ ɑː ɒ ɒː ɔ ɔj ə əw ɚ ɛ ɛː ɜ ɜː ɝ ɟ ɡ ɪ ɫ ɫ̩ ɱ ɲ ɹ ɾ ʃ ʉ ʉː ʊ ʎ ʒ ʔ θ\n\n\nBack in the Valid Labels page in LaBB-CAT, paste all of the phone symbols into the box labelled Label\n\n\n\nPress New.\nYou will see that all of the labels are separately added to the list of valid labels.\n\n\nDefining this list for the segment layer means that, when you search the segment layer for specific phones, LaBB-CAT can display a clickable list of possibilities.\n\nAt the bottom of the list, there’s a Save button. Click it to save your changes.\n\n\n\n\n\n\nSelect the participants option on the menu.\nFind the participant UC207YW and tick their checkbox.\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough we’re only going to force-align the utterances of a single speaker in this exercise, you can align the utterances of multiple speakers at once, by ticking all their checkboxes on the participants page before continuing with the next step…\n\n\n\nPress the All Utterances button above the list of participants\nPress List.\nYou will see a progress bar while all their utterances are identified. Then a results page will be displayed, listing the first 20 utterances.\nClick the Mfa button at the bottom.\nYou will see a progress bar appear, while LaBB-CAT gathers the files that MFA needs, runs MFA, and parses the resulting alignments. This will take a few minutes.\n\n\n\n\nOnce forced alignment is complete, you can inspect/correct alignments using LaBB-CAT’s integration with Praat.\n\nGo to the transcripts page and open the UC207YW.eaf transcript.\nTick both the mfa layer and the segment layer. \nYou will see which lines have been force-aligned, as they have an MFA timestamp, and have the segment layer filled in.\n\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using Praat. You can open individual utterances in Praat directly from the transcript page, but first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon  - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\nYou may be asked whether to allow the “LaBB-CAT Integration Applet” to run. If you tick the “Do not show this again” option, then this message will not appear every time you open a transcript.\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line that has been aligned, and select the Open Text Grid in Praat option on the menu.\nYou may be asked you if want to allow access to the “LaBB-CAT Integration Applet” - if so, tick “Do not show this again”, and click Allow.\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\nIf you click on a word, and hit the tab key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\n\nYou may have noticed that, each time you open an utterance in Praat, a button appears in the transcript to the left of the line, labelled Import Changes. This button allows you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nIf you feel confident using Praat, open an utterance TextGrid, adjust the alignments of the words an phones so that they’re more accurate, and then click the Import Changes button in the transcript.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments. Therefore it’s important that the changes you make are actually improvements, because HTK will never change them again.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere are some rules about what you can change: 1. You’re not allowed to add or delete words (if this is necessary, it should be done by correcting the transcript instead). 2. All the phones must be within the bounds of their own word. 3. The start of the first phone should line up with the start of the word, and the end of the last phone should line up with the end of the word. 4. You should not change the alignment of the utterance itself (which would only be possible if you select the Open Text Grid incl. ± 1 utterance in Praat option).\n\n\nIn this exercise, you have seen how MFA can be used to compute word and phone alignments automatically from your data, and when using a pronunciation dictionary and pre-trained acoustic models, the process is very straightforward.\nSuch dictionaries/models are only available for a limited number of languages, but if you have a pronunciation dictionary for the language your corpus uses, MFA can also be used to train its own acoustic models from your corpus, and then use them for forced alignment. This process involves a fair amount of careful transcription, tagging, and dictionary filling.\nPerfect automatic alignments are not guaranteed, but LaBB-CAT has a mechanism for manually correcting poor alignments."
  },
  {
    "objectID": "worksheets/course/8b-forced-alignment-mfa.html#installation",
    "href": "worksheets/course/8b-forced-alignment-mfa.html#installation",
    "title": "8b. MFA",
    "section": "",
    "text": "MFA is a 3rd-party tool (https://montrealcorpustools.github.io/Montreal-Forced-Aligner/) that LaBB-CAT integrates with via a Layer Manager module. MFA is not included as part of LaBB-CAT, and so it must be installed on the server you have installed LaBB-CAT on before you can integrate LaBB-CAT with it.\nIf MFA has not been installed already, please follow the following steps, depending on the operatings system of your LaBB-CAT server:\n\n\n\n\n\n\nLinux\n\n\n\n\n\nTo install the Montreal Forced Aligner on Linux systems for all users, so that your web server can access it if required:\n\nDownload Miniconda:\nwget https://repo.anaconda.com/miniconda/Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nStart the installer:\nsudo bash Miniconda3-py38\\_4.10.3-Linux-x86\\_64.sh\nWhen asked the location to install Miniconda, use:\n/opt/conda\nWhen asked whether the installer should initialize Miniconda, this is unnecessary so you can respond no\nChange ownership of the conda files):\nsudo chown -R $USERNAME:$USERNAME /opt/conda\nMake conda accessible to all users (so you web server can access MFA):\nchmod -R go-w /opt/conda\nchmod -R go+rX /opt/conda\nInstall the Montreal Forced Aligner\n/opt/conda/bin/conda create -n aligner -c conda-forge montreal-forced-aligner\n\n\n\n\n\n\n\n\n\n\nWindows\n\n\n\n\n\nTo install the Montreal Forced Aligner on Windows systems for all users, so that your web server can access it if required:\n\nDownload the Miniconda installer:   \nhttps://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe\nStart the installer by double-clicking it.\nWhen asked, select the “Install for all users” option. This will install conda somewhere like\nC:\\ProgramData\\Miniconda3\nWhen asked, tick the add to PATH option.\nInstall the Montreal Forced Aligner by specifying a path to the environment\nconda create -c conda-forge -p C:\\ProgramData\\Miniconda3\\envs\\aligner montreal-forced-aligner\n\n\n\n\n\n\nIf your LaBB-CAT server is installed in a Docker Container, it can download and install Miniconda and MFA itself, as part of the process of installing the MFA Manager (below)\n\n\n\n\nOnce MFA has been installed, you have to install the MFA Manager, which is the LaBB-CAT module that provides MFA with all the data it needs, and then saves to alignments MFA produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link at the bottom.\nFind MFA Manager in the list, and press its Install button and then press Install again.\n\nAs long as MFA has been installed for all users, you should see a box that’s already filled in with the location that MFA was installed to in the Path to MFA box.\n\nIf the Path to MFA box is empty and there’s an Attempt to Install MFA button, click the button, and LaBB-CAT will try to install MFA on the server for you. This process can take a few minutes, and the Configure button will be disabled until it’s finished.\nPress Configure to continue the layer manager installation.\nYou will see a window open with some information about integrating with MFA, including the information you’ve already read above.\nNow you need to add a phrase layer for the MFA configuration:\n\nLayer ID: mfa\nType: Text\nAlignment: Intervals\nManager: MFA Manager\nGenerate: always\nDescription: MFA alignment time\n\n\n\n\n\n\n\n\nNote\n\n\n\nAfter saving the layer, the MFA configuraion form will appear.\nInitially, there will be a ‘spinner’ and the form will be disabled while LaBB-CAT is making internet requests in order to retrieve lists of available dictionaries and acoustic models.\nThis process may take a few seconds, depending on LaBB-CAT’s connection to the internet.\n\n\n\nWhen you configure the layer, set the following options:\n\nDictionary Name: english_mfa\nPretrained Acoustic Models: english_mfa\nThe rest of the options can be left as their default values.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a ‘tool tip’ that describes what the option is for.\n\n\n\nPress Set Parameters We will not click Regenerate to force-align the whole corpus just yet. We need to tweak a few other settings, and then align a single participant’s speech.\n\nWhen forced-alignment is done, the resulting aligned phones will be saved on the segment layer. By default, this has it’s type set to Phonological, which assumes that the phoneme symbols will be those defined by the CELEX DISC encoding. However, MFA uses its own phoneme symbols, which are different from the CELEX DISC ones.\nTo make later processing easier, we’re going to change the type of the segment layer to Text, and let LaBB-CAT know what the possible phoneme labels are.\n\nSelect the segment layers menu option.\nYou will see a single layer listed, called segment.\nChange the Type of the segment layer to Text.\nPress Save.\nA new icon appears with a tag  icon - if you hover the mouse over this button, you’ll see it’s for setting Valid labels.\nClick the Valid labels icon\nYou will see a page that allows you to add a list of possible labels.\nBut we don’t know what the valid labels are yet. The labels used by MFA depend on the dictionary/models selected when defining the layer. The details for all dictionaries are in MFA Models documentation.\nOpen the MFA Models documentation for english_mfa.\nYou will see a page that includes information about the dictionary including the Phones used by it.\nSelect and copy the list of phones used by the english_mfa dictionary - i.e. all of these:\na aj aw aː b bʲ c cʰ d dʒ dʲ e ej f fʲ h i iː j k kʰ l m mʲ m̩ n n̩ o ow p pʰ pʲ s t tʃ tʰ tʲ u uː v vʲ w z æ ç ð ŋ ɐ ɑ ɑː ɒ ɒː ɔ ɔj ə əw ɚ ɛ ɛː ɜ ɜː ɝ ɟ ɡ ɪ ɫ ɫ̩ ɱ ɲ ɹ ɾ ʃ ʉ ʉː ʊ ʎ ʒ ʔ θ\n\n\nBack in the Valid Labels page in LaBB-CAT, paste all of the phone symbols into the box labelled Label\n\n\n\nPress New.\nYou will see that all of the labels are separately added to the list of valid labels.\n\n\nDefining this list for the segment layer means that, when you search the segment layer for specific phones, LaBB-CAT can display a clickable list of possibilities.\n\nAt the bottom of the list, there’s a Save button. Click it to save your changes."
  },
  {
    "objectID": "worksheets/course/8b-forced-alignment-mfa.html#alignment",
    "href": "worksheets/course/8b-forced-alignment-mfa.html#alignment",
    "title": "8b. MFA",
    "section": "",
    "text": "Select the participants option on the menu.\nFind the participant UC207YW and tick their checkbox.\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough we’re only going to force-align the utterances of a single speaker in this exercise, you can align the utterances of multiple speakers at once, by ticking all their checkboxes on the participants page before continuing with the next step…\n\n\n\nPress the All Utterances button above the list of participants\nPress List.\nYou will see a progress bar while all their utterances are identified. Then a results page will be displayed, listing the first 20 utterances.\nClick the Mfa button at the bottom.\nYou will see a progress bar appear, while LaBB-CAT gathers the files that MFA needs, runs MFA, and parses the resulting alignments. This will take a few minutes."
  },
  {
    "objectID": "worksheets/course/8b-forced-alignment-mfa.html#inspectioncorrection",
    "href": "worksheets/course/8b-forced-alignment-mfa.html#inspectioncorrection",
    "title": "8b. MFA",
    "section": "",
    "text": "Once forced alignment is complete, you can inspect/correct alignments using LaBB-CAT’s integration with Praat.\n\nGo to the transcripts page and open the UC207YW.eaf transcript.\nTick both the mfa layer and the segment layer. \nYou will see which lines have been force-aligned, as they have an MFA timestamp, and have the segment layer filled in.\n\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using Praat. You can open individual utterances in Praat directly from the transcript page, but first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon  - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\nYou may be asked whether to allow the “LaBB-CAT Integration Applet” to run. If you tick the “Do not show this again” option, then this message will not appear every time you open a transcript.\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line that has been aligned, and select the Open Text Grid in Praat option on the menu.\nYou may be asked you if want to allow access to the “LaBB-CAT Integration Applet” - if so, tick “Do not show this again”, and click Allow.\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\nIf you click on a word, and hit the tab key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\n\nYou may have noticed that, each time you open an utterance in Praat, a button appears in the transcript to the left of the line, labelled Import Changes. This button allows you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nIf you feel confident using Praat, open an utterance TextGrid, adjust the alignments of the words an phones so that they’re more accurate, and then click the Import Changes button in the transcript.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments. Therefore it’s important that the changes you make are actually improvements, because HTK will never change them again.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere are some rules about what you can change: 1. You’re not allowed to add or delete words (if this is necessary, it should be done by correcting the transcript instead). 2. All the phones must be within the bounds of their own word. 3. The start of the first phone should line up with the start of the word, and the end of the last phone should line up with the end of the word. 4. You should not change the alignment of the utterance itself (which would only be possible if you select the Open Text Grid incl. ± 1 utterance in Praat option).\n\n\nIn this exercise, you have seen how MFA can be used to compute word and phone alignments automatically from your data, and when using a pronunciation dictionary and pre-trained acoustic models, the process is very straightforward.\nSuch dictionaries/models are only available for a limited number of languages, but if you have a pronunciation dictionary for the language your corpus uses, MFA can also be used to train its own acoustic models from your corpus, and then use them for forced alignment. This process involves a fair amount of careful transcription, tagging, and dictionary filling.\nPerfect automatic alignments are not guaranteed, but LaBB-CAT has a mechanism for manually correcting poor alignments."
  },
  {
    "objectID": "worksheets/course/2-setting-up.html",
    "href": "worksheets/course/2-setting-up.html",
    "title": "LaBB-CAT Documentation",
    "section": "",
    "text": "In this exercise you will:\n\nDefine corpora\nDefine transcript types\nDefine speaker meta data\n\nAfter this you will have an empty LaBB-CAT database set up ready to upload transcripts into.\nNow that the software is installed, we will set up a basic structure for receiving data:\n\nOpen LaBB-CAT.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you will be running LaBB-CAT on your own computer, you will need to start LaBB-CAT, and this will open your browser on the LaBB-CAT start page.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you will have been given a link or URL to use; open your browser on that page now.\n\n\n\nThe start page has a link on it called Where do I start? - you may like to click on this link and read the first section, which explains a little about how to navigate around LaBB-CAT and where to find online help and hints. \nClick back on the start page of LaBB-CAT (the page with the Where do I start? link).\n\nNow we will set up some corpus names…\n\nOn the menu at the top, click the corpora link.\nThis page shows a list of current corpora, which only contains one corpus, called corpus.\nAbove the corpus corpus, there’s a form that you can fill in to add a new corpus. Fill in the following information:\n\nName : QB\nLanguage : English\nDescription : Quakebox recordings\n\nClick the New button to add the “QB” corpus.\nYou should see a message at the top of the page saying Record created and now the QB corpus is in the list, under the “corpus” corpus.\nAdd another corpus called UC with the description Campus recordings.\nWe won’t actually be using the corpus called corpus, so we want to delete it. To do this, click the Delete button to the right of the corpus corpus in the list.\nYou will be asked Are you sure you want to delete corpus? You are sure, so click OK.\nThe row will be deleted from the list.\n\nNow you have some corpora set up with the names you’ve provided.\nThe data we are using is a collection of stories about peoples’ experiences during the devastating earthquakes that hit the Canterbury region of New Zealand in 2010 and 2011. Some recordings are interviews, where an interviewer asks the participant questions, and others are monologues. Now we’re going to set up these two transcript types…\n\nClick on the transcript types menu option.\nYou will see a list of transcript types, although there’s currently only one type in the list, called interview.\nAbove this, fill in the empty Type box with the word: monologue\nClick the New button.\nYou will notice that now the list has two transcript types, interview and monologue. A Save button has appeared, because your changes aren’t yet saved to LaBB-CAT.\nClick the Save button.\nYou will see a message at the top saying Layer saved: transcript_type.\n\n\nNow that we have both corpora and transcript types, we’re going to set up meta-data options for the participants in our corpus…\n\nClick on the participant attributes menu option.\nYou will see a list of fields or attributes that participants (or speakers) can have. There are currently only three attributes:\n\nGender - i.e. whether the participant is female or male or something else \nBirth Year - i.e. the year the participant was born \nNotes - general arbitrary notes\n\n\nFor our corpus data, we don’t have the exact year of birth. Instead we have the age of the participant, defined by various age group categories.\n\nAs with previous pages, the headings at the top are also a form you can fill in to add a new row, which we will now fill in with the following information:\n\nAttribute ID : age_category\nType : Select (this is because we want to be able to select from a list of possible values) \nLayer Label : Age\nAccess : Public\nSearchability : Searchable (so that it appears on the search page)\nCategory : General\nDescription : Age Category\n\nPress the New button to add the attribute.\nYou will see a message saying Layer added: participant_age_category and the new attribute will now appear at the bottom of the list. Now we need to define the options for it…\n\n\n\n\n\n\n\nTip\n\n\n\n If you want more information about what each of these are for, check the online help for this page.\n\n\n\nTo the right of the Age attribute’s Category there’s an icon like a tag 🏷.\nHover your mouse over this icon to see what it does.\nPress the Valid labels icon.\nThis shows a (currently empty) list of options for the participant_age_category attribute.\nIn the blank Label box, enter: 18-25\nPress the New button to add the option.\nYou will see that “18-25” appears twice on the row that’s added:\n\nOn the left is the value that is saved in LaBB-CAT’s database.\nOn the right is an editable description that is displayed in various places in LaBB-CAT, which can be used to provide a little further explanation about the value.\n\nChange the Description to 18-25 years.\nIn the Label row at the top, enter: 26-35\nPress the New button to add the option.\nChange the Description to 26-35 years\nSimilarly, add the following age categories:\n\n36-45\n46-55\n56-65\n66-75\n76-85\n\nAdd a final option:\n\nLabel : 85+\nDescription : 85 years or more\n\nLastly, press the New button without filling in a Label to add a ‘default’ option for participants with missing data.\nPress the Save to save all your changes to LaBB-CAT.\nYou will see a message saying Layer saved: participant_age_category\nNow select the participant attributes option on the menu to return the list of all attributes.\n\n\nWe are going to add a few more attributes, but they will be ‘free text’ fields without predefined options.\n\nAdd another attribute, called ethnicity. For ‘Type’ select String, and make it Public and Searchable.\nSimilarly, add the following Public Searchable string attribute:\n\nlanguages_spoken - a list of languages they speak\n… and the following Not Searchable attributes:\ngrew_up - what country they grew up in\ngrew_up_region - what region of New Zealand they grew up in\ngrew_up_town - what town or city they grew up in\n\nLastly, as we will not be using it, delete the Birth Year attribute.\n\nNow you have an empty database for which you’ve:\n\ncreated two corpora, QB and UC,\ncreated a new transcript type, so that we can have monologues as well as interviews, and\ncreated some new attributes for participants, so we can record the ages of our speakers, their place of origin, and the languages they speak, in addition to their genders."
  },
  {
    "objectID": "worksheets/course/2-setting-up.html#setting-up",
    "href": "worksheets/course/2-setting-up.html#setting-up",
    "title": "LaBB-CAT Documentation",
    "section": "",
    "text": "In this exercise you will:\n\nDefine corpora\nDefine transcript types\nDefine speaker meta data\n\nAfter this you will have an empty LaBB-CAT database set up ready to upload transcripts into.\nNow that the software is installed, we will set up a basic structure for receiving data:\n\nOpen LaBB-CAT.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you will be running LaBB-CAT on your own computer, you will need to start LaBB-CAT, and this will open your browser on the LaBB-CAT start page.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you will have been given a link or URL to use; open your browser on that page now.\n\n\n\nThe start page has a link on it called Where do I start? - you may like to click on this link and read the first section, which explains a little about how to navigate around LaBB-CAT and where to find online help and hints. \nClick back on the start page of LaBB-CAT (the page with the Where do I start? link).\n\nNow we will set up some corpus names…\n\nOn the menu at the top, click the corpora link.\nThis page shows a list of current corpora, which only contains one corpus, called corpus.\nAbove the corpus corpus, there’s a form that you can fill in to add a new corpus. Fill in the following information:\n\nName : QB\nLanguage : English\nDescription : Quakebox recordings\n\nClick the New button to add the “QB” corpus.\nYou should see a message at the top of the page saying Record created and now the QB corpus is in the list, under the “corpus” corpus.\nAdd another corpus called UC with the description Campus recordings.\nWe won’t actually be using the corpus called corpus, so we want to delete it. To do this, click the Delete button to the right of the corpus corpus in the list.\nYou will be asked Are you sure you want to delete corpus? You are sure, so click OK.\nThe row will be deleted from the list.\n\nNow you have some corpora set up with the names you’ve provided.\nThe data we are using is a collection of stories about peoples’ experiences during the devastating earthquakes that hit the Canterbury region of New Zealand in 2010 and 2011. Some recordings are interviews, where an interviewer asks the participant questions, and others are monologues. Now we’re going to set up these two transcript types…\n\nClick on the transcript types menu option.\nYou will see a list of transcript types, although there’s currently only one type in the list, called interview.\nAbove this, fill in the empty Type box with the word: monologue\nClick the New button.\nYou will notice that now the list has two transcript types, interview and monologue. A Save button has appeared, because your changes aren’t yet saved to LaBB-CAT.\nClick the Save button.\nYou will see a message at the top saying Layer saved: transcript_type.\n\n\nNow that we have both corpora and transcript types, we’re going to set up meta-data options for the participants in our corpus…\n\nClick on the participant attributes menu option.\nYou will see a list of fields or attributes that participants (or speakers) can have. There are currently only three attributes:\n\nGender - i.e. whether the participant is female or male or something else \nBirth Year - i.e. the year the participant was born \nNotes - general arbitrary notes\n\n\nFor our corpus data, we don’t have the exact year of birth. Instead we have the age of the participant, defined by various age group categories.\n\nAs with previous pages, the headings at the top are also a form you can fill in to add a new row, which we will now fill in with the following information:\n\nAttribute ID : age_category\nType : Select (this is because we want to be able to select from a list of possible values) \nLayer Label : Age\nAccess : Public\nSearchability : Searchable (so that it appears on the search page)\nCategory : General\nDescription : Age Category\n\nPress the New button to add the attribute.\nYou will see a message saying Layer added: participant_age_category and the new attribute will now appear at the bottom of the list. Now we need to define the options for it…\n\n\n\n\n\n\n\nTip\n\n\n\n If you want more information about what each of these are for, check the online help for this page.\n\n\n\nTo the right of the Age attribute’s Category there’s an icon like a tag 🏷.\nHover your mouse over this icon to see what it does.\nPress the Valid labels icon.\nThis shows a (currently empty) list of options for the participant_age_category attribute.\nIn the blank Label box, enter: 18-25\nPress the New button to add the option.\nYou will see that “18-25” appears twice on the row that’s added:\n\nOn the left is the value that is saved in LaBB-CAT’s database.\nOn the right is an editable description that is displayed in various places in LaBB-CAT, which can be used to provide a little further explanation about the value.\n\nChange the Description to 18-25 years.\nIn the Label row at the top, enter: 26-35\nPress the New button to add the option.\nChange the Description to 26-35 years\nSimilarly, add the following age categories:\n\n36-45\n46-55\n56-65\n66-75\n76-85\n\nAdd a final option:\n\nLabel : 85+\nDescription : 85 years or more\n\nLastly, press the New button without filling in a Label to add a ‘default’ option for participants with missing data.\nPress the Save to save all your changes to LaBB-CAT.\nYou will see a message saying Layer saved: participant_age_category\nNow select the participant attributes option on the menu to return the list of all attributes.\n\n\nWe are going to add a few more attributes, but they will be ‘free text’ fields without predefined options.\n\nAdd another attribute, called ethnicity. For ‘Type’ select String, and make it Public and Searchable.\nSimilarly, add the following Public Searchable string attribute:\n\nlanguages_spoken - a list of languages they speak\n… and the following Not Searchable attributes:\ngrew_up - what country they grew up in\ngrew_up_region - what region of New Zealand they grew up in\ngrew_up_town - what town or city they grew up in\n\nLastly, as we will not be using it, delete the Birth Year attribute.\n\nNow you have an empty database for which you’ve:\n\ncreated two corpora, QB and UC,\ncreated a new transcript type, so that we can have monologues as well as interviews, and\ncreated some new attributes for participants, so we can record the ages of our speakers, their place of origin, and the languages they speak, in addition to their genders."
  },
  {
    "objectID": "worksheets/course/1-installation.html",
    "href": "worksheets/course/1-installation.html",
    "title": "LaBB-CAT Documentation",
    "section": "",
    "text": "In this exercise you will install the LaBB-CAT software.\n\n\n\n\n\n\nNote\n\n\n\nYou should only follow these steps if you will be running LaBB-CAT on your own computer.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you can skip with exercise.\n\n\nAfter this you will have an empty LaBB-CAT database set up ready to set up.\n\nYou have a file called install-labbcat.jar - double click this file to start the installer.\nIf you are using OS X, you may see a message that the file can’t be opened:\n\nIf this happens:\n\nClick the Apple icon in the top left corner of the screen.\nSelect System Preferences\nClick Security & Privacy\nNear the bottom it says “install-labbcat.jar’’ was blocked from opening because it is not from an identified developer.\n\nClick Open Anyway\nYou may see another warning about the program being downloaded from the internet\n\nClick Open\n\n\n\nClick Start\nYou will see the progress bar move as files are installed. Once this is finished, you’ll see a message saying Installation complete.\nClick Finished to close the installer.\n\nThe software is now installed. LaBB-CAT is a browser-based system, which means that it works as a mini web server on your computer, and you need to access it using your web browser.\nEach time you want to use LaBB-CAT, you must start it up, and which you’ve finished, you close it down again.\nTo start LaBB-CAT, click the LaBB-CAT icon in your applications area.\n\nOn Windows, open the Start menu and type LaBB-CAT.\nOn OS X you will find LaBB-CAT in your Applications folder.\n\nA window called “LaBB-CAT Server” will open, and after a short delay, your default web browser will open on a page called “LaBB-CAT” (The first time only, this page will initially display the LaBB-CAT licence).\n\nNow that the software is installed, we will set up a basic structure for receiving data, in the following exercise."
  },
  {
    "objectID": "worksheets/course/1-installation.html#installation",
    "href": "worksheets/course/1-installation.html#installation",
    "title": "LaBB-CAT Documentation",
    "section": "",
    "text": "In this exercise you will install the LaBB-CAT software.\n\n\n\n\n\n\nNote\n\n\n\nYou should only follow these steps if you will be running LaBB-CAT on your own computer.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you can skip with exercise.\n\n\nAfter this you will have an empty LaBB-CAT database set up ready to set up.\n\nYou have a file called install-labbcat.jar - double click this file to start the installer.\nIf you are using OS X, you may see a message that the file can’t be opened:\n\nIf this happens:\n\nClick the Apple icon in the top left corner of the screen.\nSelect System Preferences\nClick Security & Privacy\nNear the bottom it says “install-labbcat.jar’’ was blocked from opening because it is not from an identified developer.\n\nClick Open Anyway\nYou may see another warning about the program being downloaded from the internet\n\nClick Open\n\n\n\nClick Start\nYou will see the progress bar move as files are installed. Once this is finished, you’ll see a message saying Installation complete.\nClick Finished to close the installer.\n\nThe software is now installed. LaBB-CAT is a browser-based system, which means that it works as a mini web server on your computer, and you need to access it using your web browser.\nEach time you want to use LaBB-CAT, you must start it up, and which you’ve finished, you close it down again.\nTo start LaBB-CAT, click the LaBB-CAT icon in your applications area.\n\nOn Windows, open the Start menu and type LaBB-CAT.\nOn OS X you will find LaBB-CAT in your Applications folder.\n\nA window called “LaBB-CAT Server” will open, and after a short delay, your default web browser will open on a page called “LaBB-CAT” (The first time only, this page will initially display the LaBB-CAT licence).\n\nNow that the software is installed, we will set up a basic structure for receiving data, in the following exercise."
  },
  {
    "objectID": "worksheets/course/8a-forced-alignment-htk.html",
    "href": "worksheets/course/8a-forced-alignment-htk.html",
    "title": "8a. HTK",
    "section": "",
    "text": "The Hidden Markov Model Toolkit (HTK) is a speech recognition toolkit developed at Cambridge University. It is a set of programs that can be used to build speech recognition systems. Part of the process of building such systems involves force-aligning training data - i.e. automatically lining up phonemic-transcriptions of known words with the audio signal in the training recordings. LaBB-CAT takes advantage of this capability to facilitate forced-alignment for your transcripts.\nIn order to do this, HTK needs the following ingredients:\n\na set of recordings broken up into short utterances,\northographic transcriptions of each utterance, and\nphonemic transcriptions of each of the words in each utterance\n\nYou already have 1. and 2. - i.e. a set of recordings with transcripts that include the start and end times of each line.\nYou also mostly have 3. as well, if you have done a previous exercise which included generating a pronunciation layer generated using a lexicon like CELEX or the CMU Pronunciation Dictionary. However, there are words in your transcripts that aren’t in CELEX, and so we will explore some mechanisms for filling in their pronunciations.\nIn this exercise you will\n\ninstall the HTK Layer Manager,\nprovide some pronunciations that are missing,\nforce-align the speech of one of the participants in your database, and\ncheck and manually correct the alignments.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this exercise, you will set up Praat Integration in your web browser. There is currently no Praat integration support for Microsoft’s Edge’ browser, so if you normally use ‘Edge’ on Windows, you may need to swap to another browser for this exercise - e.g. Google Chrome, or Mozilla Firefox.\n\n\n\n\nHTK is not free software in the “GNU” sense - i.e. we can not distribute it with LaBB-CAT, instead you have to download it yourself from the Cambridge University website – however it is free in the “no cost” sense, you just need to register on the HTK website, and you can then download and use HTK free of charge.\n\nRegister at http://htk.eng.cam.ac.uk/register.shtml\nDownload the version of HTK that is appropriate for the computer that LaBB-CAT is installed on:\n\nFor Windows systems, there are pre-compiled .exe files that you can download.\nFor Unix-like systems including OS X, you need to download the source code, which you will then install following the provided instructions.\n\nUnzip (for Windows) or compile and install (for Unix-like systems) the downloaded files on the computer that LaBB-CAT is installed on.\n\nIf your LaBB-CAT server is installed in a Docker Container, LaBB-CAT can itself download and compile HTK (i.e. steps 2 and 3), as long has you have a username and password for the HTK website. In this case, these steps are automatically attempted when you install the HTK layer manager (below).\nNow, you have to install the HTK Layer Manager, which is the LaBB-CAT module that provides HTK with all the data it needs, and then saves to alignments HTK produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link at the bottom.\nFind HTK Manager in the list, and press its Install button, then press Install again.\nYou will see a form with boxes that may already be filled in.\nThe layer manager needs to know where the HTK programs have been saved, which is what you need to enter in the blank Path to HTK tools box.\nIf this box is not blank, it means that LaBB-CAT has already found HTK for you, so you should leave the default value already set.\n(The other two boxes you may see are for the username and password you registered on the HTK website. On some systems, if you have not already installed the HTK tools on your LaBB-CAT computer, providing the username and password allows LaBB-CAT to attempt to download and install HTK itself. This may not work in all cases, as LaBB-CAT may not have the resources or privileges it requires to compile or install software.)\nPress Configure.\nYou will see a window open with some information about the HTK Layer Manager. This page has some useful instructions on it, so keep the page open for now.\nNow you need to add a phrase layer for the HTK configuration:\n\nLayer ID: htk\nType: Text\nAlignment: Intervals\nManager: HTK Manager\nGenerate: Never (note that this may see counter-intuitive, but we will be running HTK manually rather than allowing it to run automatically when transcripts are uploaded.)\nDescription: HTK alignment time\n\nWhen you configure the layer:\n\nYour Pronunciation Layer will be the phonemes layer you created in an earlier exercise.\nTo the list of Pause Markers you should add a full-stop (period).\nThis is because the exercise transcripts use . as a ‘short-pause’ marker, not an ‘end of sentence’ marker. i.e. your Pause Markers should be:\n- .\n(a hyphen, then a space, then a full-stop/period)\nThe rest of the settings should be left with the default values.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a ‘tool tip’ that describes what the option is for.\n\n\nWe configure the layer to ‘never’ generate, because we’re going to trigger forced-alignments manually, one speaker at a time, once we’re happy that the phonemic transcriptions are in place.\n\n\n\nNow we’re going to check the situation of our pronunciations on the phonemes layer more carefully…\n\nGo to the transcripts page and open the transcript UC207YW\nTick the phonemes layer.\nHave a look through the transcript to see where the missing phonemic transcriptions are.\n\nYou’ll see they divide into various broad types:\n\nTypos like “Febuary”\nSpecialist or invented words like “tarseal”\nContractions like “me’s” and “thing’s”\nProper names like “Bealey”\nPossibly filled pauses like “um”\nHesitations and interrupted words like “exac~”, etc.\n\n\nHTK needs a phonemic transcription for every word on a line in order to force-align that line. So every line where there’s a gap on the phonemes layer would be ignored by the HTK layer manager.\n\nThere’s another problem in this transcript, which isn’t necessarily immediately obvious.\nLook for the hesitiation “w-” and the filled-pause “mm” to see if you can see what it is.\n\n‘False positives’ from the lexicon will also play havoc with forced alignment, as HTK believes what it’s told about the pronunciations given to it, and will do it’s best to find an alignment that includes every phoneme.\n\nEach of these problems needs to be addressed before we do forced alignment, although the solution for each will vary. Some involve improving the transcript, others will involve adding new words to our dictionary.\n\nFor false-positives like “w-” and “mm”, the easiest solution is to transcribe these differently. Hesitations like “w-” are discussed below. We will use “mmm” instead of “mm”.\nFor very short false-starts like “w-”, the CELEX layer manager has been built to give a helping hand. In addition to looking up phonemic transcriptions in CELEX, it will also compute them for very short tagged false-starts. The tag it recognizes is a trailing tilde ~, so we need to change “w-” to “w~” etc. Then the CELEX layer manager will append a schwa to the initial consonant, and save that as the pronunciation (i.e. /ə/).\nFor invented or misspoken words, or longer interrupted words, which we’re not likely to see again in any other transcript like, “me’s” and “exac~”, we will add a ‘pronounce’ tag in the transcript, which includes the correct pronunciation. Again, the CELEX layer manager knows to check for pronounce annotations, and uses the given phonemic transcription instead of looking up the CELEX data.\nFor proper names and contractions like “thing’s” that we’re likely to see over and over again in different transcripts, instead of tagging each one individually, we will add them to the dictionary of pronunciations that the lexicon layer manager looks up.\n\nAs you can see, the first three methods involve editing the transcript. This can be done by editing the original file in ELAN, and then re-uploading it into the database for processing.\nAlternatively, LaBB-CAT has a mechanism for editing the transcript ‘in-situ’; this doesn’t update the original file, but it’s sometimes much more convenient, and this is the method we’ll use for this exercise.\n\nClick on the word “Febuary”, and select the Edit Transcript option from the menu.\nA window will open that allows you to edit that line in the transcript.\nCorrect the spelling of the word to be “February”\nPress Update\nPress Close\nSimilarly change “w-” to be “w~” and “mm” to be “mmm”\nClick on the word “me’s”, and select the Edit Transcript option from the menu.\nIt seems unlikely that anyone else will say “me’s”, so instead of adding it to the lexicon, we’re simply going to tag this token with a pronounce tag. This is achieved by adding the pronunciation we want to tag it with in square brackets, using the DISC phoneme symbols. We have to add the pronounce tag immediately after the word, with no intervening space. (This transcription convention also works if you edit the original transcript in ELAN)\nChange the line text from “...bit of me’s a bit …” to be “…bit of me’s[miz] a bit …” instead.\nClick Update and then Close\nThe pronounce annotation you’ve just added isn’t displayed in the transcript. It’s added to the pronounce layer, which is for this type of manual pronunciation tagging.\nScroll to the top of the transcript and tick the pronounce layer so that it will be displayed.\nWhen the transcript is reloaded, you will be able to see “miz” as an annotation on “me’s”, and that this has been copied into the phonemes layer by its layer manager.\nSimilarly you should tag the word “exac~” with the pronunciation Igz{k\n\nThis method takes care of instances where the transcript is incorrect, and ‘one off’ missing pronunciations. However, for missing words that are likely to appear over and again in the corpus, including names like “Bealey”, “Wainoni”, and “Lyttleton”, and filled pauses like “mmm”, “um”, etc. it’s not efficient to tag every token. Instead, we add these to the lexicon.\n\nSelect the participants menu option.\nFind UC207YW in the list, and click on their name.\nClick the All Utterances link below their attributes.\nLeave the default selections and press List.\nThis displays a page with a list of all the speaker’s utterances, from which you can do various things with all the utterances of a particular participant in the database.\nPress the Htk button.\nYou will see a progress bar while LaBB-CAT identifies missing words. Then a page will appear that lists unknown words.\nBasically you need to fill in the boxes with the pronunciations and click Save Pronunciations.\n\n\n\n\n\n\n\nNote\n\n\n\n\nYou don’t have to fill them all in at once, you can do a few, and click Save, which will save your work and list what’s left.\nYou don’t have to fill them all in, you can leave some empty and continue with the HTK forced-alignment by clicking Start (HTK will ignore any lines where the remaining unknown words appear, but the ones you filled in will be included).\nSome of the boxes will be initially filled in with a suggestion from the lexicon layer manager - these may or may not be correct, and aren’t saved until you save them.\nThe pronunciations have to be in the ‘DISC’ format - i.e. one character per phoneme, with no spaces. There’s a ‘helper’ link on the right of each pronunciation box - if you click it, it expands into a list of clickable phonemes - just the ones that aren’t ordinary letters, and diphthongs etc.\nThe search button lets you look up the lexicon for similar words - this probably won’t help for place names, but for words like “tarseal”, you can click the lookup button, enter “tar seal” in the box as two separate words, and you’ll get back the DISC pronunciation of each word, with clickable buttons to copy the given pronunciation into the box. This is useful for digits and numbers too, which may not be in the lexicon - so for “1”, search for “one” and copy the pronunciation.\nIf you click on the word itself, the transcript for the first instance of that word is opened, in case you want to listen to it, or in case it’s actually just a typo and you want to correct the transcript.\nIf you’re using CELEX, when you specify the pronunciations, it’s recommended to put syllable separators (-) and primary stress markers (’) too - e.g. for “tarseal” you can put t#sil but it would actually be better to put t#-’sil. These markers are entered into the dictionary even though they’re stripped out for HTK, and they may come in handy later (e.g. the syllable separators are used by the CELEX layer manager to count syllables).\n\n\n\nWhen you add pronunciations this way, they’re added to the dictionary and all the instances of those words in LaBB-CAT are updated with the pronunciations - not just the participant you’re looking at, but all participants in the database. So you only have to come up with a pronunciation for each word once.\n\n\n\n\nOnce you’ve filled in all the missing pronunciations, forced alignment will start automatically. If you want to start forced alignment before you’ve entered all pronunciations, click the Start button at the bottom of the page.\n\nYou should see a progress bar while the forced alignment is running. It will take a few minutes to complete.\n\nOnce HTK has produced the word and segment alignments, it:\n\nsets the start/end times of the words on the transcript layer accordingly,\nadds new phone annotations to the segments layer with the alignments of the phones, and\nsaves a timestamp in the htk layer.\n\nWhen the layer manager has finished, you’ll see a message saying “Complete - words and phones from selected utterances are now aligned.”\n\n\n\nYou can inspect/correct alignments using LaBB-CAT’s integration with Praat.\n\nGo back to the UC207YW.eaf transcript.\nTick both the htk layer and the segments layer.\nYou will see which lines have been force-aligned, as they have an HTK timestamp, and have the segments layer filled in. If it has missed some lines, this is most likely because there is an unknown word, another speaker speaking at the same time, or possibly HTK simply failed to align the line (there are various reasons this happens, including not enough data for training, noisy recordings, inaccurate transcription, etc.).\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using Praat. You can open individual utterances in Praat directly from the transcript page, but first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon  - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\nYou may be asked whether to allow the “LaBB-CAT Integration Applet” to run. If you tick the “Do not show this again” option, then this message will not appear every time you open a transcript.\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line that has been aligned, and select the Open Text Grid in Praat option on the menu.\nYou may be asked you if want to allow access to the “LaBB-CAT Integration Applet” - if so, tick “Do not show this again”, and click Allow.\n\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\n\nIf you click on a word, and hit the tab key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\nYou may have noticed that, each time you open an utterance in Praat, a button appears in the transcript to the left of the line, labelled Import Changes. This button allows you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nIf you feel confident using Praat, open an utterance TextGrid, adjust the alignments of the words an phones so that they’re more accurate, and then click the Import Changes button in the transcript.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments. Therefore it’s important that the changes you make are actually improvements, because HTK will never change them again.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere are some rules about what you can change:\n\nYou’re not allowed to add or delete words (if this is necessary, it should be done by correcting the transcript instead).\nAll the phones must be within the bounds of their own word.\nThe start of the first phone should line up with the start of the word, and the end of the last phone should line up with the end of the word.\nYou should not change the alignment of the utterance itself (which would only be possible if you select the Open Text Grid incl. ± 1 utterance in Praat option).\n\n\n\nIn this exercise, you have seen how HTK can be used to compute word and phone alignments automatically from your data, but that there is a fair amount of careful transcription, tagging, and dictionary filling required. Even after all that work, perfect automatic alignments are not guaranteed, but LaBB-CAT has a mechanism for manually correcting poor alignments.\nAll this manual annotation and correction means a lot of work, but obviously somewhat less work than would be involved in aligning by hand the transcripts from scratch!"
  },
  {
    "objectID": "worksheets/course/8a-forced-alignment-htk.html#installation",
    "href": "worksheets/course/8a-forced-alignment-htk.html#installation",
    "title": "8a. HTK",
    "section": "",
    "text": "HTK is not free software in the “GNU” sense - i.e. we can not distribute it with LaBB-CAT, instead you have to download it yourself from the Cambridge University website – however it is free in the “no cost” sense, you just need to register on the HTK website, and you can then download and use HTK free of charge.\n\nRegister at http://htk.eng.cam.ac.uk/register.shtml\nDownload the version of HTK that is appropriate for the computer that LaBB-CAT is installed on:\n\nFor Windows systems, there are pre-compiled .exe files that you can download.\nFor Unix-like systems including OS X, you need to download the source code, which you will then install following the provided instructions.\n\nUnzip (for Windows) or compile and install (for Unix-like systems) the downloaded files on the computer that LaBB-CAT is installed on.\n\nIf your LaBB-CAT server is installed in a Docker Container, LaBB-CAT can itself download and compile HTK (i.e. steps 2 and 3), as long has you have a username and password for the HTK website. In this case, these steps are automatically attempted when you install the HTK layer manager (below).\nNow, you have to install the HTK Layer Manager, which is the LaBB-CAT module that provides HTK with all the data it needs, and then saves to alignments HTK produces back to your database.\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link at the bottom.\nFind HTK Manager in the list, and press its Install button, then press Install again.\nYou will see a form with boxes that may already be filled in.\nThe layer manager needs to know where the HTK programs have been saved, which is what you need to enter in the blank Path to HTK tools box.\nIf this box is not blank, it means that LaBB-CAT has already found HTK for you, so you should leave the default value already set.\n(The other two boxes you may see are for the username and password you registered on the HTK website. On some systems, if you have not already installed the HTK tools on your LaBB-CAT computer, providing the username and password allows LaBB-CAT to attempt to download and install HTK itself. This may not work in all cases, as LaBB-CAT may not have the resources or privileges it requires to compile or install software.)\nPress Configure.\nYou will see a window open with some information about the HTK Layer Manager. This page has some useful instructions on it, so keep the page open for now.\nNow you need to add a phrase layer for the HTK configuration:\n\nLayer ID: htk\nType: Text\nAlignment: Intervals\nManager: HTK Manager\nGenerate: Never (note that this may see counter-intuitive, but we will be running HTK manually rather than allowing it to run automatically when transcripts are uploaded.)\nDescription: HTK alignment time\n\nWhen you configure the layer:\n\nYour Pronunciation Layer will be the phonemes layer you created in an earlier exercise.\nTo the list of Pause Markers you should add a full-stop (period).\nThis is because the exercise transcripts use . as a ‘short-pause’ marker, not an ‘end of sentence’ marker. i.e. your Pause Markers should be:\n- .\n(a hyphen, then a space, then a full-stop/period)\nThe rest of the settings should be left with the default values.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a ‘tool tip’ that describes what the option is for.\n\n\nWe configure the layer to ‘never’ generate, because we’re going to trigger forced-alignments manually, one speaker at a time, once we’re happy that the phonemic transcriptions are in place."
  },
  {
    "objectID": "worksheets/course/8a-forced-alignment-htk.html#missing-pronunciations",
    "href": "worksheets/course/8a-forced-alignment-htk.html#missing-pronunciations",
    "title": "8a. HTK",
    "section": "",
    "text": "Now we’re going to check the situation of our pronunciations on the phonemes layer more carefully…\n\nGo to the transcripts page and open the transcript UC207YW\nTick the phonemes layer.\nHave a look through the transcript to see where the missing phonemic transcriptions are.\n\nYou’ll see they divide into various broad types:\n\nTypos like “Febuary”\nSpecialist or invented words like “tarseal”\nContractions like “me’s” and “thing’s”\nProper names like “Bealey”\nPossibly filled pauses like “um”\nHesitations and interrupted words like “exac~”, etc.\n\n\nHTK needs a phonemic transcription for every word on a line in order to force-align that line. So every line where there’s a gap on the phonemes layer would be ignored by the HTK layer manager.\n\nThere’s another problem in this transcript, which isn’t necessarily immediately obvious.\nLook for the hesitiation “w-” and the filled-pause “mm” to see if you can see what it is.\n\n‘False positives’ from the lexicon will also play havoc with forced alignment, as HTK believes what it’s told about the pronunciations given to it, and will do it’s best to find an alignment that includes every phoneme.\n\nEach of these problems needs to be addressed before we do forced alignment, although the solution for each will vary. Some involve improving the transcript, others will involve adding new words to our dictionary.\n\nFor false-positives like “w-” and “mm”, the easiest solution is to transcribe these differently. Hesitations like “w-” are discussed below. We will use “mmm” instead of “mm”.\nFor very short false-starts like “w-”, the CELEX layer manager has been built to give a helping hand. In addition to looking up phonemic transcriptions in CELEX, it will also compute them for very short tagged false-starts. The tag it recognizes is a trailing tilde ~, so we need to change “w-” to “w~” etc. Then the CELEX layer manager will append a schwa to the initial consonant, and save that as the pronunciation (i.e. /ə/).\nFor invented or misspoken words, or longer interrupted words, which we’re not likely to see again in any other transcript like, “me’s” and “exac~”, we will add a ‘pronounce’ tag in the transcript, which includes the correct pronunciation. Again, the CELEX layer manager knows to check for pronounce annotations, and uses the given phonemic transcription instead of looking up the CELEX data.\nFor proper names and contractions like “thing’s” that we’re likely to see over and over again in different transcripts, instead of tagging each one individually, we will add them to the dictionary of pronunciations that the lexicon layer manager looks up.\n\nAs you can see, the first three methods involve editing the transcript. This can be done by editing the original file in ELAN, and then re-uploading it into the database for processing.\nAlternatively, LaBB-CAT has a mechanism for editing the transcript ‘in-situ’; this doesn’t update the original file, but it’s sometimes much more convenient, and this is the method we’ll use for this exercise.\n\nClick on the word “Febuary”, and select the Edit Transcript option from the menu.\nA window will open that allows you to edit that line in the transcript.\nCorrect the spelling of the word to be “February”\nPress Update\nPress Close\nSimilarly change “w-” to be “w~” and “mm” to be “mmm”\nClick on the word “me’s”, and select the Edit Transcript option from the menu.\nIt seems unlikely that anyone else will say “me’s”, so instead of adding it to the lexicon, we’re simply going to tag this token with a pronounce tag. This is achieved by adding the pronunciation we want to tag it with in square brackets, using the DISC phoneme symbols. We have to add the pronounce tag immediately after the word, with no intervening space. (This transcription convention also works if you edit the original transcript in ELAN)\nChange the line text from “...bit of me’s a bit …” to be “…bit of me’s[miz] a bit …” instead.\nClick Update and then Close\nThe pronounce annotation you’ve just added isn’t displayed in the transcript. It’s added to the pronounce layer, which is for this type of manual pronunciation tagging.\nScroll to the top of the transcript and tick the pronounce layer so that it will be displayed.\nWhen the transcript is reloaded, you will be able to see “miz” as an annotation on “me’s”, and that this has been copied into the phonemes layer by its layer manager.\nSimilarly you should tag the word “exac~” with the pronunciation Igz{k\n\nThis method takes care of instances where the transcript is incorrect, and ‘one off’ missing pronunciations. However, for missing words that are likely to appear over and again in the corpus, including names like “Bealey”, “Wainoni”, and “Lyttleton”, and filled pauses like “mmm”, “um”, etc. it’s not efficient to tag every token. Instead, we add these to the lexicon.\n\nSelect the participants menu option.\nFind UC207YW in the list, and click on their name.\nClick the All Utterances link below their attributes.\nLeave the default selections and press List.\nThis displays a page with a list of all the speaker’s utterances, from which you can do various things with all the utterances of a particular participant in the database.\nPress the Htk button.\nYou will see a progress bar while LaBB-CAT identifies missing words. Then a page will appear that lists unknown words.\nBasically you need to fill in the boxes with the pronunciations and click Save Pronunciations.\n\n\n\n\n\n\n\nNote\n\n\n\n\nYou don’t have to fill them all in at once, you can do a few, and click Save, which will save your work and list what’s left.\nYou don’t have to fill them all in, you can leave some empty and continue with the HTK forced-alignment by clicking Start (HTK will ignore any lines where the remaining unknown words appear, but the ones you filled in will be included).\nSome of the boxes will be initially filled in with a suggestion from the lexicon layer manager - these may or may not be correct, and aren’t saved until you save them.\nThe pronunciations have to be in the ‘DISC’ format - i.e. one character per phoneme, with no spaces. There’s a ‘helper’ link on the right of each pronunciation box - if you click it, it expands into a list of clickable phonemes - just the ones that aren’t ordinary letters, and diphthongs etc.\nThe search button lets you look up the lexicon for similar words - this probably won’t help for place names, but for words like “tarseal”, you can click the lookup button, enter “tar seal” in the box as two separate words, and you’ll get back the DISC pronunciation of each word, with clickable buttons to copy the given pronunciation into the box. This is useful for digits and numbers too, which may not be in the lexicon - so for “1”, search for “one” and copy the pronunciation.\nIf you click on the word itself, the transcript for the first instance of that word is opened, in case you want to listen to it, or in case it’s actually just a typo and you want to correct the transcript.\nIf you’re using CELEX, when you specify the pronunciations, it’s recommended to put syllable separators (-) and primary stress markers (’) too - e.g. for “tarseal” you can put t#sil but it would actually be better to put t#-’sil. These markers are entered into the dictionary even though they’re stripped out for HTK, and they may come in handy later (e.g. the syllable separators are used by the CELEX layer manager to count syllables).\n\n\n\nWhen you add pronunciations this way, they’re added to the dictionary and all the instances of those words in LaBB-CAT are updated with the pronunciations - not just the participant you’re looking at, but all participants in the database. So you only have to come up with a pronunciation for each word once."
  },
  {
    "objectID": "worksheets/course/8a-forced-alignment-htk.html#alignment",
    "href": "worksheets/course/8a-forced-alignment-htk.html#alignment",
    "title": "8a. HTK",
    "section": "",
    "text": "Once you’ve filled in all the missing pronunciations, forced alignment will start automatically. If you want to start forced alignment before you’ve entered all pronunciations, click the Start button at the bottom of the page.\n\nYou should see a progress bar while the forced alignment is running. It will take a few minutes to complete.\n\nOnce HTK has produced the word and segment alignments, it:\n\nsets the start/end times of the words on the transcript layer accordingly,\nadds new phone annotations to the segments layer with the alignments of the phones, and\nsaves a timestamp in the htk layer.\n\nWhen the layer manager has finished, you’ll see a message saying “Complete - words and phones from selected utterances are now aligned.”"
  },
  {
    "objectID": "worksheets/course/8a-forced-alignment-htk.html#inspectioncorrection",
    "href": "worksheets/course/8a-forced-alignment-htk.html#inspectioncorrection",
    "title": "8a. HTK",
    "section": "",
    "text": "You can inspect/correct alignments using LaBB-CAT’s integration with Praat.\n\nGo back to the UC207YW.eaf transcript.\nTick both the htk layer and the segments layer.\nYou will see which lines have been force-aligned, as they have an HTK timestamp, and have the segments layer filled in. If it has missed some lines, this is most likely because there is an unknown word, another speaker speaking at the same time, or possibly HTK simply failed to align the line (there are various reasons this happens, including not enough data for training, noisy recordings, inaccurate transcription, etc.).\n\nThe interactive transcript page doesn’t show you the alignments of the words or phones, but you can see those using Praat. You can open individual utterances in Praat directly from the transcript page, but first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon  - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\nYou may be asked whether to allow the “LaBB-CAT Integration Applet” to run. If you tick the “Do not show this again” option, then this message will not appear every time you open a transcript.\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called “Praat”). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line that has been aligned, and select the Open Text Grid in Praat option on the menu.\nYou may be asked you if want to allow access to the “LaBB-CAT Integration Applet” - if so, tick “Do not show this again”, and click Allow.\n\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes the words and the segments.\n\nIf you click on a word, and hit the tab key, the word’s interval is played. Try out various words, and see what you think about how accurate HTK has been with its alignment.\nTry this out with different lines in the transcript.\nYou will see that in some cases the alignment is pretty good, and in other cases, it’s not so good. In the not-so-good cases, see if you can figure out why HTK got it wrong.\n\nYou may have noticed that, each time you open an utterance in Praat, a button appears in the transcript to the left of the line, labelled Import Changes. This button allows you to save any adjustments you might want to make to the alignments back into the LaBB-CAT database.\n\nIf you feel confident using Praat, open an utterance TextGrid, adjust the alignments of the words an phones so that they’re more accurate, and then click the Import Changes button in the transcript.\n\n\n\n\n\n\n\nWarning\n\n\n\nThese changes are flagged as manual edits, so if forced-alignment is run again, they will not be over-written with new bad alignments. Therefore it’s important that the changes you make are actually improvements, because HTK will never change them again.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere are some rules about what you can change:\n\nYou’re not allowed to add or delete words (if this is necessary, it should be done by correcting the transcript instead).\nAll the phones must be within the bounds of their own word.\nThe start of the first phone should line up with the start of the word, and the end of the last phone should line up with the end of the word.\nYou should not change the alignment of the utterance itself (which would only be possible if you select the Open Text Grid incl. ± 1 utterance in Praat option).\n\n\n\nIn this exercise, you have seen how HTK can be used to compute word and phone alignments automatically from your data, but that there is a fair amount of careful transcription, tagging, and dictionary filling required. Even after all that work, perfect automatic alignments are not guaranteed, but LaBB-CAT has a mechanism for manually correcting poor alignments.\nAll this manual annotation and correction means a lot of work, but obviously somewhat less work than would be involved in aligning by hand the transcripts from scratch!"
  },
  {
    "objectID": "worksheets/course/3-uploading-data.html",
    "href": "worksheets/course/3-uploading-data.html",
    "title": "3. Uploading Data",
    "section": "",
    "text": "3. Uploading Data\nIn this exercise you will:\n\nUpload a transcript manually\nUpload many transcripts at once using the batch uploader\nImport participant data from a CSV file\nDefine a speech elicitation task for gathering data\n\nAfter this you will have a small corpus in your LaBB-CAT database.\nBefore you start, download and unzip QuakeStories.zip so you've got the demonstration data for uploading to your corpus.\n\n\nManual Upload\n\nIn LaBB-CAT, click the upload option in the menu.\nClick the first option, upload transcripts.\nClick the left-hand Choose File button and select the file in the “QuakeStories” folder called:\n“BR178LK_MargaretSpencer.eaf”\nWhen you select a file, a new row of Choose File buttons will appear below the first. This is for adding more transcripts in the ‘episode’. An ‘episode’ is a set of transcripts that belong together because they were recorded during the same session. In our case, each recording session has only one recording.\nNext to Media on the first row, click Choose File\nEach transcript has an audio file and a video file, and you want to upload both.\nClick the file called “BR178LK_MargaretSpencer.mp4”, then hold down the Shift key on your keyboard and click the file called “BR178LK_MargaretSpencer.wav”. Then click Open.\nEnsure the Corpus option is QB\nEnsure the Type option is interview\nClick Upload\n\nEach ELAN transcript has a number of Tiers defined in it:\n\none for the participant's utterances,\nanother for an ‘interviewer’ if there is one,\none for noise annotations,\none for transcriber comments, and\none for topic annotations.\n\nEach tier must be mapped to a LaBB-CAT annotation layer.\n\nLaBB-CAT has analysed the structure of the ELAN transcript and pre-selected some default options for layer mappings. For the demo data, these defaults are correct, so you needn’t change anything.\nClick Next to continue.\nThis will display a page listing all the speakers in the transcript, so you can select which one is the ‘main participant’, which is the speaker selected by default for searches and other processing.\nEnsure that BR178LK_MargaretSpencer is ticked, and the interviewer is not ticked, and click Set Main Participants.\nThis will display a page with the name of transcript you uploaded, with an edit meta-data link, and a progress bar (which may have already finished).\nClick edit meta data\nThis will display the attributes for the transcript.\nCheck that you remembered to set Type to interview. If not, you can fix that on this page, and press the Save button that appears when you make changes.\nBelow the transcript attributes is a Participants link – click it.\nThis will list both participants in the recording, the main participant, and the interviewer.\nClick BR178LK_MargaretSpencer.\nThis will display the participant attributes we defined in an earlier exercise.\nBR178LK_MargaretSpencer is an ‘English’-speaking ‘female’ who is between ‘66 and 75 years’ old, who grew up in ‘Christchurch’, in the ‘North Canterbury’ region of ’New Zealand'.\nSet her attributes to reflect that, and click Save.\nBelow the participant attributes, there is a Transcripts link – click it.\nYou will see a list of transcripts that the speaker appears in (in this case, only one).\nEach has various icons on the right; hover your mouse over each icon, and a ‘tip’ will appear that describes what the link does.\nClick name of the BR178LK_MargaretSpencer.eaf transcript.\n\nYou will now see LaBB-CAT's ‘interactive transcript’ page for the transcript.\nAt the top there is a heading, a list of speakers, and then below this, the lines from the transcript, their speakers in the margin. This includes the words the participants utter, and also any noises, comments, and other events that were put in the transcript in ELAN.\n\nIn the top right corner are some playback controls; click the play button. You will see a shaded rectangle following the participant's speech.\nTry the other controls to see what they do.\nNow click on any word in the transcript.\nYou will see a menu appear, with options for the ‘Utterance’ (the line), and the word.\nClick the play option in the menu to see what it does.\nClick on the formats link under the title.\nYou will see a menu, which includes various formats for exporting the transcript.\nSelect ‘Plain Text Document’\nSave the resulting file on your desktop, and then open it.\nYou will see the transcript in plain-text form.\nClick the formats link, and select the ‘Praat Text Grid’ option.\nSave the resulting file on your desktop, and then open it with Praat.\nYou will see that the TextGrid has various tiers, two for full utterances (one for each speaker), and two for individual words (one for each speaker).\n(You will see that each individual word has a ‘default’ alignment - i.e. the words are evenly spread out during the duration of the line they’re in. In a later exercise we will look at ways to make these word alignments actually line up with the words in the audio signal.)\n\n\n\nBatch Upload\nIf you already have a collection of transcripts and media files (which we have for these exercises), and they are systematically organized (which they are), you may be able to save some manual uploading work by uploading them using the ‘batch upload’ utility.\n\nIn LaBB-CAT, click the upload option on the menu.\nClick the upload transcript batch link.\nThis shows a window with a large blank area in the middle with various buttons above it.\nOpen Windows Explorer or Finder, and navigate to the LaBB-CAT Workshop data folder.\nDrag the folder called “QuakeStories”, and drop it on to LaBB-CAT, on to the blank area below the buttons.\nThe previously blank area will contain a list of transcripts. Each transcript should have a value filled in for each column - Transcript, Media, Corpus, and Episode.\nMost of the transcripts are monologues, so set Type to monologue on the top left.\nOne of the transcript is highlighted in orange, and the Status says “Already exists” - this is the transcript you manually uploaded. We don’t need to upload it again, so remove it from the list by using the x button on the right hand side of that row.\nClick the Upload button above the list.\nYou will see that in the Status column, the text changes to “Uploading…” for the first transcript. The progress bar progresses, and once it's complete, the next transcript changes to “Transferring”, and so on.\n\n\n\n\n\n\n\nTip\n\n\n\nWhile the files are uploading, click  the online help link next to the upload transcript batch link you clicked above and read the conditions that must be met in order to use the batch uploader.\n\n\n\nOnce the uploader is finished, you will receive a CSV report file that lists the files you uploaded and their upload status. (If there had been any problems with the upload, the resulting error messages would be included in this report for following up.)\nYou can verify that all the transcripts are there by clicking the transcripts option on the menu in LaBB-CAT.\nYou should see a list of twenty transcripts.\nUse the Transcript box to find UC013AM_Dom.eaf (You can type just part of the name if you like)\nClick the Attributes icon for UC013AM_Dom.eaf (the one with the spanner/wrench 🔧 on it).\nChange Transcript type to interview and click Save.\nSimilarly, the following transcripts are interviews, so change their type accordingly\n\nUC215YW_DanielaMaoate-Cox.eaf\nUC226AD.eaf\n\n\n\n\nParticipant Data Import\nThe transcripts are now in the database, but the meta-data for the participants hasn't been set yet (because it’s not contained in the ELAN files). We could manually add this for each speaker, but fortunately we have it stored in a spreadsheet (actually, a CSV text file) that we can upload in one go.\n\nIn LaBB-CAT, select the upload option on the menu.\nSelect the upload participant data option.\nClick Choose File, and select the file in the LaBB-CAT Exercises data folder called participants.csv\nClick Upload\nYou will now see a list of the columns from the spreadsheet.\nFirstly, ensure that the Participant identity column is set to name. This ensures that the “name” column in the spreadsheet will be used to match names of participants in the LaBB-CAT database.\nBelow that is listed each column from the spreadsheet, with an arrow pointing to a dropdown box. The box contains various options, including each of the participant attributes set up in LaBB-CAT, an ignore option, and create a new attribute option.\nMost likely, the correct options are already selected, as we’ve already set up the correct participant attributes, but just check that they are as follows:\n\n\nThe CSV column name: → ignore because it's the Participant Identity Column identified above\nThe CSV column gender: → the Gender LaBB-CAT attribute\nThe CSV column age_category: → the Age LaBB-CAT attribute\nThe CSV column ethnicity: → the Ethnicity LaBB-CAT attribute\nThe CSV column grew_up: → the grew_up LaBB-CAT attribute\nThe CSV column grew_up_region: → the grew_up_region LaBB-CAT attribute\nThe CSV column grew_up_town: → the grew_up_town LaBB-CAT attribute\nThe CSV column languages_spoken: → the languages_spoken LaBB-CAT attribute\n\nClick import.\nYou should see a page with information about the import, including the columns that were ignored, and the number of participants that were added.\n\nTo check the participant attributes really are now set:\n\nClick the participants option on the menu. You will see a list of speakers, and page links at the bottom.\n\nThe page also includes participant attribute values where they are known.\nYou can also filter the list by these values, using the column headings above the list:\n\nUnder Gender, select the F option.\nThe page now lists only those with ‘Female’ set for the Gender attribute.\n\n\n\nElicitation Tasks\nLaBB-CAT can also make recordings of speech directly from the browser.\nLet’s suppose you want to record a number of participants reading lists of words. You can define an ‘Elicitation Task’ that includes a series of steps, one for each set of words you want participants to read.\nFirst we’re going to create a corpus to receive our recordings, and a transcript type to mark the recordings as word lists …\n\nIn LaBB-CAT, select the corpora option on the menu.\nAdd a corpus called CC with a description Canterbury Corpus.\nClick the transcript types option on the menu.\nAdd a transcript type called wordlist.\n\nNow we’ll create the elicitation task, which defines what prompts and texts the participant sees during the task.\n\nClick the elicitation tasks option on the menu. The page you see is a list of elicitation tasks defined, which is currently empty.\nFill in the blank form with the following details:\n\nID: nze-wordlist\ndescription: New Zealand English Word List\ncorpus: CC (the corpus you just created)\ntranscript type: wordlist (the transcript type you just created)\npreamble: “In this task your speech will be recorded. Please ensure you’re in a quiet place.”\nThis is the first text the participant sees when they access the task, before giving consent or going through the steps.\nconsent: “I give consent for the use of my speech data for this research.”\nThis is the text of the participant's consent for their participation and the use of their data. Before starting the task steps, they must 'sign' this consent by typing their name in a box at the bottom. The text, with their name and the date incorporated, with be made into a PDF file which is uploaded with their recordings, and is made available for them to download.\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor both the preamble and the consent form, you can format the text with bold, italic, and underlined text, etc. by using the controls above the text area.\n\n\n\n\n\n\n\n\nTip\n\n\n\n Check the online help on this page for further details about settings and important information about browser limitations.\n\n\n\nPress New to add the task.\nPress Define Steps.\n\nOn this page you are going to add steps for the task. The first step, called “Welcome”, has already been added, and we’ll use it for giving the participant some detailed instructions about what follows. We'll add a series of steps after the “Welcome” step, one for each group of words we want the participant to read.\n\nThe form you can see defines the details of the first “Welcome” step.\n Check the online help on this page for further details about this page and the options on it.\nClose the online help page to return to the “define elicitation steps” page.\nFill in the following details:\n\nShow: Always\nCountdown Seconds: 0\nTitle: Instructions\nPrompt: Please read aloud the following sets of words. Press “Next” after each set.\nElicit: Nothing\nTranscript: (leave this box blank)\nImage/Video: no image/video\n\n\n\nNext we’ll define what demographic information we will ask each participant before they start recording. In this case, we will ask for their gender and what languages they speak.\n\nClick the  button to add a new step.\nFill in the following details:\n\nShow: Always\nCountdown Seconds: 0\nTitle: Languages\nPrompt: What languages do you speak?\nElicit: Attribute Value\nAttribute: participant_languages_spoken\nImage/Video: no image/video\n\nClick the  button to add a new step\nFill in the following details:\n\nShow: Always\nCountdown Seconds: 0\nTitle: Gender\nPrompt: What is your gender?\nElicit: Attribute Value\nAttribute: participant_gender\nImage/Video: no image/video\n\n\nNow we can defined some prompts for them to read aloud.\n\nClick the  button to add a new step\nFill in the following details:\n\nShow: Always\nCountdown Seconds: 0\nTitle: (leave this box blank)\nPrompt: Please read the following aloud:\nElicit: Audio\nTranscript: 1. hit hid hint\nMax Seconds: 30\nNext Button: Shown\nImage/Video: no image/video\n\nClick the  button to add a new step\nFill in the same details as the previous step, except:\nTranscript: 2. boot booed boo tune dune\nAdd a new step for Transcript: 3. bird curt burn\nAdd a new step for Transcript: 4. bat bad back bag ban\nAdd a new step for Transcript: 5. bet bed beck beg ben\nAdd one last step, with the following details:\n\nShow: Always\nCountdown Seconds: 0\nTitle: Finished\nPrompt: Thanks for your participation!\nElicit: Nothing\nTranscript: (leave this box blank)\nImage/Video: no image/video\n\n\nThis last step is what is displayed to the participant when they’ve finished all the steps.\n\nClick Save Changes\n\n\n\nYour task is almost ready. We just need to define which options for gender they can see.\n\nSelect the elicitation tasks option on the menu.\nPress Participant Attributes.\nSelect Options\nThis displays a list of the gender options that are visible to the participant. As you can see it's currently empty. In this case, we want to display all options for them to select.\nPress Add All\nYou will see that all the options (M, F, and ‘(not specified)’) have been added to the list. If you wanted to, you could edit the “description” of the individual items (e.g. translate them to another language if your participants don’t speech English), or delete options you don’t want them to be able to select.\nPress the Delete button next to the ‘(not specified)’ option, and click OK to confirm.\nPress Save Changes.\n\nYour task is now fully defined and ready to go.\nNow you’re going to run through the elicitation task yourself …\n\nSelect the elicitation tasks option on the menu.\nPress the Elicitation Task button on the bottom right.\nYou should see a page that displays the task’s ‘preamble’ that you defined earlier.\n\n\nClick Next.\nYou should see a page that displays the task’s consent form that you defined earlier, with a box to enter your name in order to ‘sign’ the consent.\nEnter your name and click Next.\nYou will be given the chance to save your copy of the consent form.\nSave the consent form and open it to check the contents.\nClose the consent form to return to the task.\nYou should see a page with some text about enabling your microphone.\nIf you don’t, and instead see a message about your browser not being supported, this means that your web browser doesn’t support recording sound. In this case, copy the address of the page at the top, and paste it into another browser (e.g. Google Chrome or Mozilla Firefox).\nClick Next and follow the instructions.\n\nOnce you've enabled your browser for access to your microphone, you will be asked for the demographic details you defined earlier.\nAfter you enter these, the task steps will begin, and you should follow the instructions, reading the prompts aloud and clicking Next after each group of words.\nEach time somebody performs the task, they're assigned a unique Participant ID, which is linked to their demographic data and the recordings.\n\nPress the Back button on your browser to return to the define elicitation tasks page in LaBB-CAT.\nPress the participants option on the menu.\nUnder the Corpus heading, select the CC option.\nYou will see one participant; the one you just created by doing the task.\nPress the participant ID to open their attributes page, and check that the demographic information you entered has been saved.\nPress the participant ID to open their attributes page, and check You will see that the participant has five transcripts, one for each of the task steps where audio was recorded.\nPress the Transcripts link at the bottom to lilst the transcripts.\nOpen the first transcript.\nYou will see that the transcript starts with a comment, which is the prompt text you were shown during the step, and that the transcript contains one utterance.\nPlay the audio to ensure it was recorded correctly.\n(If the last transcript you looked at had video, you may need to tick the checkbox next to the “wav” option in the top right corner, in order to select audio for playback.)\n\nAlthough these ‘task step’ transcripts are very short, they behave the same as any other transcript; they can be exported, annotated, searched, etc.\n\nYou now have a small database with a number of speakers in it, so we can start creating some annotations and doing some searches ... \n \n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/course/7-lexicon.html",
    "href": "worksheets/course/7-lexicon.html",
    "title": "7. Lexicons",
    "section": "",
    "text": "7. Lexicons\nLaBB-CAT can be integrated with various lexicon to facilitate automatic tagging of word tokens.\nFor our English data, there are two main options:\n\nCELEX, a ‘British English’ lexicon which must be purchased from the LDC, and\nThe CMU Pronouncing Dictionary, an ‘American English’ lexicon that's free to download.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/course/8-forced-alignment.html",
    "href": "worksheets/course/8-forced-alignment.html",
    "title": "8. Forced Alignment",
    "section": "",
    "text": "8. Forced Alignment\nForced alignment is the process of automatically determining the start and end times of words, and the phones within each word.\nThe options for forced alignment are:\n\nHTK - the Hidden Markov Model Toolkit\nMFA - the Montreal Forced Aligner\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/course/8-forced-alignment.html#forced-alignment",
    "href": "worksheets/course/8-forced-alignment.html#forced-alignment",
    "title": "LaBB-CAT Documentation",
    "section": "",
    "text": "Forced alignment is the process of automatically determining the start and end times of words, and the phones within each word.\nThe options for forced alignment are:\n\nHTK - the Hidden Markov Model Toolkit\nMFA - the Montreal Forced Aligner"
  },
  {
    "objectID": "worksheets/course/index.html",
    "href": "worksheets/course/index.html",
    "title": "LaBB-CAT Course",
    "section": "",
    "text": "LaBB-CAT Course\nThis course is intended to teach participants how to use LaBB-CAT from scratch, including: - installing and setting up the software - uploading data - defining speech-elicitation tasks - manually annotating the transcripts - setting up automatic annotations of different types - forced alignment\nIt is designed to be taught in three two-hour sessions.\nMuch of this material is demonstrated in these YouTube videos\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/course/6-automatic-annotation.html",
    "href": "worksheets/course/6-automatic-annotation.html",
    "title": "6. Automatic Annotation",
    "section": "",
    "text": "You can configure LaBB-CAT to automatically generate annotations, using ‘layer managers’. Basically, layer managers are automatic annotation modules that take data in one annotation layer, do some kind of computation on it, and save the result to another annotation layer.\nIn this exercise, we will use the following layer managers:\n\nFrequency Layer Manager, which counts tokens of each word type, over a configurable scope.\nPorter Stemmer, which applies the Porter algorithm to word orthographies to compute word stems.\nPattern Matcher, which creates annotations based on matching regular expressions against words.\nStatistics Layer Manager, which computes aggregated information, like word count or duration, over groups of words.\n\nLaBB-CAT comes with number of layer managers pre-installed; you can see a list of installed layer managers by clicking the layer managers menu option. Other layer managers have to be manually installed.\n\nFor this exercise, we’ll pretend we’ve got a couple more mini-research projects:\n\nwe’re interested in looking at how rare or common words are in our data, and\nwe want to study ‘filled pauses’ like “um”, “ah”, etc.\n\n\n\nTo start with, we’ll simply annotate each word token in the database with the count of how many times that word appears in the database...\n\nFirst of all, create a new project called frequency, using the steps we saw before.\nSelect the word layers menu option.\nYou will see a list of word layers (including the ‘custom’ layer for the “the” project we created earlier).\nAdd a new layer, with the following settings:\n\nLayer ID: frequency\nType: Number\nAlignment: None\nManager: Frequency Layer Manager\nGenerate: Always\nProject: frequency\nDescription: Count of tokens of the same type within each corpus\nPress the New button\n\nYou will see the layer configuration form. Fill it in with the following details:\n\nSummary: Raw Count\nLayer to summarize: orthography\nScope of Summary: Corpus (leave the box next to that with the [each corpus] option selected)\nMain participants only: ticked\nParticipants: un-ticked\nFilter Layer: un-ticked\nWord pairs: un-ticked\nPause Markers: [leave this blank]\nTranscript types: un-tick wordlist (as counting word list tokens would artificially inflate frequencies of those words)\nAnnotate tokens: ticked\n\n\n\n\n\n\n\n\nNote\n\n\n\n If you want more information about what these options mean, check the online help page.\n\n\n\nPress Save\nYou will see a message asking you if you want generate the layer data now.\nPress Regenerate.\nYou will see a progress bar moving across the page while the counts are being generated. When it is finished, you will see a message saying “Layer complete…”\n\nNow each word in each transcript is annotated with the count of the number of instances of that word with the corpus of the transcript.\nTo see what that looks like…\n\nSelect the transcripts menu option.\nSelect the name of the first transcript in the list.\nAt the top of the transcript there is now a list of projects. Tick the “frequency” project.\nThis will reveal the frequency layer in the list of layers.\nTick the frequency layer.\nWhen the transcript reloads, you will see that above each word is a number. That number is the number of times that word appears in the transcript’s corpus.\ne.g. if the word “and” has 669 above it, and the transcript is in the QB corpus, that means that the word “and” appears in the QB corpus 669 times.\n\nThe newly-generated annotations are also searchable…\n\nSelect the search menu option.\nIf the frequency project and layer are not already ticked, tick them to add the frequency layer to the search matrix.\n\nIn the search matrix, you will notice that, unlike the orthography layer, which has one box for a regular expression, the frequency layer has two boxes, marked “≥” and “&lt;”. For a layer of type Number (which is what you specified above), instead of a regular expression, you can match by numeric range.\nWe want all the words that appeared only once in their corpus. Enter a number or numbers in the appropriate box (you can leave either box blank) and press Search.\nPress ▼ 20 More Matches a couple of times, to get a good idea of the range of results.\n\nThe results you see may contain words that don’t seem rare at all. That they only appear once is a product of two factors:\n\nthere isn’t that much data in our example database, and\n\nthese are counts of ‘wordforms’ - i.e. the surface spelling of the word; e.g. the word “damaging” might be quite rare, even though there are more instances of words from the same stem like “damage”, and “damaged”. This second factor will be addressed soon…\n\nYou can also extract the annotations into CSV results from other searches…\n\nOn the search page, do a search for “the” followed by a word that starts with a vowel.\nWhen the results page appears, click the ▼ button next to the CSV Export button.\nUnder the list of Word layers, tick the frequency layer.\nClick the CSV Export button.\nSave and open the resulting CSV file.\nYou will notice that in the spreadsheet there are two columns:\n\nMatch frequency: this lists the frequency of each word that matched, in order. i.e. in this case two numbers, the frequency of “the”, followed by the frequency of the word after it.\nTarget frequency: this contains a single frequency, in this case the frequency of the first word that matched a pattern - i.e. “the”\n\nAs an aside, you can also select other layers to include in the CSV file. For example, some of the transcripts include topic-tags that were made in the original ELAN transcript.\nExport your search results to CSV again, this time including the topic layer, and see what that looks like.\n\n\nThe Frequency Layer Manager also keeps a word-list with token counts for each corpus…\n\nClick the layer managers menu option.\nOn the “Frequency Layer Manager” row, click the Extensions button.\nYou will see a drop-down box with each corpus in it.\nSelect QB and click Export.\nSave and open the resulting CSV file.\nYou will see an alphabetical list of all the distinct word types in the QB corpus, and next to each, a count of the number of tokens of that type in the QB corpus.\n\n\n\n\nAs pointed out above, although the ‘wordform’ counts might be useful, it also may be useful to lump together different forms of the same stem for the counts. e.g. if there’s 1 “damaging” token, 28 “damage” token, and 18 “damaged” token, it may be useful to count these all together as 47 tokens of the same stem.\nIn order to achieve this, we first need to ‘stem’ all the words in the database - i.e. reduce all the wordforms so that tokens like “damaging”, “damage”, “damaged”, and “damages” all have the same ‘stem’ annotation. Then we can gather frequency statistics on the stems.\nThe Porter Stemmer Layer Manager is one way to achieve this. First, we need to install this layer manager (which only works on English data, so it’s not installed by default).\n\nSelect the layer managers menu option.\nNear the bottom of the page, select the List of layer managers that are not yet installed link.\nFind the “Porter Stemmer” in the list, and press its Install button, and then Install again to continue.\nAfter it is installed, a tab appears with some information about what the layer manager does. You may wish to read this page for your information. Afterwards, you can close the tab to take you back to the LaBB-CAT browser tab.\nSelect the word layers menu option.\nAdd a new layer with the following attributes:\n\nLayer ID: stem\nType: Text\nAlignment: None\nManager: Porter Stemmer\nGenerate: Always\nProject: frequency\nDescription: The stem of the word according to the Porter algorithm\n\nClick the New button.\n\nThe Porter Stemmer’s default configuration is fine for our purposes, so press Set Parameters.\nPress Regenerate.\nYou will see a progress bar, and once it’s finished, you will see a message saying “Layer complete…”\nSelect the transcripts menu option.\nClick the name of the first transcript.\nTick the stem layer we just added.\nWhen the transcript refreshes, you will see, above each word, its ‘stem’ according to the Porter algorithm.\n\nYou will notice that, although the stems are not what you might regard as being the ‘lemma’ of each word (i.e. not necessarily valid words of English in themselves), they nevertheless generally strip off plural and 3rd-person-present suffixes, such that different wordforms of the same lemma will have the same ‘stem’.\nNow that we have generated a layer of ‘stems’ for the wordforms on the orthography layer, we can generate frequency data from the stem layer as well…\n\nClick the word layers menu option.\nAdd a new layer with the following attributes:\n\nLayer ID: stemFrequency\nType: Number\nAlignment: None\nManager: Frequency Layer Manager\nProject: frequency\nDescription: Count of tokens of the same stem within each corpus\n\nPress the New button.\n\nConfigure the layer exactly as before, except this time, set the Layer to summarize: setting to the stem layer we created above. Save your settings and press Regenerate.\nThe layer will be generated.\nDo a search of all speakers, for words with a value of 1 on your new stemFrequency layer.\nYou should notice that the variety of words returned seem a little ‘rarer’ that those returned previously when you were searching the wordform frequency layer.\n\n\n\n\nWe will now create some automatic annotations of a different kind. Let’s suppose that we’re interested in ‘filled pauses’ – words like “um”, “ah”, “er”, “mmm”, etc. You can actually identify them using regular expressions…\n\nDo a search of all speakers, for the word ah. Select the no matches, only a summary of results option.\nNote the number of results you get back.\nNow do a similar search, for the pattern: a+h+ i.e. 1 or more a’s followed by one or more h’s.\nNote the number of result you get back is more than in the previous search. It turns out the transcribers, when transcribing the word “ah” weren’t entirely consistent in their spelling of that word. That’s ok, because with a little imagination, we can invent searches that will identify filled pauses like “um”, “ah”, and “mm”, even if they’ve been spelt “umm”, “ahh”, or “mmm”.\n(It turns out that there’s a good reason to prefer “mmm” over “mm”, but we’ll see that in a later exercise)\nTry out a few different searches to see if you can identify different ways that transcribers have spelt filled pauses like this.\n\nWe could annotate these as filled pauses by searching, annotating a CSV file, and uploading the CSV annotations, as we did previously. However, there is a layer manager that can do this for us, for all the existing data, and for any new transcripts that might be uploaded in the future: the “Pattern Matcher” layer manager.\n\nFirst of all, create a new project called pauses.\nNow create a new word layer, with the following attributes:\n\nLayer ID: pause\nType: Text\nAlignment: None\nManager: Pattern Matcher\nGenerate: Always\nProject: pauses\nDescription: Filled pauses annotated by regular expression\n\nPress the New button\n\nSet the Source Layer to be orthography.\nThe Destination Layer and language-related settings can be left with their default values.\n\nBelow this, there is a currently empty list of “Mappings”. We are going to add regular expressions to this list, which will identify filled pauses.\n\nOn the new empty row that’s already in the list by default, click on the box labelled “Source pattern”, and enter: u+m+\nTo the right of this, click in the “Destination Label” box and enter: um\nThis will make the layer manager find any instances of words that match the pattern “u+m+” on the orthography layer, and in each case, save the annotation”um” on our new pause layer.\nPress the + button to add a new blank row, and add another regular expression:\n\nSource Pattern: a+h+\nDestination Label: ah\n\nPress the + button again, and add another regular expression:\n\nSource Pattern: mm+\nDestination Label: mm\n\nAdd any more regular expressions you think might help identify filled pauses.\nUnder the patterns, select the option to Delete annotations in target layer whose source matches no pattern\n\n\n\n\n\n\n\nTip\n\n\n\nⓘ If you would like more information about the pattern configuration and what kinds of target annotations you can create, you will find that clicking on the brief description of the layer manager above the form expands to provide more detail.\n\n\n\nPress Set Parameters and Regenerate to generate the layer.\nYou will see a progress bar while the layer manager annotates all the filled pauses in the database.\nTo see what this looks like in a transcript, perform a search for um on your new pause layer, and click on the first match.\nYou should see that each instance of the word “um” (or its variants) has been annotated, as have instances of “ah” and “mm”.\n\nNow that these filled pauses are automatically annotated, there are various things you might do with the annotations. You could:\n\ninclude them in the context of multi-word searches, for example you might want to study the effects of a filled pause on the following or preceding word, or\nsearch for only the pauses themselves, for selected speakers, in order to study what kinds of filled pauses are used by which speakers in what contexts, what their durations are, etc.\n\n\n\n\nIn fact, we can use another layer manager to automatically count them for each speaker, and for each utterance in the transcript. In order to do this, we are going to create a ‘phrase layer’, which is a layer that can contain annotations over groups of words (as opposed to against individual words). The layer manager we will use can also annotate participants…\n\nClick the phrase layers option on the menu.\nYou will see a list of phrase layers that are already set up, including language and (named) entity.\nAdd a new layer with the following characteristics:\n\nLayer ID: pauseCount\nType: Number\nAlignment: Intervals\nManager: Statistics Layer Manager\nGenerate: Always\nProject: pauses\nDescription: Count of filled pauses, for the utterance and the speaker\n\nClick New\n\nYou will see a form for the layer’s configuration. Fill in the details as follows:\n\nLayer to summarize: pause\nStatistic: Token Count\nPattern to match: [leave this blank]\nContext: [leave this blank]\nPause Threshold: [leave this blank]\nMain-participant utterances only: ticked\nScopes: tick Utterances, and under Participants:, select the option add new attribute called pauseCount\nTranscript types: leave all the options ticked\n\n\n\n\n\n\n\n\nTip\n\n\n\n If you would like more information about what these settings and the other options do, try the online help for this page.\n\n\n\nSave the layer configuration, and then press Regenerate.\nYou will see a progress bar while the layer manager annotates all the transcripts in the database.\nTo see what this looks like in the transcripts, select the transcripts option on the menu and open the first transcript in the list.\nUnder the list of projects, if the pauses project isn’t already ticked, tick it, which will reveal the pauseCount layer in the list of layers.\nTick the pauseCount layer.\nScrolling down the transcript, you will see that, wherever there is a filled pause like “um”, the entire utterance in which it appears has a bracket across the top of the words, labelled with the number of filled-pauses that occurs in that utterance.\nScroll to the top of the transcript, and click on the name of the main participant.\nYou will see the participant’s attributes page, which now includes the participant’s pauseCount attribute.\nBoth the local utterance count, and the participant’s overall count, can also be exported to CSV search results files.\nSelect search and perform a search involving the pause layer.\nAt the bottom of the results page, click the ▼ button next to the CSV Export button, to reveal the layer options.\nUnder Participant layers tick the pauseCount attribute.\nUnder Phrase layers tick the pauseCount layer.\nPress CSV Export, and save and open the resulting file.\nYou will notice that there is a column called “participant_pauseCount” with the participant’s global count, and another called “Target pauseCount” with the local utterance count.\n\nThe Statistics Layer Manager can also incorporate time information in its computation, so it can be used to compute speech-rate. We could use it on our example database to compute words-per-minute for utterances, turns, speakers, etc.\nIf you like, you can try to figure out how to set up a “words-per-minute” layer now.\nHowever, normally speech-rate is expressed in syllables per minute. We don’t have any way to get syllable-counts for our words yet, but we will be doing that in a later exercise...\nIn this exercise, you’ve seen how layer managers can be used to compute new annotations automatically from existing annotations, e.g.\n\nWords can be tagged with their frequency in the LaBB-CAT database, or its corpora.\nWords can be tagged with their ‘stem’ using the Porter Stemmer.\nWords can be tagged with annotations on the basis of regular expressions.\nGroups of words can be tagged with aggregated information like word count or rate over time."
  },
  {
    "objectID": "worksheets/course/6-automatic-annotation.html#frequency",
    "href": "worksheets/course/6-automatic-annotation.html#frequency",
    "title": "6. Automatic Annotation",
    "section": "",
    "text": "To start with, we’ll simply annotate each word token in the database with the count of how many times that word appears in the database...\n\nFirst of all, create a new project called frequency, using the steps we saw before.\nSelect the word layers menu option.\nYou will see a list of word layers (including the ‘custom’ layer for the “the” project we created earlier).\nAdd a new layer, with the following settings:\n\nLayer ID: frequency\nType: Number\nAlignment: None\nManager: Frequency Layer Manager\nGenerate: Always\nProject: frequency\nDescription: Count of tokens of the same type within each corpus\nPress the New button\n\nYou will see the layer configuration form. Fill it in with the following details:\n\nSummary: Raw Count\nLayer to summarize: orthography\nScope of Summary: Corpus (leave the box next to that with the [each corpus] option selected)\nMain participants only: ticked\nParticipants: un-ticked\nFilter Layer: un-ticked\nWord pairs: un-ticked\nPause Markers: [leave this blank]\nTranscript types: un-tick wordlist (as counting word list tokens would artificially inflate frequencies of those words)\nAnnotate tokens: ticked\n\n\n\n\n\n\n\n\nNote\n\n\n\n If you want more information about what these options mean, check the online help page.\n\n\n\nPress Save\nYou will see a message asking you if you want generate the layer data now.\nPress Regenerate.\nYou will see a progress bar moving across the page while the counts are being generated. When it is finished, you will see a message saying “Layer complete…”\n\nNow each word in each transcript is annotated with the count of the number of instances of that word with the corpus of the transcript.\nTo see what that looks like…\n\nSelect the transcripts menu option.\nSelect the name of the first transcript in the list.\nAt the top of the transcript there is now a list of projects. Tick the “frequency” project.\nThis will reveal the frequency layer in the list of layers.\nTick the frequency layer.\nWhen the transcript reloads, you will see that above each word is a number. That number is the number of times that word appears in the transcript’s corpus.\ne.g. if the word “and” has 669 above it, and the transcript is in the QB corpus, that means that the word “and” appears in the QB corpus 669 times.\n\nThe newly-generated annotations are also searchable…\n\nSelect the search menu option.\nIf the frequency project and layer are not already ticked, tick them to add the frequency layer to the search matrix.\n\nIn the search matrix, you will notice that, unlike the orthography layer, which has one box for a regular expression, the frequency layer has two boxes, marked “≥” and “&lt;”. For a layer of type Number (which is what you specified above), instead of a regular expression, you can match by numeric range.\nWe want all the words that appeared only once in their corpus. Enter a number or numbers in the appropriate box (you can leave either box blank) and press Search.\nPress ▼ 20 More Matches a couple of times, to get a good idea of the range of results.\n\nThe results you see may contain words that don’t seem rare at all. That they only appear once is a product of two factors:\n\nthere isn’t that much data in our example database, and\n\nthese are counts of ‘wordforms’ - i.e. the surface spelling of the word; e.g. the word “damaging” might be quite rare, even though there are more instances of words from the same stem like “damage”, and “damaged”. This second factor will be addressed soon…\n\nYou can also extract the annotations into CSV results from other searches…\n\nOn the search page, do a search for “the” followed by a word that starts with a vowel.\nWhen the results page appears, click the ▼ button next to the CSV Export button.\nUnder the list of Word layers, tick the frequency layer.\nClick the CSV Export button.\nSave and open the resulting CSV file.\nYou will notice that in the spreadsheet there are two columns:\n\nMatch frequency: this lists the frequency of each word that matched, in order. i.e. in this case two numbers, the frequency of “the”, followed by the frequency of the word after it.\nTarget frequency: this contains a single frequency, in this case the frequency of the first word that matched a pattern - i.e. “the”\n\nAs an aside, you can also select other layers to include in the CSV file. For example, some of the transcripts include topic-tags that were made in the original ELAN transcript.\nExport your search results to CSV again, this time including the topic layer, and see what that looks like.\n\n\nThe Frequency Layer Manager also keeps a word-list with token counts for each corpus…\n\nClick the layer managers menu option.\nOn the “Frequency Layer Manager” row, click the Extensions button.\nYou will see a drop-down box with each corpus in it.\nSelect QB and click Export.\nSave and open the resulting CSV file.\nYou will see an alphabetical list of all the distinct word types in the QB corpus, and next to each, a count of the number of tokens of that type in the QB corpus."
  },
  {
    "objectID": "worksheets/course/6-automatic-annotation.html#porter-stemmer",
    "href": "worksheets/course/6-automatic-annotation.html#porter-stemmer",
    "title": "6. Automatic Annotation",
    "section": "",
    "text": "As pointed out above, although the ‘wordform’ counts might be useful, it also may be useful to lump together different forms of the same stem for the counts. e.g. if there’s 1 “damaging” token, 28 “damage” token, and 18 “damaged” token, it may be useful to count these all together as 47 tokens of the same stem.\nIn order to achieve this, we first need to ‘stem’ all the words in the database - i.e. reduce all the wordforms so that tokens like “damaging”, “damage”, “damaged”, and “damages” all have the same ‘stem’ annotation. Then we can gather frequency statistics on the stems.\nThe Porter Stemmer Layer Manager is one way to achieve this. First, we need to install this layer manager (which only works on English data, so it’s not installed by default).\n\nSelect the layer managers menu option.\nNear the bottom of the page, select the List of layer managers that are not yet installed link.\nFind the “Porter Stemmer” in the list, and press its Install button, and then Install again to continue.\nAfter it is installed, a tab appears with some information about what the layer manager does. You may wish to read this page for your information. Afterwards, you can close the tab to take you back to the LaBB-CAT browser tab.\nSelect the word layers menu option.\nAdd a new layer with the following attributes:\n\nLayer ID: stem\nType: Text\nAlignment: None\nManager: Porter Stemmer\nGenerate: Always\nProject: frequency\nDescription: The stem of the word according to the Porter algorithm\n\nClick the New button.\n\nThe Porter Stemmer’s default configuration is fine for our purposes, so press Set Parameters.\nPress Regenerate.\nYou will see a progress bar, and once it’s finished, you will see a message saying “Layer complete…”\nSelect the transcripts menu option.\nClick the name of the first transcript.\nTick the stem layer we just added.\nWhen the transcript refreshes, you will see, above each word, its ‘stem’ according to the Porter algorithm.\n\nYou will notice that, although the stems are not what you might regard as being the ‘lemma’ of each word (i.e. not necessarily valid words of English in themselves), they nevertheless generally strip off plural and 3rd-person-present suffixes, such that different wordforms of the same lemma will have the same ‘stem’.\nNow that we have generated a layer of ‘stems’ for the wordforms on the orthography layer, we can generate frequency data from the stem layer as well…\n\nClick the word layers menu option.\nAdd a new layer with the following attributes:\n\nLayer ID: stemFrequency\nType: Number\nAlignment: None\nManager: Frequency Layer Manager\nProject: frequency\nDescription: Count of tokens of the same stem within each corpus\n\nPress the New button.\n\nConfigure the layer exactly as before, except this time, set the Layer to summarize: setting to the stem layer we created above. Save your settings and press Regenerate.\nThe layer will be generated.\nDo a search of all speakers, for words with a value of 1 on your new stemFrequency layer.\nYou should notice that the variety of words returned seem a little ‘rarer’ that those returned previously when you were searching the wordform frequency layer."
  },
  {
    "objectID": "worksheets/course/6-automatic-annotation.html#pattern-matcher",
    "href": "worksheets/course/6-automatic-annotation.html#pattern-matcher",
    "title": "6. Automatic Annotation",
    "section": "",
    "text": "We will now create some automatic annotations of a different kind. Let’s suppose that we’re interested in ‘filled pauses’ – words like “um”, “ah”, “er”, “mmm”, etc. You can actually identify them using regular expressions…\n\nDo a search of all speakers, for the word ah. Select the no matches, only a summary of results option.\nNote the number of results you get back.\nNow do a similar search, for the pattern: a+h+ i.e. 1 or more a’s followed by one or more h’s.\nNote the number of result you get back is more than in the previous search. It turns out the transcribers, when transcribing the word “ah” weren’t entirely consistent in their spelling of that word. That’s ok, because with a little imagination, we can invent searches that will identify filled pauses like “um”, “ah”, and “mm”, even if they’ve been spelt “umm”, “ahh”, or “mmm”.\n(It turns out that there’s a good reason to prefer “mmm” over “mm”, but we’ll see that in a later exercise)\nTry out a few different searches to see if you can identify different ways that transcribers have spelt filled pauses like this.\n\nWe could annotate these as filled pauses by searching, annotating a CSV file, and uploading the CSV annotations, as we did previously. However, there is a layer manager that can do this for us, for all the existing data, and for any new transcripts that might be uploaded in the future: the “Pattern Matcher” layer manager.\n\nFirst of all, create a new project called pauses.\nNow create a new word layer, with the following attributes:\n\nLayer ID: pause\nType: Text\nAlignment: None\nManager: Pattern Matcher\nGenerate: Always\nProject: pauses\nDescription: Filled pauses annotated by regular expression\n\nPress the New button\n\nSet the Source Layer to be orthography.\nThe Destination Layer and language-related settings can be left with their default values.\n\nBelow this, there is a currently empty list of “Mappings”. We are going to add regular expressions to this list, which will identify filled pauses.\n\nOn the new empty row that’s already in the list by default, click on the box labelled “Source pattern”, and enter: u+m+\nTo the right of this, click in the “Destination Label” box and enter: um\nThis will make the layer manager find any instances of words that match the pattern “u+m+” on the orthography layer, and in each case, save the annotation”um” on our new pause layer.\nPress the + button to add a new blank row, and add another regular expression:\n\nSource Pattern: a+h+\nDestination Label: ah\n\nPress the + button again, and add another regular expression:\n\nSource Pattern: mm+\nDestination Label: mm\n\nAdd any more regular expressions you think might help identify filled pauses.\nUnder the patterns, select the option to Delete annotations in target layer whose source matches no pattern\n\n\n\n\n\n\n\nTip\n\n\n\nⓘ If you would like more information about the pattern configuration and what kinds of target annotations you can create, you will find that clicking on the brief description of the layer manager above the form expands to provide more detail.\n\n\n\nPress Set Parameters and Regenerate to generate the layer.\nYou will see a progress bar while the layer manager annotates all the filled pauses in the database.\nTo see what this looks like in a transcript, perform a search for um on your new pause layer, and click on the first match.\nYou should see that each instance of the word “um” (or its variants) has been annotated, as have instances of “ah” and “mm”.\n\nNow that these filled pauses are automatically annotated, there are various things you might do with the annotations. You could:\n\ninclude them in the context of multi-word searches, for example you might want to study the effects of a filled pause on the following or preceding word, or\nsearch for only the pauses themselves, for selected speakers, in order to study what kinds of filled pauses are used by which speakers in what contexts, what their durations are, etc."
  },
  {
    "objectID": "worksheets/course/6-automatic-annotation.html#statistics-layer-manager",
    "href": "worksheets/course/6-automatic-annotation.html#statistics-layer-manager",
    "title": "6. Automatic Annotation",
    "section": "",
    "text": "In fact, we can use another layer manager to automatically count them for each speaker, and for each utterance in the transcript. In order to do this, we are going to create a ‘phrase layer’, which is a layer that can contain annotations over groups of words (as opposed to against individual words). The layer manager we will use can also annotate participants…\n\nClick the phrase layers option on the menu.\nYou will see a list of phrase layers that are already set up, including language and (named) entity.\nAdd a new layer with the following characteristics:\n\nLayer ID: pauseCount\nType: Number\nAlignment: Intervals\nManager: Statistics Layer Manager\nGenerate: Always\nProject: pauses\nDescription: Count of filled pauses, for the utterance and the speaker\n\nClick New\n\nYou will see a form for the layer’s configuration. Fill in the details as follows:\n\nLayer to summarize: pause\nStatistic: Token Count\nPattern to match: [leave this blank]\nContext: [leave this blank]\nPause Threshold: [leave this blank]\nMain-participant utterances only: ticked\nScopes: tick Utterances, and under Participants:, select the option add new attribute called pauseCount\nTranscript types: leave all the options ticked\n\n\n\n\n\n\n\n\nTip\n\n\n\n If you would like more information about what these settings and the other options do, try the online help for this page.\n\n\n\nSave the layer configuration, and then press Regenerate.\nYou will see a progress bar while the layer manager annotates all the transcripts in the database.\nTo see what this looks like in the transcripts, select the transcripts option on the menu and open the first transcript in the list.\nUnder the list of projects, if the pauses project isn’t already ticked, tick it, which will reveal the pauseCount layer in the list of layers.\nTick the pauseCount layer.\nScrolling down the transcript, you will see that, wherever there is a filled pause like “um”, the entire utterance in which it appears has a bracket across the top of the words, labelled with the number of filled-pauses that occurs in that utterance.\nScroll to the top of the transcript, and click on the name of the main participant.\nYou will see the participant’s attributes page, which now includes the participant’s pauseCount attribute.\nBoth the local utterance count, and the participant’s overall count, can also be exported to CSV search results files.\nSelect search and perform a search involving the pause layer.\nAt the bottom of the results page, click the ▼ button next to the CSV Export button, to reveal the layer options.\nUnder Participant layers tick the pauseCount attribute.\nUnder Phrase layers tick the pauseCount layer.\nPress CSV Export, and save and open the resulting file.\nYou will notice that there is a column called “participant_pauseCount” with the participant’s global count, and another called “Target pauseCount” with the local utterance count.\n\nThe Statistics Layer Manager can also incorporate time information in its computation, so it can be used to compute speech-rate. We could use it on our example database to compute words-per-minute for utterances, turns, speakers, etc.\nIf you like, you can try to figure out how to set up a “words-per-minute” layer now.\nHowever, normally speech-rate is expressed in syllables per minute. We don’t have any way to get syllable-counts for our words yet, but we will be doing that in a later exercise...\nIn this exercise, you’ve seen how layer managers can be used to compute new annotations automatically from existing annotations, e.g.\n\nWords can be tagged with their frequency in the LaBB-CAT database, or its corpora.\nWords can be tagged with their ‘stem’ using the Porter Stemmer.\nWords can be tagged with annotations on the basis of regular expressions.\nGroups of words can be tagged with aggregated information like word count or rate over time."
  },
  {
    "objectID": "worksheets/course/7b-lexicon-cmudict.html",
    "href": "worksheets/course/7b-lexicon-cmudict.html",
    "title": "7b. CMU Pronouncing Dictionary",
    "section": "",
    "text": "7b. CMU Pronouncing Dictionary\nLaBB-CAT can be integrated with the CMU Pronouncing Dictionary, which is a free pronunciation dictionary of English maintained by the Speech Group in the School of Computer Science at Carnegie Mellon University. The pronunciations are based on ‘American English’, so are suitable for ‘American English’ recordings.\nIt can also serve as a free alternative to the CELEX lexicon (which is based on ‘British English’), for those that have not purchased CELEX, although is less ideal for ‘non-rhotic’ varieties of English.\nIn this exercise you will:\n\nInstall the CMU Pronouncing Dictionary layer manager\nUse it to create new annotations for word pronunciations\nIncorporate the new layers in more sophisticated searches\n\n\nThe first thing we’re going to do is install the CMU Dict layer manager…\n\nSelect the layer managers menu option.\nFollow the List of layer managers that are not yet installed link near the bottom.\nFind “CMU Pronouncing Dictionary” in the list, and press its Install button, Install again, and then the Configure button. You will see a progress bar while the layer manager loads the data from the dictionary file into the LaBB-CAT database. This will take a minute or so.\nOnce it’s finished, you will see a page with information about the CMU Pronouncing Dictionary layer manager.\n\nNow that we’ve installed the layer manager, we’ll create a layer that contains word pronunciations.\n\nAdd a word layer managed by the CMU Pronouncing Dictionary for word pronunciation - i.e.:\n\nLayer ID: phonemes\nType: Phonological\nAlignment: None\nManager: CMU Pronouncing Dictionary\nDescription: CMU Pronouncing Dictionary pronunciations\n...configured with the Encoding: field set to CELEX DISC, and the default values for everything else.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you’re curious about what the configuration options do, hover your mouse over each option to see a ‘tool tip’ that describes what the option is for.\n\n\n\nOnce the layer has finished generating, select the transcripts menu option, and find and open NB926_IsobelleDoig.eaf.\nTick your new phonemes layer.\nYou will see that each word is tagged with a phonemic transcription.\n\nYou will notice that the annotations are displayed using IPA symbols. However, the layer manager doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme.\nThe IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ASCII option on the layer in the transcript.\n\nSelect ASCII on the phonemes layer, to see what the layer manager is actually producing.\n\nYou may find that this is somewhat harder to read. Diphthongs are generally represented by digits, and various other characters are used to represent affricates, etc.\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in the table below), because they are what we have to use when searching on the phonemes layer, which we are going to try now.\nAs you may have seen on the layer configuration page, there is another possible representation of the pronunciations, called ‘ARPABET’; this is what is used in the original dictionary file published by CMU, and uses up to three uppercase characters per phoneme. While we’re not using ARPABET in this exercise, you can use it if you like, and the ARPABET symbols are included in the table. In the table, you will see that there are gaps where no ARPABET version of the phoneme is shown; this means that the CMU Pronouncing Dictionary contains no entries that include that phoneme.\n\n\n\n\nIPA\nDISC\nARPABET\n \n \nIPA\nDISC\nARPABET\n \n\n\np\np\nP\npat\n \nɪ\nI\nIH\nKIT\n\n\nb\nb\nB\nbad\n \nε\nE\nEH\nDRESS\n\n\nt\nt\nT\ntack\n \næ\n{\nAE\nTRAP\n\n\nd\nd\nD\ndad\n \nʌ\nV\nAH\nSTRUT\n\n\nk\nk\nK\ncad\n \nɒ\nQ\nAH\nLOT\n\n\ng\ng\nG\ngame\n \nʊ\nU\nUH\nFOOT\n\n\nŋ\nN\nNG\nbang\n \nə\n@\n[vowel ending in 0]\nanother\n\n\nm\nm\nM\nmat\n \ni:\ni\nIY\nFLEECE\n\n\nn\nn\nN\nnat\n \nα: \n#\nAA\nfather\n\n\nl\nl\nL\nlad\n \nɔ:\n$\nAO\nTHOUGHT\n\n\nr\nr\nR\nrat\n \nu:\nu\nUW\nGOOSE\n\n\nf\nf\nF\nfat\n \nɜ:\n3\nER\nNURSE\n\n\nv\nv\nV\nvat\n \neɪ\n1\nEY\nFACE\n\n\nθ\nT\nTH\nthin\n \nαɪ\n2\nAY\nPRICE\n\n\nð\nD\nDH\nthen\n \nɔɪ\n4\nOY\nCHOICE\n\n\ns\ns\nS\nsap\n \nəʊ\n5\nOW\nGOAT\n\n\nz\nz\nZ\nzap\n \nαʊ\n6\nAW\nMOUTH\n\n\n∫\nS\nSH\nsheep\n \nɪə\n7\n \nNEAR\n\n\nʒ\nZ\nZH\nmeasure\n \nεə\n8\n \nSQUARE\n\n\nj\nj\nY\nyank\n \nʊə\n9\n \nCURE\n\n\nx\nx\n \nloch\n \næ\nc\n \ntimbre\n\n\nh\nh\nHH\nhad\n \nɑ̃ː\nq\n \ndétente\n\n\nw\nw\nW\nwet\n \næ̃ː\n0\n \nlingerie\n\n\nʧ\nJ\nCH\ncheap\n \nɒ̃ː\n~\n \nbouillon\n\n\nʤ\n_\nJH\njeep\n \n \n \n \n \n\n\nŋ̩\nC\n \nbacon\n \n \n \n \n \n\n\nm̩\nF\n \nidealism\n \n \n \n \n \n\n\nn̩\nH\n \nburden\n \n \n \n \n \n\n\nl̩\nP\n \n dangle\n \n \n \n \n \n\n\n\nIn the transcript, you may notice there are gaps in the layer - i.e. words that are not tagged with a pronunciation.\nFor example, around the middle of the transcript, the word “compactums” is not tagged, because the CMU Pronouncing Dictionary has no entry for that word.\nThere are various possible solutions for this, but one is to tag word tokens with their pronunciations directly in the transcript. This has been done in the case of “compactums”; manual pronunciation tags are saved on the pronounce layer\n\nScroll to the top of the transcript, un-tick the phonemes layer and tick the pronounce layer.\nWhen the transcript re-loads to show the pronounce layer tags, find “compactums” again.\n\nYou will see it has been tagged with an annotation labelled “kəmpæktəmz”, which was manually added by the transcriber of the transcript, in the original ELAN file.\nWe want all pronunciations to be present on the phonemes layer, which is currently managed by the CMU Pronouncing Dictionary layer manager. LaBB-CAT allows layers to have more than one layer manager, however; a layer can have a main layer manager, and a number of ‘auxiliary’ managers that perform extra annotation tasks.\nWe are going to add an auxiliary layer manager to the phonemes layer, which will copy any pronounce annotations it finds to the phonemes layer. This will fill in the gaps in the CMU Pronouncing dictionary, at least for the tokens that have manual pronounce tags.\n\nSelect the word layers option on the menu.\nOn the phonemes layer row, there are a number of buttons on the right, including one with a  icon. Hover your mouse over this button to see what it does, and then click it.\nYou will see a page explaining that will copy any manually tagged pronunciations from the from the pronounce layer into the phonemes.\nClick yes to continue.\nYou will see a progress bar while the auxiliary layer manager copies the pronounce annotations to the phonemes layer.\nWhen it’s finished, select the transcripts menu option, and open NB926_IsobelleDoig.eaf again.\nTick the phonemes layer.\nFind the word “compactums” in the transcript.\n\nYou will see it now has a phonemes tag, just like the rest of the word tokens.\n\nSelect the search option from the menu.\nSearch your new phonemes layer for words that start with h\n\nYou will see that the results contain words that you might not expect, like “where”, “which” and “when”.\n\nClick one of these unexpected results, to open the transcript.   \nYou will see that, in the transcript, the pronunciation appears to start with /w/, not with /h/.\nClick on the word and select the Edit option on the menu that appears.\nNow look for the phonemes layer. You will see that, in addition to the pronunciation that starts with /w/, there’s another annotation that starts with /h/, which is invisible on the transcript.\n\n\nThese are all the possible phonemic transcriptions for the word. Only the first one is displayed in the transcript, but when you do searches, all of them are searched. This can result in unexpected matches like this, but it can be useful, as it ensures that when you search for a particular phonemic pattern, all possible tokens are returned, not just those that match on the most ‘normal’ transcription.\nNow we’re going to try to do a search for the word “the” followed by a word that starts with schwa.\n\nSelect the search option from the menu.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little « button to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it.\nFind the schwa symbol ə and click it.\nYou will see that an @ symbol appears in the box.\n@ is the DISC symbol for /ə/, so in order to search for schwa, we have to use it in our search pattern.\nWe want words that start with schwa, so type .* after the @ symbol.\nClick Search.\n\nYou will see that some of the words being matched are words that you might not normally think start with a schwa. LaBB-CAT is matching words against all their possible phonemic transcriptions, so if the CMU dictionary has multiple possible pronunciations for a word, and one of them starts with schwa, it will be matched.\nYou can check this by clicking on a match, and then clicking on the word in the transcript and selecting Edit, which displays all the annotations for the given token.\nIf you check the table above, you will see that ə has no specific representation in ARPABET. This means that no CMU Pronouncing Dictionary pronunciations include schwa explicitly. Instead, ‘unstressed’ versions of other vowels are used. For example, the word “transcription” is transcribed T R AE2 N S K R IH1 P SH AH0 N in the original dictionary file; the final vowel AH is the ‘STRUT’ vowel, and the 0 means it’s ‘unstressed’. The layer manager translates this to DISC as tr{nskrIpS@n.\nNow that we have phonemic transcripts, we can do abetter job of the search we tried in the first exercise - “the” followed by a word starting with a vowel…\n\nChange your search so that, instead of just @ at the beginning of the word, it matches any vowel.\n\nYou could use the square-brackets [] at the start of your pattern, and type all vowel symbols inside them - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the IPA helper, including all the diphthongs.\nAlternatively, you can simply click the VOWEL link in the ‘IPA helper’, which will add all the DISC vowels for you, already enclosed in square-brackets.\n\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve generated a few different layers, and have seen how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nWords which have the DRESS vowel as the second phoneme\nThe word “the” followed by a word beginning with the phoneme /k/\nWords ending with a front vowel, followed by words beginning with /p/ or /b/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/course/9-aligned-data.html",
    "href": "worksheets/course/9-aligned-data.html",
    "title": "9. Aligned Data",
    "section": "",
    "text": "In a previous exercise, we force-aligned words and their segments, and also did a little hand-correction of the alignments. Now that we’ve got some relatively reliable phone-level alignments, we’re going to explore their search and annotation possibilities.\nIn this exercise you will\n\nsee how the alignments affect speech rate computations,\nautomatically annotate pauses between words,\nsearch for tokens of individual, time-aligned segments,\ncreate time-aligned segment annotations via Praat, and\nautomatically extract features from search-result intervals using Praat\n\n\n\n\nWhen inspecting alignments on the segment layer, you will have noticed that the forced aligner introduces pauses between the words – almost always at the beginnings and ends of utterances, and also sometimes in between. These pauses will affect the speech-rate statistics that are computed by the Statistics layer manager for the speech rate layer that we set up earlier. Now that there are pauses, these are excluded (i.e. not counted as speech) for the speech-rate statistics.\nHowever, for this change to be affected, the speech rate layer needs to be regenerated.\n\nGo to the transcripts page and open the transcript we force-aligned earlier, UC207YW.eaf, if you don’t still have it open.\nTick the speechRate layer for display, and make a note of some of the speech rate annotations (which ones are there depends on what you selected as the scopes for the layer).\nAt the top of the transcript, in the formats menu, select the regenerate layers option.\nThis will display a list of managed layers.\nSelect the speechRate layer and press Regenerate.\nYou will see a progress bar while the statistics for the transcript are computed again.\nOnce that’s finished, go back to the transcript and check the speech rate annotations again. You should see that at least some of them are different.\n\nThese pauses can also be directly annotated, in case you’re interesting in finding them for analysis.\n\nAdd a new word layer with the following attributes:\n\nLayer ID: previousPause\nType: Number\nAlignment: None\nManager: Context Layer Manager\nGenerate: Always\nProject: pauses\nDescription: Length in seconds of the preceding pause\n\nConfigure the layer with the following settings:\n\nSource layer: word\nSelect the Pause Detection option\nMinimum Pause Length: unticked\nMaximum Pause Length: unticked\nOnly pauses within the same turn: unticked\nLeave the Annotate the word following the pause option selected.\n\n\n\n\n\n\n\n\nTip\n\n\n\n You may be interested in looking at the online help to find out what kinds of annotations the Context layermanager can create.\n\n\n\nPress Save and then Regenerate.\nOnce the layer is generated, go back to the UC207YW.eaf transcript and display the previousPause layer (you might have to tick the pauses project to make the layer option visible).\n(You also might want to un-tick some of the other layers, to avoid clutter.)\nYou should see that a subset of words are annotated with a number, which is the length of the pause before that word.\nOpen one or two such utterances in Praat to check that the lengths are accurate.\n\nYou may notice that pauses in the middle of utterances are always right, but the pause before the first word in the utterance seems wrong. See if you can figure out why.\n\n\n\nNow we’re going to search for some instances of vowels of interest (the FLEECE vowel in this case), and annotate them with formant measurements.\n\nFirst of all, create a new project called formants\nNow select the segment layers menu option.\n\nWe’re going to add a layer that annotates segments - i.e. individual phones within words.\n\nFill in the new-layer form at the bottom with the following details:\n\nLayer ID: F1\nType: Number\nAlignment: Instants\nManager: (don’t select any of the options, this is a manual annotation layer)\nGenerate: (not relevant as it’s not a managed layer)\nProject: formants\nDescription: First Formant\nPress New.\n\nSelect search on the menu.\nTick the segment layer.\n\nThe segments layer contains annotations at the sub-word level - i.e. there are potentially multiple annotations per word, each annotation representing a phone of the word. You will see that, as with other layers, there is a box on the segments layer for a regular expression.\nAs with other patterns in the search matrix, the pattern that you enter in the box is matched against individual annotations. So if you enter i in the in the box, it will match each FLEECE vowel segment in each word in the database.\n\n\n\n\n\n\nImportant\n\n\n\nIt’s important to realise that if you enter a pattern that would match more than a single phoneme symbol on this layer then no search results will be returned, because each annotation on this layer is only a single character long (remember the DISC encoding uses one character per phoneme).\nFor example, if you enter .*IN for your search, intending to match all words ending in “…ing”, then no results will be returned, because no single segment will ever match that pattern.\n\n\n\nWe want to search for all instances of the FLEECE vowel, so use the label selector - the button labelled « - to find the right symbol for the FLEECE vowel and click it.\nIf you used HTK for forced alignment, this will be i\nIf you used MFA for forced alignment, this will be iː\n\nIn our particular database, if there’s any annotation on the segments layer, then we can be sure that it’s been aligned at least by HTK or MFA (if not manually corrected). However, there are configurations of CELEX layers that would put unaligned annotations on that layer.\n\nTo make sure we only get words that have been aligned by HTK/MFA or manually, tick the only match words that are aligned option.\nPress Search.\nOnce the search is finished, you should notice that the only transcripts returned are those that include speakers you’ve done force-alignment on.\nClick on the first result, to open its transcript.\nScroll to the top and tick the include empty layers option.\nTick the formants project option.\nThis will reveal the (empty) F1 layer below segments in the list of layers.\nTick the F1 layer.\nThe transcript will reload to include that layer (even though it’s currently empty).\nAlso tick the segments layer if it’s currently un-ticked.\nClick on the first search result (which is highlighted in green).\nSelect the Open TextGrid in Praat option.\nThis will open the audio of the line, with a TextGrid that includes the aligned words and segments.\nThere’s also a tier for the F1 layer.\n\n\n\n\n\n\n\nTip\n\n\n\nIf there’s no F1 tier in the TextGrid, it’s because the alignment setting for the layer is set to Not aligned instead of Instants.\nYou can fix that using the segments layers option in the menu.\n\n\n\nIn Praat, find our instance of the FLEECE vowel, and click on a good point in the spectrogram for measuring it’s F1 value.\nGet the F1 value (hit the F1 key on your keyboard)\nCopy the value that is displayed on to the clipboard (i.e. select it and hit Ctrl + C on your keyboard)\nBack in the TextGrid, add a boundary on the F1 tier at that point (if it’s the fourth tier, hit Ctrl + F4 on your keyboard)\nPaste the F1 value that you copied earlier (i.e. hit Ctrl + V on your keyboard)\nNow you’ve annotated the vowel with its F1 value, and we want to save that annotation back to the LaBB-CAT database.\nClick back on the LaBB-CAT transcript window.\nClick the Import Changes button that has appeared to the left ofthe line with the first match.\nYou should see a message indicating that the annotations has been saved.\nRepeat the above steps for the next few matches in the transcript.\nOnce you’ve added at least a handful of annotations on the F1 layer, refresh the interactive transcript page (i.e. use the reload button in your browser, or the F5 key).\nYou will see that for each match you’ve annotated, the F1 value you entered appears below the corresponding word. The transcript doesn’t display the time-alignment information, but that is also stored in the database.\nOpen the TextGrid for one of the lines you’ve annotated.\nYou should see the annotation that you made is in the TextGrid, at the corresponding point in time.\n\nThese manual annotations are also searchable (so for example you could search for all the FLEECE vowels with an F1 measure within a particular range), and can be exported in CSV search results. To see that in action:\n\nRepeat the segment search we did before (i.e. all aligned FLEECE vowels).\nOnce you see the results page, click the ▼ button next to the CSV Export button link, and tick the F1 layer.\nPress CVS Export.\nSave and open the resulting file.\nYou will see that there are two columns, one called “Target F1”, which contains the annotations you have made, and the other called “Target F1 start”, which contains the time of the annotation, in seconds from the beginning of the recording.\n\n\n\n\nWe will now see that you can use Praat to automatically extract certain measurements, given start and end times from search results. In order for this to work, we first need to ensure that the LaBB-CAT server knows where Praat is installed:\n\nIn LaBB-CAT, select the system attributes menu option.\nThis shows a form with various options on it, one of which is Praat Path\nIf the Praat Path option is blank, enter the location of Praat on your LaBB-CAT server, and click Save. The setting should be the path to the folder that contains praat, e.g.\n\non Windows, this might be C:\\Program Files\\Praat\non OS X, this might be /Applications\n\n\nLet’s say we want to extract F1 and F2 from all our aligned FLEECE vowels.\nWe are going to use the CSV file you just extracted.\nIf you don’t have it any more, repeat the search and export the results to CSV.\nThe CSV file includes a column called “Target segment”, which contains the annotation that matched the pattern (in this case they will all be “i”), and columns called “Target segment start” and “Target segment end” - these are the start and end times of each matching FLEECE vowel.\nWe are going to use these start/end times to get Praat to take formant measurements for us.\n\nIn LaBB-CAT, click the upload menu option.\nClick the process with praat option.\n\n\n\n\n\n\n\nTip\n\n\n\nIf the option process with praat is not there, it means that the Praat Path attribute is not set. Go back to the steps above and ensure that Praat Path is set before continuing…\n\n\n\nClick Choose File and select the CSV results that you saved above.\nYou will see a form to fill in, and the first couple of settings (Transcript Name column and Participant column should be already filled in.\nFor the Start Time column ensure that the Target segment start option is selected.\nFor the End Time column ensure the Target segment end option is selected.\nThese two settings define the start/end times of the phone.\n\nFor some measurements you might extract from Praat, processing signal that includes surrounding context is usually a good idea. You’ll see there’s a setting for that (which you can leave at the default of 0.025s), and you will see options for various measurements.\nThe default options are for F1 and F2 only, but if you feel like getting other measurements, feel free to tick those options too. You can expand the advanced settings section by clicking the triangular bullet next to “Formants” and other measurements, which allow you to specify more detail about how Praat should do its computations. Again, feel free to look at those and try different settings.\n\nClick Process.\nYou will see a progress bar while LaBB-CAT generates a Praat scripts and runs them with Praat.\nOnce Praat has finished processing the intervals, you will get a CSV file (you might have to click the CSV file with measurements link) - save and open it.\nYou will see that it’s a copy of the CSV file you uploaded, with some extra columns added on the right.\n\nDepending on your settings, this will include at least one column per measurement you selected (the formant columns also include on that contains the time at which the measurements were taken), and a final column called “Error” which is hopefully blank, but which might contain errors reported back by Praat (e.g. if it couldn’t find the audio file or ran into any other problem during processing).\n\n\nDuring this exercise, you have seen that the inter-word pauses created by forced alignment introduce the possibility of more accurate speech-rate statistics, and can themselves be automatically annotated, in case they are of interest for search or analysis.\nYou’ve also seen that you can create manual time-aligned annotation layers, which can be used to annotate phones (they can also be used for words or spans of words, by creating word layers or phrase/span layers), and that you can also use intervals from CSV search results to extract acoustic measurements automatically using Praat.\nObviously these measurements are as reliable as the intervals themselves, so care needs to be taken to maximise the likelihood of good HTK alignments, and to check and possibly manually-correct those alignments."
  },
  {
    "objectID": "worksheets/course/9-aligned-data.html#pauses-and-speech-rate",
    "href": "worksheets/course/9-aligned-data.html#pauses-and-speech-rate",
    "title": "9. Aligned Data",
    "section": "",
    "text": "When inspecting alignments on the segment layer, you will have noticed that the forced aligner introduces pauses between the words – almost always at the beginnings and ends of utterances, and also sometimes in between. These pauses will affect the speech-rate statistics that are computed by the Statistics layer manager for the speech rate layer that we set up earlier. Now that there are pauses, these are excluded (i.e. not counted as speech) for the speech-rate statistics.\nHowever, for this change to be affected, the speech rate layer needs to be regenerated.\n\nGo to the transcripts page and open the transcript we force-aligned earlier, UC207YW.eaf, if you don’t still have it open.\nTick the speechRate layer for display, and make a note of some of the speech rate annotations (which ones are there depends on what you selected as the scopes for the layer).\nAt the top of the transcript, in the formats menu, select the regenerate layers option.\nThis will display a list of managed layers.\nSelect the speechRate layer and press Regenerate.\nYou will see a progress bar while the statistics for the transcript are computed again.\nOnce that’s finished, go back to the transcript and check the speech rate annotations again. You should see that at least some of them are different.\n\nThese pauses can also be directly annotated, in case you’re interesting in finding them for analysis.\n\nAdd a new word layer with the following attributes:\n\nLayer ID: previousPause\nType: Number\nAlignment: None\nManager: Context Layer Manager\nGenerate: Always\nProject: pauses\nDescription: Length in seconds of the preceding pause\n\nConfigure the layer with the following settings:\n\nSource layer: word\nSelect the Pause Detection option\nMinimum Pause Length: unticked\nMaximum Pause Length: unticked\nOnly pauses within the same turn: unticked\nLeave the Annotate the word following the pause option selected.\n\n\n\n\n\n\n\n\nTip\n\n\n\n You may be interested in looking at the online help to find out what kinds of annotations the Context layermanager can create.\n\n\n\nPress Save and then Regenerate.\nOnce the layer is generated, go back to the UC207YW.eaf transcript and display the previousPause layer (you might have to tick the pauses project to make the layer option visible).\n(You also might want to un-tick some of the other layers, to avoid clutter.)\nYou should see that a subset of words are annotated with a number, which is the length of the pause before that word.\nOpen one or two such utterances in Praat to check that the lengths are accurate.\n\nYou may notice that pauses in the middle of utterances are always right, but the pause before the first word in the utterance seems wrong. See if you can figure out why."
  },
  {
    "objectID": "worksheets/course/9-aligned-data.html#time-aligned-segment-annotations",
    "href": "worksheets/course/9-aligned-data.html#time-aligned-segment-annotations",
    "title": "9. Aligned Data",
    "section": "",
    "text": "Now we’re going to search for some instances of vowels of interest (the FLEECE vowel in this case), and annotate them with formant measurements.\n\nFirst of all, create a new project called formants\nNow select the segment layers menu option.\n\nWe’re going to add a layer that annotates segments - i.e. individual phones within words.\n\nFill in the new-layer form at the bottom with the following details:\n\nLayer ID: F1\nType: Number\nAlignment: Instants\nManager: (don’t select any of the options, this is a manual annotation layer)\nGenerate: (not relevant as it’s not a managed layer)\nProject: formants\nDescription: First Formant\nPress New.\n\nSelect search on the menu.\nTick the segment layer.\n\nThe segments layer contains annotations at the sub-word level - i.e. there are potentially multiple annotations per word, each annotation representing a phone of the word. You will see that, as with other layers, there is a box on the segments layer for a regular expression.\nAs with other patterns in the search matrix, the pattern that you enter in the box is matched against individual annotations. So if you enter i in the in the box, it will match each FLEECE vowel segment in each word in the database.\n\n\n\n\n\n\nImportant\n\n\n\nIt’s important to realise that if you enter a pattern that would match more than a single phoneme symbol on this layer then no search results will be returned, because each annotation on this layer is only a single character long (remember the DISC encoding uses one character per phoneme).\nFor example, if you enter .*IN for your search, intending to match all words ending in “…ing”, then no results will be returned, because no single segment will ever match that pattern.\n\n\n\nWe want to search for all instances of the FLEECE vowel, so use the label selector - the button labelled « - to find the right symbol for the FLEECE vowel and click it.\nIf you used HTK for forced alignment, this will be i\nIf you used MFA for forced alignment, this will be iː\n\nIn our particular database, if there’s any annotation on the segments layer, then we can be sure that it’s been aligned at least by HTK or MFA (if not manually corrected). However, there are configurations of CELEX layers that would put unaligned annotations on that layer.\n\nTo make sure we only get words that have been aligned by HTK/MFA or manually, tick the only match words that are aligned option.\nPress Search.\nOnce the search is finished, you should notice that the only transcripts returned are those that include speakers you’ve done force-alignment on.\nClick on the first result, to open its transcript.\nScroll to the top and tick the include empty layers option.\nTick the formants project option.\nThis will reveal the (empty) F1 layer below segments in the list of layers.\nTick the F1 layer.\nThe transcript will reload to include that layer (even though it’s currently empty).\nAlso tick the segments layer if it’s currently un-ticked.\nClick on the first search result (which is highlighted in green).\nSelect the Open TextGrid in Praat option.\nThis will open the audio of the line, with a TextGrid that includes the aligned words and segments.\nThere’s also a tier for the F1 layer.\n\n\n\n\n\n\n\nTip\n\n\n\nIf there’s no F1 tier in the TextGrid, it’s because the alignment setting for the layer is set to Not aligned instead of Instants.\nYou can fix that using the segments layers option in the menu.\n\n\n\nIn Praat, find our instance of the FLEECE vowel, and click on a good point in the spectrogram for measuring it’s F1 value.\nGet the F1 value (hit the F1 key on your keyboard)\nCopy the value that is displayed on to the clipboard (i.e. select it and hit Ctrl + C on your keyboard)\nBack in the TextGrid, add a boundary on the F1 tier at that point (if it’s the fourth tier, hit Ctrl + F4 on your keyboard)\nPaste the F1 value that you copied earlier (i.e. hit Ctrl + V on your keyboard)\nNow you’ve annotated the vowel with its F1 value, and we want to save that annotation back to the LaBB-CAT database.\nClick back on the LaBB-CAT transcript window.\nClick the Import Changes button that has appeared to the left ofthe line with the first match.\nYou should see a message indicating that the annotations has been saved.\nRepeat the above steps for the next few matches in the transcript.\nOnce you’ve added at least a handful of annotations on the F1 layer, refresh the interactive transcript page (i.e. use the reload button in your browser, or the F5 key).\nYou will see that for each match you’ve annotated, the F1 value you entered appears below the corresponding word. The transcript doesn’t display the time-alignment information, but that is also stored in the database.\nOpen the TextGrid for one of the lines you’ve annotated.\nYou should see the annotation that you made is in the TextGrid, at the corresponding point in time.\n\nThese manual annotations are also searchable (so for example you could search for all the FLEECE vowels with an F1 measure within a particular range), and can be exported in CSV search results. To see that in action:\n\nRepeat the segment search we did before (i.e. all aligned FLEECE vowels).\nOnce you see the results page, click the ▼ button next to the CSV Export button link, and tick the F1 layer.\nPress CVS Export.\nSave and open the resulting file.\nYou will see that there are two columns, one called “Target F1”, which contains the annotations you have made, and the other called “Target F1 start”, which contains the time of the annotation, in seconds from the beginning of the recording."
  },
  {
    "objectID": "worksheets/course/9-aligned-data.html#process-with-praat",
    "href": "worksheets/course/9-aligned-data.html#process-with-praat",
    "title": "9. Aligned Data",
    "section": "",
    "text": "We will now see that you can use Praat to automatically extract certain measurements, given start and end times from search results. In order for this to work, we first need to ensure that the LaBB-CAT server knows where Praat is installed:\n\nIn LaBB-CAT, select the system attributes menu option.\nThis shows a form with various options on it, one of which is Praat Path\nIf the Praat Path option is blank, enter the location of Praat on your LaBB-CAT server, and click Save. The setting should be the path to the folder that contains praat, e.g.\n\non Windows, this might be C:\\Program Files\\Praat\non OS X, this might be /Applications\n\n\nLet’s say we want to extract F1 and F2 from all our aligned FLEECE vowels.\nWe are going to use the CSV file you just extracted.\nIf you don’t have it any more, repeat the search and export the results to CSV.\nThe CSV file includes a column called “Target segment”, which contains the annotation that matched the pattern (in this case they will all be “i”), and columns called “Target segment start” and “Target segment end” - these are the start and end times of each matching FLEECE vowel.\nWe are going to use these start/end times to get Praat to take formant measurements for us.\n\nIn LaBB-CAT, click the upload menu option.\nClick the process with praat option.\n\n\n\n\n\n\n\nTip\n\n\n\nIf the option process with praat is not there, it means that the Praat Path attribute is not set. Go back to the steps above and ensure that Praat Path is set before continuing…\n\n\n\nClick Choose File and select the CSV results that you saved above.\nYou will see a form to fill in, and the first couple of settings (Transcript Name column and Participant column should be already filled in.\nFor the Start Time column ensure that the Target segment start option is selected.\nFor the End Time column ensure the Target segment end option is selected.\nThese two settings define the start/end times of the phone.\n\nFor some measurements you might extract from Praat, processing signal that includes surrounding context is usually a good idea. You’ll see there’s a setting for that (which you can leave at the default of 0.025s), and you will see options for various measurements.\nThe default options are for F1 and F2 only, but if you feel like getting other measurements, feel free to tick those options too. You can expand the advanced settings section by clicking the triangular bullet next to “Formants” and other measurements, which allow you to specify more detail about how Praat should do its computations. Again, feel free to look at those and try different settings.\n\nClick Process.\nYou will see a progress bar while LaBB-CAT generates a Praat scripts and runs them with Praat.\nOnce Praat has finished processing the intervals, you will get a CSV file (you might have to click the CSV file with measurements link) - save and open it.\nYou will see that it’s a copy of the CSV file you uploaded, with some extra columns added on the right.\n\nDepending on your settings, this will include at least one column per measurement you selected (the formant columns also include on that contains the time at which the measurements were taken), and a final column called “Error” which is hopefully blank, but which might contain errors reported back by Praat (e.g. if it couldn’t find the audio file or ran into any other problem during processing).\n\n\nDuring this exercise, you have seen that the inter-word pauses created by forced alignment introduce the possibility of more accurate speech-rate statistics, and can themselves be automatically annotated, in case they are of interest for search or analysis.\nYou’ve also seen that you can create manual time-aligned annotation layers, which can be used to annotate phones (they can also be used for words or spans of words, by creating word layers or phrase/span layers), and that you can also use intervals from CSV search results to extract acoustic measurements automatically using Praat.\nObviously these measurements are as reliable as the intervals themselves, so care needs to be taken to maximise the likelihood of good HTK alignments, and to check and possibly manually-correct those alignments."
  },
  {
    "objectID": "worksheets/course/4-searching.html",
    "href": "worksheets/course/4-searching.html",
    "title": "4. Searching",
    "section": "",
    "text": "4. Searching\nNow that you have some transcripts in your database, we’ll try out LaBB-CAT’s search functions a little.\nSearching broadly involves the following steps:\n\nSelecting participants whose utterances you want to search,\nSpecifying one or more patterns to search for, and\nExploring or extracting the search results.\n\n\nWe’ll start with a very simple search - all the instances of the word “the” uttered by monolingual English-speaking males.\n\nIn LaBB-CAT, click on the participants link on the menu.\nThis takes you to a page listing all participants, where you can filter participants by their attributes. You can see various participant attributes listed across the top of page.\nWe’re interested in male participants, so under the Gender attribute, select M.\nAfter a short delay the page will display a list of all the male participants in the database.\nWe want the participants who speak only English, so enter English under Languages Spoken\nThe page will then display a list of male participants who include English in their languages. It also includes participants who speak other languages, who we want to eliminate.\nThe Languages filter box accepts a ‘regular expression’ so if we enter ^English$ in the box, only those with English as their sole language will be listed. This is because, in regular expressions, ^ means “the beginning” and $ means “the end”, so ^English$ means, “English at the beginning, and at the end”\nClick the Layered Search button at the top of the list.\nYou will see the participants you selected listed at the top, above a list of annotation layers. Below that, there’s a “Search Matrix”, although it doesn’t look much like a matrix yet, because it’s only one layer high and one word wide…\nIn the box under the word “orthography” type the word the\n\nNow press the Search button at the bottom (or hit Enter).\nA progress bar will appear, and then shortly after that, a new tab will open, which has a list of search results in it. Your browser’s popup-blocker might prevent the results page from opening - you can fix that either by allowing the popups in your browser, or by clicking the Display results link that appears after the search finishes.\nEach match is highlighted and shown with some context (the previous word and the following word in the transcript). The amount of context is controlled by a drop-down list at the top.\nSelect 5 words to see more context around each match.\nClick on the first match.\nYou will see that the interactive transcript page opens in a new tab, with the match at the top, and highlighted. You will also see that all the other matches from the same transcript are also highlighted.\nWe’ve already seen what can be done in the interactive transcript page, so close the tab to return to the results page.\nEach result line has a ticked checkbox next to it. Scroll to the bottom of the list.\nYou’ll see that there are buttons at the bottom, which perform operations on the ticked results, including CSV Export, Utterance Export, and Audio Export.\nUn-tick the “Select all results” checkbox, and then tick a handful of results in the list.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can select a group of matches by ticking the first one, and then holding down the Shift key while ticking the last one.\n\n\n\nPress the Audio Export button.\nSave and open the resulting zip file.\nYou’ll see that the files are systematically named to include:\n\nthe name of the transcript\nthe start and end time of the extracted utterance\n\nNow go back to the results page and tick the Prefix Names checkbox.\nPress the Audio Export button again.\nSave and open the resulting zip file.\nThis time you’ll see that the files are also prefixed by the result number.\nYou may notice that there are more audio files this time; that’s because there were multiple results in the same utterance. Previously, only one copy of the utterance was exported, but this time, each match has its own copy of the utterance audio, prefixed by the result number.\nNow go back to the results page and un-tick the Prefix Names checkbox.\nClick the Utterance Export button.\nSave and open the resulting zip file.\nYou’ll see that the TextGrid names match the audio file names in the first zip file.\nOpen one of the TextGrids in Praat.\nYou’ll see that the TextGrid includes a tier named target… which indicates which token(s) in the word… tier matched the search pattern.\nBack on the results page, click the CSV Export button.\nSave the resulting file, and open it.\nYou may have to specify some import options, in which case it may be handy to know that the field separator is comma, and the fields are quoted by speech marks.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using Microsoft Excel and you find it doesn’t open all the columns correctly:\n\nCreate a new workbook in Excel.\nClick the ‘Data’ tab.\nOn the “Get External Data” ribbon click ‘From Text’.\nSelect the CSV file you downloaded.\nSelect ‘Delimited’ and click Next.\nEnsure ‘Comma’ is the only delimiter ticked and click Next.\nClick Finish and then OK.\n\n\n\nYou will see a spreadsheet with one line per selected result, and various columns containing information about the speaker, the corpus, the match line and word, and a URL to the interactive transcript for the match.\nWith this spreadsheet, you can work ‘offline’ with the results, tagging them, computing statistics in Excel, R, or any other program that can work with CSV files. We’ll look at a few more uses for the CSV results files later…\n\nClose the CSV file, and the results page, and go back to the search matrix page.\n\nWe’ve seen that you can search for exact word matches, but you can also search for patterns, using ‘regular expressions’. Now we’re going to search for words beginning with “the…”\n\nChange the orthography search text to the.* (i.e. after the word “the”, append a full-stop and an asterisk.\n\nThe full-stop means “any character at all”, and the asterisk means “zero or more of the previous thing”, so .* means “zero or more characters”.\nClick Search.\nYou will see that now the search results include the word “the” and also words like “then”, “there”, “they”, etc.\nNow go back to the search page, and change the asterisk to a plus-sign, which means “one or more of the previous thing”\n\nClick Search\nYou will see that now the search results exclude the word “the”, only including words where the initial “the...” is followed by at least one character.\nNow change your search by replacing the e in “the” with [aeiou] - so your search pattern will be:\nth[aeiou].+\nThe square-brackets mean “any one of the things inside the brackets”, so [aeiou] means “any vowel”\n\n\n\n\n\n\n\nNote\n\n\n\nWhile you are typing the regular expression, you may notice that the text goes red; this means that what’s currently in the box is not a valid regular expression. That’s fine while you’re still typing, but when you’re ready to search, if the text is red, the search will likely fail. If the regular expression text is red, you can see what the problem with it is by hovering your mouse over the red text; a ‘tip’ will appear showing an error message\n\n\n\nClick Search.\nYou will now see that the results include words like “think”, “that”, “thought”, etc.\n\n\n\n\n\n\n\nTip\n\n\n\n You can get more information about regular expressions by using the online help on the search page, and also by clicking the the regular expressions link above the search matrix.\n\n\n\nUp until now, we’ve only been matching against one word at a time. Now we’re going to include patterns for a chain of words…\n\n\nOn the search page, to the right of the search matrix, there’s a + button. Click it.\n    \nNow you will see that our search matrix is one layer high by two words wide.\nChange the entries on the orthography layer so that it will match the word “the” followed immediately by a word that starts with a vowel, and click Search.\nCheck the search results are giving you what you expected.\nNow search for “the” followed, within two words, by a word that starts with a vowel.\nDream up some other searches that interest you, and try out other options on the search page.\n\n\n\n\n\n\n\nTip\n\n\n\n If in doubt about a search option, try the online help page.\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html",
    "href": "worksheets/course/7a-lexicon-celex.html",
    "title": "7a. CELEX",
    "section": "",
    "text": "LaBB-CAT can be integrated with the CELEX lexicon, which can be purchased from the Linguistic Data Consortium (LDC) and includes lemma, part of speech, morphological, phonological, and frequency information for English, German, and Dutch.\n(If you don’t have CELEX, there is an alternative version of this exercise that uses the free ‘CMU Pronunouncing Dictionary’ lexicon, which you can work through instead.)\nIn this exercise you will:\n\nInstall the CELEX layer manager\nUse it to create new annotations for word morphology, syntactic category, and phonology\nCompute speech rate in syllables per minute\nIncorporate the new layers in more sophisticated searches\n\n\n\nThe first thing we’re going to do is install the CELEX layer manager.\nThis requires having the LDC’s CELEX data files on your computer, which will be processed by the layer manager in order to insert the data into the LaBB-CAT database. If you received the CELEX files in a ZIP file, you need to unzip that into a folder, and remember the location of that folder, as you’ll need it during the installation process.\n\nFirst of all, create a new project called CELEX with a description: CELEX Annotations\nClick the layer managers menu option.\nClick the List of layer managers that are not yet installed link near the bottom.\nFind “CELEX English” in the list, and click its Install button.\nYou will see a form asking for various details. You can leave most of these with their default values. The one exception is the CELEX  ENGLISH data folder option.\nSet the CELEX ENGLISH data folder parameter to the directory path that leads to the CELEX files on your LaBB-CAT computer.\nClick Install.\nYou will see a progress bar while the layer manager loads the data from the CELEX files into the LaBB-CAT database. This will take a few minutes.\nOnce it’s finished, you will see a new window open with information about the CELEX English layer manager.\nReading this information page, you will see some instructions on how to create CELEX annotation layers - leave this tab open for now, as we’re going to need those instructions next.\n\n\n\n\nNow that we’ve installed the layer manager, we’ll create our first annotations from CELEX - a layer with morphological annotations.\n\nFollow the instructions on the information page to create a layer for word morphology - i.e.:\n\nLayer ID: morphology\nType: Text\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescript Morphological parses\n...configured with the Morphology option selected, and the default values for everything else.\n\n\n\n\n\n\n\n\nTip\n\n\n\n If you’re curious about what the configuration options do, and how you can test out the results of your configuration, check the online help page when you are configuring the layer.\n\n\n\nOnce the layer has finished generating, select the transcripts menu option, and open the first transcript in the list.\nTick the CELEX project, and then tick your new morphology layer.\nYou will see that each word is tagged with morphological information.\n\nIf you were to do a search for words ending in “ing” on the orthography layer, you would get both gerunds like “coming” and also words like “thing” and “anything” whose “ing” is part of the base word, not a morphological affix. You can now tell these apart in searches, by searching the morphology layer for words that end in “+ing”.\n\nDo a search on the orthography layer of words ending in “ing”. Leave the results tab open, so you can compare these results with the next search …\nNow do a search on the morphology layer of words ending in “+ing”, and compare the results.\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that in regular expressions the ‘plus’ character + has a special meaning - it means “one or more of the previous thing”.\nIn order to search for a literal “+” in the annotation, you have to ‘escape’ the +. Consult the Regular Expression help page to figure out how to do that.\n\n\n\n\n\nNow we will create a layer for syntactic categories from CELEX.\n\nCreate a new layer for annotating words with their syntactic categories from CELEX:\n\nLayer ID: syntacticCategory\nType: Text\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescription: All possible syntactic categories\n...configured with the Syntax option ticked, and the default values for everything else.\n\nOnce the layer has finished generating, go to the search page and do a search for the word “fine” on the orthography layer.\nOpen the transcript of the first match.\nTick the syntactic category layer to display the annotations that have just been computed.\nNow find your instance of the word “fine” again (it’s highlighted in the transcript text).\nYou will see that it has “A” for adjective above it.\nClick on the word “fine” and select the Edit option on the menu that appears.\nNow look for the syntacticCategory layer. You will see that, in addition to “A”, there are several other annotations that are invisible on the transcript.\nThese are all the possible syntactic categories for the word “fine” ordered most-frequent first. Only the first one is displayed in the transcript, but when you do searches, all of them are searched.\nOn the search page, do a search for fine on the orthography layer and A on the syntactic category layer in the same column.\nThis has the effect of ‘ANDing’ together the patterns for a single word, so it will give you words that have “fine” on the orthography layer have A on the syntacticCategory* layer.\n\nDo another search, for fine on the orthography layer and V on the syntacticCategory layer.\nNotice that the results are the same. This is because all of the instances of “fine” are marked as ‘possibly an adjective’ and also ‘possibly a verb’.\n\nAs you can see, simply tagging tokens with all possible syntactic categories from the CELEX lexicon leads to search results that are heavy on false positives. In order to tag tokens with a single syntactic category, we would need to perform ‘disambiguation’ by taking the surrounding transcript into account, in order to decide which of all the possibilities is the correct syntactic category. LaBB-CAT has two layer managers that perform such part-of-speech tagging: the “StanfordPosTagger” and the “MorTagger”. Neither of these use the exact same syntactic labels that are used by CELEX.\n\n\n\nCELEX can be used to retrieve syllable-counts for words, which in turn can be used, with duration information, to compute speech rate. For each line in each transcript, we already have the start time and the end time, from which we can calculate the duration of the line. All we need now is the number of syllables per line, and we can compute the syllables-per-minute speech rate for each line.\n\nCreate a new word layer:\n\nLayer ID: syllableCount\nType: Number\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescription: Number of syllables\n...configured with the Syllable count option ticked\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nEnsure the First match only option is ticked for this layer.\n If you’re not sure why, this is explained in the online help for the layer configuration page.\n\n\n\nOnce the layer has finished generating, have a look at a transcript or two to check the results.\nClick the phrase layers menu option.\nAdd a new phrase layer for speech rate. Key points are:\n\nThe layer manager to use is the Statistics Layer Manager.\nThe layer to summarize should be the syllableCount layer.\nThe statistic to compute is Label-Sum Rate (per minute)\n\n\n\n\n\n\nTip\n\n\n\n If in doubt, the online help may help.\n\n\nYou can calculate over whatever scopes you like, but if you select Utterances this will give you a local speech rate which might be useful when looking at individual search results, and Participants might be interesting if you want to compare speech rate between speakers.\n\nHave a look in a transcript or two, and a participant or two, to see what the annotations you just generated look like.\n\n\n\n\nNow we’re going to create a phonemic-transcription layer.\n\nCreate a new word layer, called phonemes, similar to previous CELEX layers. Key points are:\n\nThe layer type should be set to Phonological.\nThe Phonology option should be selected in the layer configuration.\nMake sure the Pronounce Event Override option is ticked.\nThis means that if the original ELAN transcript contained a ‘pronounce’ annotation for a word (these are marked in ELAN with square brackets), specifying its pronunciation, then the ‘pronounce’ annotation is used instead of the phonemic transcription from CELEX.\nEnsure the Generates Segments option is un-ticked.\nThis option allows the layer manager to create segment (sub-word) annotations from the phonemic transcriptions, but we don’t want this because in the next exercise, we’re going to get HTK to do that instead.\n\nOnce the layer is finished generating, go to a transcript to see what it looks like.\n\nYou will notice that the annotations are displayed using IPA symbols. However, CELEX doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme. The IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ASCII option on the layer in the transcript.\n\nSelect ASCII on the phonemes layer, to see what CELEX is actually producing.\n\nYou may find that this is somewhat harder to read. Diphthongs are generally represented by digits, schwa is @, and various other characters are used to represent affricates, etc.\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in table below, because they are what we have to use when searching on the *phonemes* layer, which we are going to try now.\n\n\n\n\nIPA\nDISC\n \n \nIPA\nDISC\n \n\n\np\np\npat\n \nɪ\nI\nKIT\n\n\nb\nb\nbad\n \nε\nE\nDRESS\n\n\nt\nt\ntack\n \næ\n{\nTRAP\n\n\nd\nd\ndad\n \nʌ\nV\nSTRUT\n\n\nk\nk\ncad\n \nɒ\nQ\nLOT\n\n\ng\ng\ngame\n \nʊ\nU\nFOOT\n\n\nŋ\nN\nbang\n \nə\n@\nanother\n\n\nm\nm\nmat\n \ni:\ni\nFLEECE\n\n\nn\nn\nnat\n \nα: \n#\nfather\n\n\nl\nl\nlad\n \nɔ:\n$\nTHOUGHT\n\n\nr\nr\nrat\n \nu:\nu\nGOOSE\n\n\nf\nf\nfat\n \nɜ:\n3\nNURSE\n\n\nv\nv\nvat\n \neɪ\n1\nFACE\n\n\nθ\nT\nthin\n \nαɪ\n2\nPRICE\n\n\nð\nD\nthen\n \nɔɪ\n4\nCHOICE\n\n\ns\ns\nsap\n \nəʊ\n5\nGOAT\n\n\nz\nz\nzap\n \nαʊ\n6\nMOUTH\n\n\n∫\nS\nsheep\n \nɪə\n7\nNEAR\n\n\nʒ\nZ\nmeasure\n \nεə\n8\nSQUARE\n\n\nj\nj\nyank\n \nʊə\n9\nCURE\n\n\nx\nx\nloch\n \næ\nc\ntimbre\n\n\nh\nh\nhad\n \nɑ̃ː\nq\ndétente\n\n\nw\nw\nwet\n \næ̃ː\n0\nlingerie\n\n\nʧ\nJ\ncheap\n \nɒ̃ː\n~\nbouillon\n\n\nʤ\n_\njeep\n \n \n \n \n\n\nŋ̩\nC\nbacon\n \n \n \n \n\n\nm̩\nF\nidealism\n \n \n \n \n\n\nn̩\nH\nburden\n \n \n \n \n\n\nl̩\nP\ndangle\n \n \n \n \n\n\n\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\n\nNow we’re going to do a search for the word “the” followed by a word that starts with schwa.\n\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little « button to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it.\nFind the schwa symbol ə and click it.\nYou will see that a @ symbol appears in the box.\n@ is the DISC symbol for ə, so in order to search for schwa, we have to use it in our search pattern.\nWe want words that start with schwa, so type .* after the @ symbol.\nClick Search.\n\nYou will see that some of the words being matched are words that you might not normally think start with a schwa. LaBB-CAT is matching words against all their possible phonemic transcriptions, so if CELEX has multiple possible pronunciations for a word, and one of them starts with schwa, it will be matched.\nYou can check this by clicking on a match, and then clicking on the word in the transcript and selecting Edit, which displays all the annotations for the given token.\nNow that we have phonemic transcripts, we can do a better job of the search we tried in an earlier exercise – “the” followed by a word starting with a vowel…\n\nChange your search so that, instead of just @ at the beginning of the word, it matches any vowel.\n\nYou could use the square-brackets [] at the start of your pattern, and type all vowel symbols inside them - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the IPA helper, including all the diphthongs.\nAlternatively, you can simply click the VOWEL link in the ‘Phoneme Symbol Selector’, which will add all the DISC vowels for you, already enclosed in square-brackets.\n\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve generated a few different layers, and have seen how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nInstances of an article followed by a noun\nWords which have the DRESS vowel as the second phoneme\nThe word “the” followed by a word beginning with the phoneme /k/\nWords ending with schwa, followed by words beginning with /p/ or /b/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/\nPlurals that end in /s/ or /z/ or /ɪz/"
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html#installation",
    "href": "worksheets/course/7a-lexicon-celex.html#installation",
    "title": "7a. CELEX",
    "section": "",
    "text": "The first thing we’re going to do is install the CELEX layer manager.\nThis requires having the LDC’s CELEX data files on your computer, which will be processed by the layer manager in order to insert the data into the LaBB-CAT database. If you received the CELEX files in a ZIP file, you need to unzip that into a folder, and remember the location of that folder, as you’ll need it during the installation process.\n\nFirst of all, create a new project called CELEX with a description: CELEX Annotations\nClick the layer managers menu option.\nClick the List of layer managers that are not yet installed link near the bottom.\nFind “CELEX English” in the list, and click its Install button.\nYou will see a form asking for various details. You can leave most of these with their default values. The one exception is the CELEX  ENGLISH data folder option.\nSet the CELEX ENGLISH data folder parameter to the directory path that leads to the CELEX files on your LaBB-CAT computer.\nClick Install.\nYou will see a progress bar while the layer manager loads the data from the CELEX files into the LaBB-CAT database. This will take a few minutes.\nOnce it’s finished, you will see a new window open with information about the CELEX English layer manager.\nReading this information page, you will see some instructions on how to create CELEX annotation layers - leave this tab open for now, as we’re going to need those instructions next."
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html#morphology",
    "href": "worksheets/course/7a-lexicon-celex.html#morphology",
    "title": "7a. CELEX",
    "section": "",
    "text": "Now that we’ve installed the layer manager, we’ll create our first annotations from CELEX - a layer with morphological annotations.\n\nFollow the instructions on the information page to create a layer for word morphology - i.e.:\n\nLayer ID: morphology\nType: Text\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescript Morphological parses\n...configured with the Morphology option selected, and the default values for everything else.\n\n\n\n\n\n\n\n\nTip\n\n\n\n If you’re curious about what the configuration options do, and how you can test out the results of your configuration, check the online help page when you are configuring the layer.\n\n\n\nOnce the layer has finished generating, select the transcripts menu option, and open the first transcript in the list.\nTick the CELEX project, and then tick your new morphology layer.\nYou will see that each word is tagged with morphological information.\n\nIf you were to do a search for words ending in “ing” on the orthography layer, you would get both gerunds like “coming” and also words like “thing” and “anything” whose “ing” is part of the base word, not a morphological affix. You can now tell these apart in searches, by searching the morphology layer for words that end in “+ing”.\n\nDo a search on the orthography layer of words ending in “ing”. Leave the results tab open, so you can compare these results with the next search …\nNow do a search on the morphology layer of words ending in “+ing”, and compare the results.\n\n\n\n\n\n\n\nImportant\n\n\n\nRemember that in regular expressions the ‘plus’ character + has a special meaning - it means “one or more of the previous thing”.\nIn order to search for a literal “+” in the annotation, you have to ‘escape’ the +. Consult the Regular Expression help page to figure out how to do that."
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html#syntactic-categories",
    "href": "worksheets/course/7a-lexicon-celex.html#syntactic-categories",
    "title": "7a. CELEX",
    "section": "",
    "text": "Now we will create a layer for syntactic categories from CELEX.\n\nCreate a new layer for annotating words with their syntactic categories from CELEX:\n\nLayer ID: syntacticCategory\nType: Text\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescription: All possible syntactic categories\n...configured with the Syntax option ticked, and the default values for everything else.\n\nOnce the layer has finished generating, go to the search page and do a search for the word “fine” on the orthography layer.\nOpen the transcript of the first match.\nTick the syntactic category layer to display the annotations that have just been computed.\nNow find your instance of the word “fine” again (it’s highlighted in the transcript text).\nYou will see that it has “A” for adjective above it.\nClick on the word “fine” and select the Edit option on the menu that appears.\nNow look for the syntacticCategory layer. You will see that, in addition to “A”, there are several other annotations that are invisible on the transcript.\nThese are all the possible syntactic categories for the word “fine” ordered most-frequent first. Only the first one is displayed in the transcript, but when you do searches, all of them are searched.\nOn the search page, do a search for fine on the orthography layer and A on the syntactic category layer in the same column.\nThis has the effect of ‘ANDing’ together the patterns for a single word, so it will give you words that have “fine” on the orthography layer have A on the syntacticCategory* layer.\n\nDo another search, for fine on the orthography layer and V on the syntacticCategory layer.\nNotice that the results are the same. This is because all of the instances of “fine” are marked as ‘possibly an adjective’ and also ‘possibly a verb’.\n\nAs you can see, simply tagging tokens with all possible syntactic categories from the CELEX lexicon leads to search results that are heavy on false positives. In order to tag tokens with a single syntactic category, we would need to perform ‘disambiguation’ by taking the surrounding transcript into account, in order to decide which of all the possibilities is the correct syntactic category. LaBB-CAT has two layer managers that perform such part-of-speech tagging: the “StanfordPosTagger” and the “MorTagger”. Neither of these use the exact same syntactic labels that are used by CELEX."
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html#syllable-count-and-speech-rate",
    "href": "worksheets/course/7a-lexicon-celex.html#syllable-count-and-speech-rate",
    "title": "7a. CELEX",
    "section": "",
    "text": "CELEX can be used to retrieve syllable-counts for words, which in turn can be used, with duration information, to compute speech rate. For each line in each transcript, we already have the start time and the end time, from which we can calculate the duration of the line. All we need now is the number of syllables per line, and we can compute the syllables-per-minute speech rate for each line.\n\nCreate a new word layer:\n\nLayer ID: syllableCount\nType: Number\nAlignment: None\nManager: CELEX English\nProject: CELEX\nDescription: Number of syllables\n...configured with the Syllable count option ticked\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nEnsure the First match only option is ticked for this layer.\n If you’re not sure why, this is explained in the online help for the layer configuration page.\n\n\n\nOnce the layer has finished generating, have a look at a transcript or two to check the results.\nClick the phrase layers menu option.\nAdd a new phrase layer for speech rate. Key points are:\n\nThe layer manager to use is the Statistics Layer Manager.\nThe layer to summarize should be the syllableCount layer.\nThe statistic to compute is Label-Sum Rate (per minute)\n\n\n\n\n\n\nTip\n\n\n\n If in doubt, the online help may help.\n\n\nYou can calculate over whatever scopes you like, but if you select Utterances this will give you a local speech rate which might be useful when looking at individual search results, and Participants might be interesting if you want to compare speech rate between speakers.\n\nHave a look in a transcript or two, and a participant or two, to see what the annotations you just generated look like."
  },
  {
    "objectID": "worksheets/course/7a-lexicon-celex.html#phonology",
    "href": "worksheets/course/7a-lexicon-celex.html#phonology",
    "title": "7a. CELEX",
    "section": "",
    "text": "Now we’re going to create a phonemic-transcription layer.\n\nCreate a new word layer, called phonemes, similar to previous CELEX layers. Key points are:\n\nThe layer type should be set to Phonological.\nThe Phonology option should be selected in the layer configuration.\nMake sure the Pronounce Event Override option is ticked.\nThis means that if the original ELAN transcript contained a ‘pronounce’ annotation for a word (these are marked in ELAN with square brackets), specifying its pronunciation, then the ‘pronounce’ annotation is used instead of the phonemic transcription from CELEX.\nEnsure the Generates Segments option is un-ticked.\nThis option allows the layer manager to create segment (sub-word) annotations from the phonemic transcriptions, but we don’t want this because in the next exercise, we’re going to get HTK to do that instead.\n\nOnce the layer is finished generating, go to a transcript to see what it looks like.\n\nYou will notice that the annotations are displayed using IPA symbols. However, CELEX doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme. The IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ASCII option on the layer in the transcript.\n\nSelect ASCII on the phonemes layer, to see what CELEX is actually producing.\n\nYou may find that this is somewhat harder to read. Diphthongs are generally represented by digits, schwa is @, and various other characters are used to represent affricates, etc.\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in table below, because they are what we have to use when searching on the *phonemes* layer, which we are going to try now.\n\n\n\n\nIPA\nDISC\n \n \nIPA\nDISC\n \n\n\np\np\npat\n \nɪ\nI\nKIT\n\n\nb\nb\nbad\n \nε\nE\nDRESS\n\n\nt\nt\ntack\n \næ\n{\nTRAP\n\n\nd\nd\ndad\n \nʌ\nV\nSTRUT\n\n\nk\nk\ncad\n \nɒ\nQ\nLOT\n\n\ng\ng\ngame\n \nʊ\nU\nFOOT\n\n\nŋ\nN\nbang\n \nə\n@\nanother\n\n\nm\nm\nmat\n \ni:\ni\nFLEECE\n\n\nn\nn\nnat\n \nα: \n#\nfather\n\n\nl\nl\nlad\n \nɔ:\n$\nTHOUGHT\n\n\nr\nr\nrat\n \nu:\nu\nGOOSE\n\n\nf\nf\nfat\n \nɜ:\n3\nNURSE\n\n\nv\nv\nvat\n \neɪ\n1\nFACE\n\n\nθ\nT\nthin\n \nαɪ\n2\nPRICE\n\n\nð\nD\nthen\n \nɔɪ\n4\nCHOICE\n\n\ns\ns\nsap\n \nəʊ\n5\nGOAT\n\n\nz\nz\nzap\n \nαʊ\n6\nMOUTH\n\n\n∫\nS\nsheep\n \nɪə\n7\nNEAR\n\n\nʒ\nZ\nmeasure\n \nεə\n8\nSQUARE\n\n\nj\nj\nyank\n \nʊə\n9\nCURE\n\n\nx\nx\nloch\n \næ\nc\ntimbre\n\n\nh\nh\nhad\n \nɑ̃ː\nq\ndétente\n\n\nw\nw\nwet\n \næ̃ː\n0\nlingerie\n\n\nʧ\nJ\ncheap\n \nɒ̃ː\n~\nbouillon\n\n\nʤ\n_\njeep\n \n \n \n \n\n\nŋ̩\nC\nbacon\n \n \n \n \n\n\nm̩\nF\nidealism\n \n \n \n \n\n\nn̩\nH\nburden\n \n \n \n \n\n\nl̩\nP\ndangle\n \n \n \n \n\n\n\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\n\nNow we’re going to do a search for the word “the” followed by a word that starts with schwa.\n\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little « button to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it.\nFind the schwa symbol ə and click it.\nYou will see that a @ symbol appears in the box.\n@ is the DISC symbol for ə, so in order to search for schwa, we have to use it in our search pattern.\nWe want words that start with schwa, so type .* after the @ symbol.\nClick Search.\n\nYou will see that some of the words being matched are words that you might not normally think start with a schwa. LaBB-CAT is matching words against all their possible phonemic transcriptions, so if CELEX has multiple possible pronunciations for a word, and one of them starts with schwa, it will be matched.\nYou can check this by clicking on a match, and then clicking on the word in the transcript and selecting Edit, which displays all the annotations for the given token.\nNow that we have phonemic transcripts, we can do a better job of the search we tried in an earlier exercise – “the” followed by a word starting with a vowel…\n\nChange your search so that, instead of just @ at the beginning of the word, it matches any vowel.\n\nYou could use the square-brackets [] at the start of your pattern, and type all vowel symbols inside them - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the IPA helper, including all the diphthongs.\nAlternatively, you can simply click the VOWEL link in the ‘Phoneme Symbol Selector’, which will add all the DISC vowels for you, already enclosed in square-brackets.\n\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve generated a few different layers, and have seen how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nInstances of an article followed by a noun\nWords which have the DRESS vowel as the second phoneme\nThe word “the” followed by a word beginning with the phoneme /k/\nWords ending with schwa, followed by words beginning with /p/ or /b/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/\nPlurals that end in /s/ or /z/ or /ɪz/"
  },
  {
    "objectID": "worksheets/course/5-manual-annotation.html",
    "href": "worksheets/course/5-manual-annotation.html",
    "title": "5. Manual Annotation",
    "section": "",
    "text": "5. Manual Annotation\nNow we’re going to create our own layer for manual annotations, and explore ways of populating it. Let’s say we’re interested in the pronunciation of the vowel in the word “the” when the following word starts with a vowel. We’re going to:\n\nCreate a layer for annotations on tokens of the word “the”.\nSearch for tokens using word orthography, and identify ‘false positives’ (e.g. cases like “…the one…” where the spelling of the following word starts with a vowel but it’s not pronounced as a vowel).\nFor the ‘true positives’, perform some auditory analysis (i.e. listen to them) and tag each token accordingly.\n\n\n\nTo embark on this mini project, we’re first going to create a ‘project’ in LaBB-CAT to categorize our annotations.\nSelect the projects link on the menu.\nAdd a project called “the” with a description something like Pronunciation of the vowel in the when followed by a word-initial vowel, by filling in the form and pressing the New button.\n\nNow we’re going to create a layer to store our annotations…\n\nSelect on the word layers option on the menu.\nYou will see a list of existing word layers, including the orthography layer, the lexical layer, etc.\nThe row of column headings at the top is also a form for adding a new layer.\nFill in the top row with the following details:\n\nLayer ID: the\nType: Text\nAlignment: None (our annotations are simply tags on words, inheriting their start/end times from the word token they tag)\nManager: don’t select any manager, as we’ll be adding manual annotations, rather than automatically generated ones\nGenerate: don’t select any option (this setting is only relevant for managed layers, so it doesn’t actually matter what you select here)\nProject: the\nDescription: \"the\" followed by a word-initial vowel\n\nPress New to add the layer.\n\nNow we’re ready to find some tokens…\n\nSelect participants on the menu.\nWe’re going to search all male monolinguals, so filter by the appropriate attribute values and then click Layered Search.\nSearch for instances of the word “the” followed immediately by a word starting with a vowel, on the orthography layer.\nExport the results to a CSV file and open it.\n\nNow we’re going to annotate the CSV file to identify false positives.\n\nAdd a column to the right-hand side of the spreadsheet, called “The” - i.e. on the first line, in the cell to the right the last column header, enter the word The\n\nFor each row in the spreadsheet check the contents of the “Match transcript” column, and decide whether the match is a ‘false positive’ or not. False positives are cases like “the one”, where the second word actually starts with a non-vowel sound (i.e. the word “one” actually starts with a /w/ phoneme).\nFor false positives, enter FP in your new “The” column. For all the others, enter TP.\nSave the CSV file.\nYou may be asked if you want to change the format of the file. Resist the temptation to do this - we are going to upload this file into LaBB-CAT, and it can only understand CSV files.\n\nNow that we’ve annotated our results, we’re going to to load our annotations into the new layer we created in LaBB-CAT…\n\nIn LaBB-CAT, select the upload menu option.\nSelect the upload csv annotations option.\nPress Choose File and select the annotated CSV file you just saved.\nPress Upload.\nOn the form that appears, you can leave the default choices for the options. Just ensure that the Tag Words option is selected at the top, and at the bottom the The column in the spreadsheet is mapped to the the layer in LaBB-CAT.\nClick Insert Annotations.\nYou will see a message about how many annotations were added. Now, within LaBB-CAT, each token mentioned in your CSV file has been tagged with either “TP” or “FP”\n\nNow that we’ve seen one way to add annotations to the database, using CSV files, we will try another way - editing word annotations directly from the interactive transcript.\nWe’re going to find our ‘true positive’ tokens of the word “the”, and annotate each depending on how the speaker pronounces it in the recording.\n\nIn LaBB-CAT, select the search menu option, which by default searches utterances of all participants.\nThis time we’re going to search for the true-positive annotations we just inserted.\nUnder the “Tick layers to include” heading, there’s now a “Project” column that includes the “the” project we added at the start. Tick that project, so that the layer associated with it is displayed in the list of Word layers to the right.\nTick your custom layer (called “the”) in the Word column.\nYour search matrix is now two layers high by one word wide.\nSearch for TP on the the layer.\nThe results page should show you all the words you annotated with TP in your CSV file above.\nClick on the first match.\nThis will open the interactive transcript for the match. You’ll be able to see not only the transcript text, but also the TP/FP tags you have added.\nClick on the first match in the transcript, and select the Play option to play the line.\n\nListen carefully to see whether the speaker pronounces the word “the” like “thee” or not. If they do, we’re going to annotate the word with the code i. Otherwise we’re going to annotate it with the code @.\nClick the match word again, and select the last option on the menu: Edit. A window will appear, with a list of layers. Each line has the token’s annotation on the given row. So at the bottom, the value on the word layer is probably “the” or maybe “The” or “the .” or something similar. On the orthography layer, the annotation will be “the”. Other layers may be blank, except for your the layer, whose annotation will be “TP”.\nWe’re going to change the “TP” annotation depending on the pronunciation of the token. So replace “TP” with i or @ as appropriate.\nClick the Save button  to save your annotation to the database.\nClose the “edit word” window.\nBack in the interactive transcript, find the next match result - it will be highlighted.\nAnnotate the next match in exactly the same way - play the utterance, listen to the pronunciation, and change the TP to an appropriate code.\nSimilarly annotate the rest of the matches in the transcript.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may notice that, although you’re changing labels to i or @, the tags still appear as TP in the transcript page; this is simply because that’s what the tag was when you opened the transcript. If you refresh the page, you’ll see your new tags instead of the old ones.\n\n\n\nOnce you’ve annotated the last match in the transcript, close the browser’s tab.\nThis will take you back to the search results page.\nYou’ve already annotated all the matches in the first transcript, so move to the next transcript in the results list, and click the first match.\nAnnotate all the “TP” tokens in the transcript.\nRepeat the above steps until you’ve annotated all the matches.\n\nYou’ve now used two methods for annotating words. Although this is a small, toy example, you can hopefully see that you could manage a larger annotation project involving much more tokens, possibly multiple annotators, and working either offline (with a CSV file and maybe extracted WAV files) or online (directly in the interactive transcript page), as preferred.\nThere are other ways to add manual annotations, which relate to concrete points or intervals in time during the recording. We will see how to do this later…\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/express-tutorial/3-basic-searching.html",
    "href": "worksheets/express-tutorial/3-basic-searching.html",
    "title": "3 - Basic Searching",
    "section": "",
    "text": "3 - Basic Searching\nNow that you have some transcripts in your database, we’ll try out LaBB-CAT’s search functions a little.\nSearching broadly involves the following steps:\n\nselecting participants whose utterances you want to search, \nspecifying one or more patterns to search for, and\nexploring or extracting the search results.\n\n\nWe’ll start with a very simple search - all the instances of the word “the” uttered by monolingual English-speaking males.\n\nIn LaBB-CAT, select the participants link on the menu.\nThis takes you to the Participants page you have already seen in a previous exercise, where you can list participants and filter them by their attributes. You can see various participant attributes listed across top of the page.\n\nWe’re interested in male participants, so under the word Gender, select M.\nThe page will then display a list of all the male participants in the database.\nWe want the participants who speak only English, so enter the following regular expression under Languages:\n^English$\nThis pattern, starting with “^” and ending with “$”, means “match only values that start and end with the word ‘English’” - i.e. English is the only word mentioned.\nThe page will then display a list of male participants who list only English as their language.\n\nPress Layered Search at the top.\nYou will see the participants you selected listed at the top, followed by a list of layers (which we’ll ignore for now). Below that, there’s a heading Search Matrix with various controls. This is the ‘search matrix’, although it doesn’t look much like a matrix yet, because it’s only one layer high and one word wide…\nIn the box labelled ‘regular expression’ under the word orthography type the word the\n\nNow press the Search button at the bottom.\nA progress bar will appear, and then shortly after that, a new window will open, which has a list of search results in it.\n\n\n\n\n\n\n\nNote\n\n\n\nYour browser’s popup-blocker might prevent the results page from opening – you can fix that either by allowing the popups in your browser, or by clicking the Display results link that appears after the search finishes.\n\n\n\nEach match is highlighted and shown within a few words context. Click on the first match.\nYou will see that the interactive transcript page opens in a new browser tab, with the match at the top, and highlighted. You will also see that all the other matches from the same transcript are also highlighted.\nWe’ve already seen what can be done in the interactive transcript page, so close the tab to return to the results page.\nEach result line has a ticked checkbox next to it. Scroll to the bottom of the list.\nYou’ll see that there are several buttons at the bottom, which perform operations on the ticked results CSV Export, Utterance Export, and Audio Export (among others).\nUntick the Select all results checkbox, and then tick a handful of results in the list.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can select a group of matches by ticking the first one, and then holding down the Shift key while ticking the last one.\n\n\n\nHover the mouse pointer over the Prefix Names checkbox to see what this option does, and then tick it.\nClick the Audio Export button.\nSave and open the resulting zip file. \nYou’ll see that the files are systematically named to include:\n\nthe result number,\nthe name of the transcript, and\nthe start and end time of the extracted utterance.\n\nNow go back to the results page and click the Utterance Export button.\nSave and open the resulting zip file.\nYou’ll see that the TextGrid names match the audio file names in the previous zip file.\nOpen one of the TextGrids in Praat.\nYou’ll see that, in addition to the utterance and word tiers, there’s also a target tier which marks the words that matched the search.\nBack on the results page, click the CSV Export button.\nSave the resulting file, and open it.\n\nYou may have to specify some import options, in which case it may be handy to know that the field separator is comma, and the fields are quoted by speech marks.\n\n\n\n\n\n\nNote\n\n\n\nIf you’re using Microsoft Excel and you find it doesn’t open all the columns correctly: 1. Create a new workbook in Excel. 2. Click the Data tab. 3. On the Get External Data ribbon click From Text. 4. Select the CSV file you downloaded. 5. Select Delimited and click Next. 6. Ensure Comma is the only delimiter ticked and click Next. 7. Click Finish and then OK.\n\n\nYou will see a spreadsheet with one line per selected result, and various columns containing information about the speaker, the corpus, the match line and word, and a URL to the interactive transcript for the match.\nWith this spreadsheet, you can work ‘offline’ with the results, tagging them, computing statistics in Excel, R, or any other program that can work with CSV files. There are a few more uses for the CSV results files, which are dealt with in a separate tutorial…\n\nClose the CSV file, and the results page, and go back to the search matrix page.\n\nWe’ve seen that you can search for exact word matches, but you can also search for patterns, using ‘regular expressions’. Now we’re going to search for words beginning with “the…”\n\nChange the orthography search text to the.*\n(i.e. after the word “the”, append a full-stop and an asterisk.\n \nThe full-stop means “any character at all”, and the asterisk means “zero or more of the previous thing”, so *.** means “zero or more characters”.\nPress Search.\nYou will see that now the search results include the word “the” and also words like “then”, “there”, “they”, etc.\nNow go back to the search page, and change the asterisk to a plus-sign, which means “one or more of the previous thing”\n\nClick Search\nYou will see that now the search results exclude the word “the”, only including words where the initial “the…” is followed by at least one character.\nNow change your search by replacing the e in “the” with [aeiou] - so your search pattern will be th[aeiou].+\nThe square-brackets mean “any one of the things inside the brackets”, so [aeiou] means “any vowel”.\nClick Search\nYou will now see that the results include words like “think”, “that”, “thought”, etc.\n\n\n\n\n\n\n\nTip\n\n\n\n You can get more information about regular expressions by using the online help on the search page, and also by clicking the the regular expressions link above the search matrix.\n\n\nUp until now, we’ve only been matching against one word at a time. Now we’re going to include patterns for a chain of words.\n\nOn the search page, to the right of the search matrix, there’s a + button. Press it.\n![A frame labelled ‘orthography’ with ’matches thaeiou’’ inside, followed by a dropdown box with ‘followed immediately bo’ selected, followed by another frame labelled ‘orthography’ containing a ‘matches’ dropdown box and an empty ‘regular expression’ box\nNow you will see that our search matrix is one layer high by two words wide.\nChange the entries on the orthography layer so that it will match the word “the” followed immediately by a word that starts with a vowel, and click Search.\nCheck the search results are giving you what you expected.\nNow search for “the” followed, within two words, by a word that starts with a vowel.\nDream up some other searches that interest you, and try out other options on the search page.\n\n\n\n\n\n\n\nTip\n\n\n\n If in doubt about a search option, try the online help page.\n\n\nBecause we’re searching by word orthography, you will have noticed that your searches for words starting with a vowel return words where the spelling starts with a vowel, but the pronunciation doesn’t, e.g. “one”, “once”, etc. In order to search by pronunciation, we need to add a layer of pronunciation annotations.  We’ll do that in the next exercise…\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html",
    "href": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html",
    "title": "4 - The CMU Dictionary and Cross Layer Searching",
    "section": "",
    "text": "LaBB-CAT can be integrated with the CMU Pronouncing Dictionary, which is a free pronounciation dictionary of English maintained by the Speech Group in the School of Computer Science at Carnegie Mellon University. The pronunciations are based on American English, so are suitable for American English recordings.\nIt can also serve as a free alternative to the CELEX lexicon (which is based on British English), for those that have not purchased CELEX, although is less ideal for ‘non-rhotic’ varieties of English.\nIn this exercise you will:\n\ninstall the CMU Pronouncing Dictionary layer manager,\nuse it to create new annotations for word pronunciations, and \nincorporate the new layers in more sophisticated searches.\n\n\n\nThe first thing we’re going to to is install the CMU Pronouncing Dictionary layer manager…\n\nSelect the layer managers menu option.\nYou will see a list of pre-installed layer managers, which are modules that can perform automatic annotation tasks. The CMU Pronouncing Dictionary layer manager isn’t pre-installed, because it is language-specific.\nFollow the List of layer managers that are not yet installed link near the bottom.\nFind CMU Pronouncing Dictionary in the list, and press its Install button.\nPress Install on the resulting information page.\nThis displays some further information about the layer manager, allowing you to upload an alternative version of the dictionary file.\nWe be using the standard file that is included with the layer manager.\nPress Configure.\nYou will see a progress bar while the layer manager loads the data from the dictionary file into the LaBB-CAT database. This will take a minute or so.\nOnce it’s finished, you will see a new window open with information about the CMU Pronouncing Dictionary layer manager.\n\n\n\n\nNow that we’ve installed the layer manager, we’ll create a layer that contains word pronunciations.\n\nSelect the word layers option on the menu.\nYou will see a list of existing word layers, including the orthography layer, the lexical layer, etc.\nThe column headings are also a form for defining a new word layer. Fill in the following details in this form:\n\nLayer ID: phonemes\nType: Phonological\nAlignment: None\nManager: CMU Pronouncing Dictionary\nDescription: All possible phonemic transcriptions for each word.\n\nPress New to add the layer.\nYou will see the layer configuration form.\nSet the Encoding field to CELEX DISC, and the default values for everything else.\n\n\n\n\n\n\n\nTip\n\n\n\n If you’re curious about what the configuration options do, hover your mouse over each one to see further information about what the setting does.\n\n\n\nPress Set Parameters.\nYou will see a message asking you if you want (re)generate the layer data now.\nPress Regenerate.\nYou will see a progress bar moving across the page while the annotations are being generated. When it is finished, you will see a message saying Layer complete.\nOnce the layer has finished generating, click the transcripts menu option, and open the first transcript in the list.\nTick your new phonemes layer.\nYou will see that each word is tagged with a phonemic transcription. You will notice that the annotations are displayed using IPA symbols. However, the layer manager doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme.\nThe IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ASCII option on the layer in the transcript.\nSelect ASCII on the phonemes layer, to see what the layer manager is actually producing.\nYou may find that this is somewhat harder to read. It’s similar to the ‘SAMPA’ system for encoding phonemes, but diphthongs are generally represented by digits, and various other characters are used to represent affricates, etc.\nSelect IPA on the phonemes layer, to return to the IPA view of the layer.\n\n\n\n\n\nIt’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in the table below), because they are what we have to use when searching on the phonemes layer, which we are going to try now.\nThere is another possible representation of the pronunciations, called ARPABET; this is what is used in the original dictionary file published by CMU, and uses up to three uppercase characters per phoneme. While we’re not using ARPABET in this exercise, you can use it if you like, and the ARPABET symbols are included in the table. In the table, there are gaps where no ARPABET version of the phoneme is shown; this means that the CMU Pronouncing Dictionary contains no entries that include that phoneme.\n\n\n\nIPA\nDISC\nARPABET\n \n \nIPA\nDISC\nARPABET\n \n\n\np\np\nP\npat\n \nɪ\nI\nIH\nKIT\n\n\nb\nb\nB\nbad\n \nε\nE\nEH\nDRESS\n\n\nt\nt\nT\ntack\n \næ\n{\nAE\nTRAP\n\n\nd\nd\nD\ndad\n \nʌ\nV\nAH\nSTRUT\n\n\nk\nk\nK\ncad\n \nɒ\nQ\nAH\nLOT\n\n\ng\ng\nG\ngame\n \nʊ\nU\nUH\nFOOT\n\n\nŋ\nN\nNG\nbang\n \nə\n@\n[vowel ending in 0]\nanother\n\n\nm\nm\nM\nmat\n \ni:\ni\nIY\nFLEECE\n\n\nn\nn\nN\nnat\n \nα: \n#\nAA\nfather\n\n\nl\nl\nL\nlad\n \nɔ:\n$\nAO\nTHOUGHT\n\n\nr\nr\nR\nrat\n \nu:\nu\nUW\nGOOSE\n\n\nf\nf\nF\nfat\n \nɜ:\n3\nER\nNURSE\n\n\nv\nv\nV\nvat\n \neɪ\n1\nEY\nFACE\n\n\nθ\nT\nTH\nthin\n \nαɪ\n2\nAY\nPRICE\n\n\nð\nD\nDH\nthen\n \nɔɪ\n4\nOY\nCHOICE\n\n\ns\ns\nS\nsap\n \nəʊ\n5\nOW\nGOAT\n\n\nz\nz\nZ\nzap\n \nαʊ\n6\nAW\nMOUTH\n\n\n∫\nS\nSH\nsheep\n \nɪə\n7\n \nNEAR\n\n\nʒ\nZ\nZH\nmeasure\n \nεə\n8\n \nSQUARE\n\n\nj\nj\nY\nyank\n \nʊə\n9\n \nCURE\n\n\nx\nx\n \nloch\n \næ\nc\n \ntimbre\n\n\nh\nh\nHH\nhad\n \nɑ̃ː\nq\n \ndétente\n\n\nw\nw\nW\nwet\n \næ̃ː\n0\n \nlingerie\n\n\nʧ\nJ\nCH\ncheap\n \nɒ̃ː\n~\n \nbouillon\n\n\nʤ\n_\nJH\njeep\n \n \n \n \n \n\n\nŋ̩\nC\n \nbacon\n \n \n \n \n \n\n\nm̩\nF\n \nidealism\n \n \n \n \n \n\n\nn̩\nH\n \nburden\n \n \n \n \n \n\n\nl̩\nP\n \n dangle\n \n \n \n \n \n\n\n\n\nSelect the search option on the menu, which allows you to search all participants by default.\nIf it’s not already ticked, tick the new phonemes layer.\nNow you will see that our search matrix is two layers high by one word wide.\n\nSearch your new phonemes layer for words that start with h by entering the appropriate regular expression in the phonemes box.\nYou will see that the results contain words that you might not expect, like “where”, “which” and “when”.\nClick one of these unexpected results, to open the transcript.\nYou will see that, in the transcript, the pronunciation appears to start with /w/, not with /h/.\nClick on the word and select the bottom Edit option on the menu that appears.\nThis opens a small window that displays all annotations on that word token.\nNow look for the phonemes layer. You will see that, in addition to the pronunciation that starts with /w/, there’s another annotation that starts with /h/, which is invisible on the transcript.\nThese are all the possible phonemic transcriptions for the word, ordered most-frequent first. Only the first one is displayed in the transcript, but when you do searches, all of them are searched. This can result in unexpected matches like this, but it can be useful, as it ensures that when you search for a particular phonemic pattern, all possible tokens are returned, not just those that match on the most ‘normal’ transcription.\n\n\nNow that we have phonemic transcripts, we can do a better job of the search we tried in the earlier exercise - “the” followed by a word starting with a vowel…\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little symbol « to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it; clicking on a phoneme adds its DISC representation to the search box.\nYou could use the square-brackets [ at the start of your pattern, and click all vowel symbols to add all possible vowels - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the IPA helper, including all the diphthongs.\n\nAlternatively, you can simply click the VOWEL link in the ‘phoneme symbol selector’, which will add all the DISC vowels for you, already enclosed in square-brackets.\nBe sure to append the ‘any vowel’ regular expression with .* to ensure the search matches words that have phonemes after the initial vowel\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve generated an annotation layer, and have seen how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nWords which have the DRESS vowel as the second phoneme\nWords ending with a front vowel, followed by words beginning with /p/ or /b/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/"
  },
  {
    "objectID": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#install-the-cmu-dictionary",
    "href": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#install-the-cmu-dictionary",
    "title": "4 - The CMU Dictionary and Cross Layer Searching",
    "section": "",
    "text": "The first thing we’re going to to is install the CMU Pronouncing Dictionary layer manager…\n\nSelect the layer managers menu option.\nYou will see a list of pre-installed layer managers, which are modules that can perform automatic annotation tasks. The CMU Pronouncing Dictionary layer manager isn’t pre-installed, because it is language-specific.\nFollow the List of layer managers that are not yet installed link near the bottom.\nFind CMU Pronouncing Dictionary in the list, and press its Install button.\nPress Install on the resulting information page.\nThis displays some further information about the layer manager, allowing you to upload an alternative version of the dictionary file.\nWe be using the standard file that is included with the layer manager.\nPress Configure.\nYou will see a progress bar while the layer manager loads the data from the dictionary file into the LaBB-CAT database. This will take a minute or so.\nOnce it’s finished, you will see a new window open with information about the CMU Pronouncing Dictionary layer manager."
  },
  {
    "objectID": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#annotate-words-with-pronunciations",
    "href": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#annotate-words-with-pronunciations",
    "title": "4 - The CMU Dictionary and Cross Layer Searching",
    "section": "",
    "text": "Now that we’ve installed the layer manager, we’ll create a layer that contains word pronunciations.\n\nSelect the word layers option on the menu.\nYou will see a list of existing word layers, including the orthography layer, the lexical layer, etc.\nThe column headings are also a form for defining a new word layer. Fill in the following details in this form:\n\nLayer ID: phonemes\nType: Phonological\nAlignment: None\nManager: CMU Pronouncing Dictionary\nDescription: All possible phonemic transcriptions for each word.\n\nPress New to add the layer.\nYou will see the layer configuration form.\nSet the Encoding field to CELEX DISC, and the default values for everything else.\n\n\n\n\n\n\n\nTip\n\n\n\n If you’re curious about what the configuration options do, hover your mouse over each one to see further information about what the setting does.\n\n\n\nPress Set Parameters.\nYou will see a message asking you if you want (re)generate the layer data now.\nPress Regenerate.\nYou will see a progress bar moving across the page while the annotations are being generated. When it is finished, you will see a message saying Layer complete.\nOnce the layer has finished generating, click the transcripts menu option, and open the first transcript in the list.\nTick your new phonemes layer.\nYou will see that each word is tagged with a phonemic transcription. You will notice that the annotations are displayed using IPA symbols. However, the layer manager doesn’t use IPA symbols directly, it actually uses the ‘DISC’ encoding for phonemes, which uses ordinary ‘typewriter’ characters (ASCII), and uses exactly one character per phoneme.\nThe IPA symbols are being displayed by LaBB-CAT to provide a linguist-friendly representation of the phonemic transcription. But you can see the underlying DISC characters by selecting the ASCII option on the layer in the transcript.\nSelect ASCII on the phonemes layer, to see what the layer manager is actually producing.\nYou may find that this is somewhat harder to read. It’s similar to the ‘SAMPA’ system for encoding phonemes, but diphthongs are generally represented by digits, and various other characters are used to represent affricates, etc.\nSelect IPA on the phonemes layer, to return to the IPA view of the layer."
  },
  {
    "objectID": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#search-across-layers",
    "href": "worksheets/express-tutorial/4-cmudict-and-cross-layer-search.html#search-across-layers",
    "title": "4 - The CMU Dictionary and Cross Layer Searching",
    "section": "",
    "text": "It’s nice to display the IPA symbols, but it’s important to understand the DISC symbols (shown in the table below), because they are what we have to use when searching on the phonemes layer, which we are going to try now.\nThere is another possible representation of the pronunciations, called ARPABET; this is what is used in the original dictionary file published by CMU, and uses up to three uppercase characters per phoneme. While we’re not using ARPABET in this exercise, you can use it if you like, and the ARPABET symbols are included in the table. In the table, there are gaps where no ARPABET version of the phoneme is shown; this means that the CMU Pronouncing Dictionary contains no entries that include that phoneme.\n\n\n\nIPA\nDISC\nARPABET\n \n \nIPA\nDISC\nARPABET\n \n\n\np\np\nP\npat\n \nɪ\nI\nIH\nKIT\n\n\nb\nb\nB\nbad\n \nε\nE\nEH\nDRESS\n\n\nt\nt\nT\ntack\n \næ\n{\nAE\nTRAP\n\n\nd\nd\nD\ndad\n \nʌ\nV\nAH\nSTRUT\n\n\nk\nk\nK\ncad\n \nɒ\nQ\nAH\nLOT\n\n\ng\ng\nG\ngame\n \nʊ\nU\nUH\nFOOT\n\n\nŋ\nN\nNG\nbang\n \nə\n@\n[vowel ending in 0]\nanother\n\n\nm\nm\nM\nmat\n \ni:\ni\nIY\nFLEECE\n\n\nn\nn\nN\nnat\n \nα: \n#\nAA\nfather\n\n\nl\nl\nL\nlad\n \nɔ:\n$\nAO\nTHOUGHT\n\n\nr\nr\nR\nrat\n \nu:\nu\nUW\nGOOSE\n\n\nf\nf\nF\nfat\n \nɜ:\n3\nER\nNURSE\n\n\nv\nv\nV\nvat\n \neɪ\n1\nEY\nFACE\n\n\nθ\nT\nTH\nthin\n \nαɪ\n2\nAY\nPRICE\n\n\nð\nD\nDH\nthen\n \nɔɪ\n4\nOY\nCHOICE\n\n\ns\ns\nS\nsap\n \nəʊ\n5\nOW\nGOAT\n\n\nz\nz\nZ\nzap\n \nαʊ\n6\nAW\nMOUTH\n\n\n∫\nS\nSH\nsheep\n \nɪə\n7\n \nNEAR\n\n\nʒ\nZ\nZH\nmeasure\n \nεə\n8\n \nSQUARE\n\n\nj\nj\nY\nyank\n \nʊə\n9\n \nCURE\n\n\nx\nx\n \nloch\n \næ\nc\n \ntimbre\n\n\nh\nh\nHH\nhad\n \nɑ̃ː\nq\n \ndétente\n\n\nw\nw\nW\nwet\n \næ̃ː\n0\n \nlingerie\n\n\nʧ\nJ\nCH\ncheap\n \nɒ̃ː\n~\n \nbouillon\n\n\nʤ\n_\nJH\njeep\n \n \n \n \n \n\n\nŋ̩\nC\n \nbacon\n \n \n \n \n \n\n\nm̩\nF\n \nidealism\n \n \n \n \n \n\n\nn̩\nH\n \nburden\n \n \n \n \n \n\n\nl̩\nP\n \n dangle\n \n \n \n \n \n\n\n\n\nSelect the search option on the menu, which allows you to search all participants by default.\nIf it’s not already ticked, tick the new phonemes layer.\nNow you will see that our search matrix is two layers high by one word wide.\n\nSearch your new phonemes layer for words that start with h by entering the appropriate regular expression in the phonemes box.\nYou will see that the results contain words that you might not expect, like “where”, “which” and “when”.\nClick one of these unexpected results, to open the transcript.\nYou will see that, in the transcript, the pronunciation appears to start with /w/, not with /h/.\nClick on the word and select the bottom Edit option on the menu that appears.\nThis opens a small window that displays all annotations on that word token.\nNow look for the phonemes layer. You will see that, in addition to the pronunciation that starts with /w/, there’s another annotation that starts with /h/, which is invisible on the transcript.\nThese are all the possible phonemic transcriptions for the word, ordered most-frequent first. Only the first one is displayed in the transcript, but when you do searches, all of them are searched. This can result in unexpected matches like this, but it can be useful, as it ensures that when you search for a particular phonemic pattern, all possible tokens are returned, not just those that match on the most ‘normal’ transcription.\n\n\nNow that we have phonemic transcripts, we can do a better job of the search we tried in the earlier exercise - “the” followed by a word starting with a vowel…\n\nGo to the search page.\nCreate a search matrix that’s two words wide, and includes the orthography and phonemes layers.\nType the in the first orthography box.\nClick the second box on the phonemes layer, but don’t enter anything in the box yet.\nThe box has a little symbol « to the right of it.\nHover the mouse over it to see what it says, and then click it.\nYou will see that a section opens with a bunch of phoneme symbols on it; clicking on a phoneme adds its DISC representation to the search box.\nYou could use the square-brackets [ at the start of your pattern, and click all vowel symbols to add all possible vowels - Note that the vowels in the DISC representation extend beyond a, e, i, o, and u - you should add in all the vowels you see in the list that appears when you expand the IPA helper, including all the diphthongs.\n\nAlternatively, you can simply click the VOWEL link in the ‘phoneme symbol selector’, which will add all the DISC vowels for you, already enclosed in square-brackets.\nBe sure to append the ‘any vowel’ regular expression with .* to ensure the search matches words that have phonemes after the initial vowel\nRun the search and check that it’s giving you what you expect. Notice that now there are no ‘false positives’ like “the one” that we were getting when searching by orthography alone.\n\nNow that you’ve generated an annotation layer, and have seen how the search matrix works, you might want to try out some of the following searches, or invent some others:\n\nWords which have the DRESS vowel as the second phoneme\nWords ending with a front vowel, followed by words beginning with /p/ or /b/\nWords that begin with “k” in their spelling, but begin with the phoneme /n/\nWords that begin with “k” in their spelling, but do not begin with the phoneme /n/"
  },
  {
    "objectID": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html",
    "href": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html",
    "title": "1 - Install and Configure LaBB-CAT",
    "section": "",
    "text": "In this exercise you will:\n\nInstall the LaBB-CAT software\nDefine corpora\nDefine transcript types\n\nAfter this you will have an empty LaBB-CAT database set up ready to upload transcripts into.\n\n\nYou should only follow these steps if you will be running LaBB-CAT on your own computer.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you can skip to Setup below.\n\nYou have a file called install-labbcat.jar - double click this file to start the installer.\n\n\n\n\n\n\n\nIf you are using OS X\n\n\n\n\n\nOn Mac OS X you may see a message that the file can’t be opened: \nIf this happens:\n\nClick the Apple icon in the top left corner of the screen.\nSelect System Preferences\nClick Security & Privacy\nNear the bottom it says “install-labbcat.jar” was blocked from opening because it is not from an identified developer.\n\nClick Open Anyway\nYou may see another warning about the program being downloaded from the internet\n\nClick Open\n \n\n\n\n\n\nClick Start\nYou will see the progress bar move as files are installed. Once this is finished, you’ll see a message saying “Installation complete.”\n\n\nClick Finished to close the installer\n\nThe software is now installed. LaBB-CAT is a browser-based system, which means that it works as a mini web server on your computer, and you need to access it using your web browser.\nEach time you want to use LaBB-CAT, you must start it up, and which you’ve finished, you close it down again.\nTo start LaBB-CAT, click the LaBB-CAT icon in your applications area.\n\nOn Windows, open the Start menu and type LaBB-CAT.\nOn OS X you will find LaBB-CAT in your Applications folder.\n\nA window called “LaBB-CAT Server” will open, and after a short delay, your default web browser will open on a page called “LaBB-CAT”.\n\n\n\nNow we will set up a basic structure for receiving data:\n\nOpen your web browser on LaBB-CAT’s start page.\nThe very first time the page opens, you will see the software’s licence. Press I Agree to access the start page.\nThe start page has a link on it called “Where do I start?” - you may like to click on this link and read the first section, which explains a little about how to navigate around LaBB-CAT and where to find online help and hints.\nClick back on the start page of LaBB-CAT (the page with the “Where do I start?” link).\n\nNow we will set up some corpus names…\n\nOn the menu at the top, select the corpora link.\nThis page shows a list of current corpora, which only contains one corpus, called corpus.\nThe column headings at the top, Name, Language, and Description, also make a form you can fill in to create a new corpus. Fill in the following information:\n\nName: QB\nLanguage: English\nDescription: Quakebox recordings\n\nPress the New button on the right to add the QB corpus.\nYou should see a message at the top of the page saying “Record created” and now the QB corpus is in the list, under the corpus corpus.\nAdd another corpus called UC with the description Campus recordings\nWe won’t actually be using the corpus called corpus, so we want to delete it. To do this, press the Delete button to the right of the ‘corpus’ corpus in the list.\nYou will be asked “Are you sure you want to delete this record?”\nYou are sure, so press OK\nThe row will be deleted from the list.\n\nNow you have some corpora set up with the names you’ve provided.\nThe data we are using is a collection of stories about peoples’ experiences during the devastating earthquakes that hit the Canterbury region of New Zealand in 2010 and 2011. Some recordings are interviews, where an interviewer asks the participant questions, and others are monologues. Now we’re going to set up these two transcript types …\n\nSelect on the transcript types menu option.\nYou will see a list of transcript types, although there’s currently only one type in the list, called ‘interview’.\nAbove this, fill in the empty Type box with the word: monologue\nPress the New button on the right.\nYou will notice that now the list has two transcript types, interview and monologue.\nPress the Save button that has appeared below the other buttons to confirm this change.\nYou will see a message at the top saying “Updated transcript types”.\n\n\nNow you have an empty database for which you’ve:\n\ncreated two corpora, QB and UC, and\ncreated a new transcript type, so that we can have monologues as well as interviews."
  },
  {
    "objectID": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html#installation",
    "href": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html#installation",
    "title": "1 - Install and Configure LaBB-CAT",
    "section": "",
    "text": "You should only follow these steps if you will be running LaBB-CAT on your own computer.\nIf you are using a LaBB-CAT server that’s already been installed for you elsewhere, you can skip to Setup below.\n\nYou have a file called install-labbcat.jar - double click this file to start the installer.\n\n\n\n\n\n\n\nIf you are using OS X\n\n\n\n\n\nOn Mac OS X you may see a message that the file can’t be opened: \nIf this happens:\n\nClick the Apple icon in the top left corner of the screen.\nSelect System Preferences\nClick Security & Privacy\nNear the bottom it says “install-labbcat.jar” was blocked from opening because it is not from an identified developer.\n\nClick Open Anyway\nYou may see another warning about the program being downloaded from the internet\n\nClick Open\n \n\n\n\n\n\nClick Start\nYou will see the progress bar move as files are installed. Once this is finished, you’ll see a message saying “Installation complete.”\n\n\nClick Finished to close the installer\n\nThe software is now installed. LaBB-CAT is a browser-based system, which means that it works as a mini web server on your computer, and you need to access it using your web browser.\nEach time you want to use LaBB-CAT, you must start it up, and which you’ve finished, you close it down again.\nTo start LaBB-CAT, click the LaBB-CAT icon in your applications area.\n\nOn Windows, open the Start menu and type LaBB-CAT.\nOn OS X you will find LaBB-CAT in your Applications folder.\n\nA window called “LaBB-CAT Server” will open, and after a short delay, your default web browser will open on a page called “LaBB-CAT”."
  },
  {
    "objectID": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html#sec-setup",
    "href": "worksheets/express-tutorial/1-install-and-configure-labb-cat.html#sec-setup",
    "title": "1 - Install and Configure LaBB-CAT",
    "section": "",
    "text": "Now we will set up a basic structure for receiving data:\n\nOpen your web browser on LaBB-CAT’s start page.\nThe very first time the page opens, you will see the software’s licence. Press I Agree to access the start page.\nThe start page has a link on it called “Where do I start?” - you may like to click on this link and read the first section, which explains a little about how to navigate around LaBB-CAT and where to find online help and hints.\nClick back on the start page of LaBB-CAT (the page with the “Where do I start?” link).\n\nNow we will set up some corpus names…\n\nOn the menu at the top, select the corpora link.\nThis page shows a list of current corpora, which only contains one corpus, called corpus.\nThe column headings at the top, Name, Language, and Description, also make a form you can fill in to create a new corpus. Fill in the following information:\n\nName: QB\nLanguage: English\nDescription: Quakebox recordings\n\nPress the New button on the right to add the QB corpus.\nYou should see a message at the top of the page saying “Record created” and now the QB corpus is in the list, under the corpus corpus.\nAdd another corpus called UC with the description Campus recordings\nWe won’t actually be using the corpus called corpus, so we want to delete it. To do this, press the Delete button to the right of the ‘corpus’ corpus in the list.\nYou will be asked “Are you sure you want to delete this record?”\nYou are sure, so press OK\nThe row will be deleted from the list.\n\nNow you have some corpora set up with the names you’ve provided.\nThe data we are using is a collection of stories about peoples’ experiences during the devastating earthquakes that hit the Canterbury region of New Zealand in 2010 and 2011. Some recordings are interviews, where an interviewer asks the participant questions, and others are monologues. Now we’re going to set up these two transcript types …\n\nSelect on the transcript types menu option.\nYou will see a list of transcript types, although there’s currently only one type in the list, called ‘interview’.\nAbove this, fill in the empty Type box with the word: monologue\nPress the New button on the right.\nYou will notice that now the list has two transcript types, interview and monologue.\nPress the Save button that has appeared below the other buttons to confirm this change.\nYou will see a message at the top saying “Updated transcript types”.\n\n\nNow you have an empty database for which you’ve:\n\ncreated two corpora, QB and UC, and\ncreated a new transcript type, so that we can have monologues as well as interviews."
  },
  {
    "objectID": "worksheets/express-tutorial/index.html",
    "href": "worksheets/express-tutorial/index.html",
    "title": "LaBB-CAT Express Tutorial",
    "section": "",
    "text": "LaBB-CAT Express Tutorial\nThis tutorial is a very brief introduction to the LaBB-CAT corpus analysis tool. There are four exercises, in which you:\n\ninstall and configure LaBB-CAT,\nupload a small corpus of recordings into your database and explore the transcript page,\nsearch for some tokens using regular expressions and,\ninstall and configure a module for automatically annotating tokens with their phonemic transcriptions, and search for tokens based on pronunciation instead of spelling.\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  },
  {
    "objectID": "worksheets/express-tutorial/2-upload-data.html",
    "href": "worksheets/express-tutorial/2-upload-data.html",
    "title": "2 - Upload Data",
    "section": "",
    "text": "Transcripts can be uploaded manually, one at a time, using the upload transcripts option in the upload menu.\nHowever, if you already have a collection of transcripts and media files (which we have for these exercises - download QuakeStories.zip to get the workshop data), and they are systematically organised (which they are), you may be able to save some manual uploading work by uploading them using the ‘batch upload’ utility.\n\nIn LaBB-CAT, select the upload option on the menu.\nSelect the upload transcript batch link.\nThis shows a window with a large blank area in the middle with various buttons above it.\nOpen Windows Explorer or Finder, and navigate to the LaBB-CAT Workshop data folder.\nDrag the folder called “QuakeStories”, and drop it on to LaBB-CAT, on to the blank area below the buttons.\nThe previously blank area will contain a list of transcripts. Each transcript should have a value filled in for each column - Transcript, Media, Corpus, and Episode.\nMost of the transcripts are monologues, so set Type to monologue on the top left.\nClick the Upload button above the list.\nYou will see that in the Status column, the text changes to “Uploading…” for the first transcript. The progress bar progresses, and once it’s complete, the next transcript changes to “Transferring”, and so on.\nWhile the files are uploading, click  the online help link next to the upload transcript batch link you clicked above and read the conditions that must be met in order to use the batch uploader.\nOnce the uploader is finished, you can verify that all the transcripts are there by selecting the transcripts option on the menu in LaBB-CAT.\nYou should see a list of twenty transcripts.\nUse the “Transcript” box to find UC013AM_Dom.eaf\n(You can type just part of the name if you like)\nSelect the Attributes icon for UC013AM_Dom.eaf\n(the one with the spanner/wrench icon 🔧 on the right).\nChange type to interview and press Save.\nSimilarly, the following transcripts are interviews, so change their type accordingly\n\nUC215YW_DanielaMaoate-Cox.eaf\nUC226AD.eaf\n\nThe heading at the top of the transcript attributes page, which is the name of the transcript, is a link. Click the link.\n\nYou will now see LaBB-CAT’s ‘interactive transcript’ page for the transcript.\nAt the top there is a heading, a list of speakers, and then below this, the lines from the transcript, their speakers in the margin. This includes the words the participants utter, and also any noises, comments, and other annotations that were put in the transcript in ELAN.\n\nIn the top right corner are some playback controls; click the play button. You will see a shaded rectangle following the participant’s speech.\nTry the other controls to see what they do.\nNow click on any word in the transcript.\nYou will see a menu appear, with options for the ‘Utterance’ (the line), and the word.\nClick the play option in the menu to see what it does.\nClick on the formats link under the title.\nYou will see a menu, which includes various formats for exporting the transcript.\nSelect Plain Text Document\nSave the resulting file on your desktop, and then open it.\nYou will see the transcript in plain-text form.\nClick the formats link, and select the Praat Text Grid option.\nSave the resulting file on your desktop, and then open it with Praat.\n\nYou will see that the TextGrid has various tiers, two for utterances (one for each speaker), and two for individual words (one for each speaker).\n(You will see that each individual word has a ‘default’ alignment - i.e. the words are evenly spread out during the duration of the line they’re in. It is possible to make these word alignments actually line up with the words in the audio signal, using forced aligment, which is the subject of another tutorial.)\n\n\nYou can also open individual utterances in Praat directly from the transcript page, if you have Praat installed. But first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\n\n\n\n\n\n\n\nTip\n\n\n\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called \"Praat\"). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n There are illustrated instructions for setting up Praat integration for each web browser in the online help for the transcript page; check there if you run into problems.\n\n\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line in the transcript, and select the Open Text Grid in Praat option on the menu.\n\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes a tier for the utterance, and another tier for individual word alignments. You could manually align them here, but it’s much more efficient to use HTK to force-align the utterances. Forced alignment is the subject of another tutorial…\n\n\n\nThe transcripts are now in the database, but the meta-data for the participants hasn’t been set yet (because it’s not contained in the ELAN files). We could manually add this for each speaker, but fortunately we have it stored in a spreadsheet (actually, a CSV text file) that we can upload in one go.\n\nIn LaBB-CAT, select the upload option on the menu.\nSelect the upload participant data option.\nClick Choose File, and select the file in the LaBB-CAT Exercises data folder called “participants.csv”.\nClick Upload\nYou will now see a list of the columns from the spreadsheet.\nFirstly, ensure that the Participant identity column is set to name. This ensures that the “name” column in the spreadsheet will be used to match names of participants in the LaBB-CAT database.\nBelow that is listed each column from the spreadsheet, with an arrow pointing to a drop-down box. The box contains various options, including each of the participant attributes set up in LaBB-CAT, an ignore option, and create a new attribute option.\nSelect the options as follows as follows:\n\n\nThe CSV column name: → ignore because it’s the Participant Identity Column identified above\nThe CSV column gender: → the Gender LaBB-CAT attribute\nThe CSV column age_category: → the create a new attribute called option, and set the Label to “Age”\nThe CSV column ethnicity: → the create a new attribute called option, and set the Label to “Ethnicity”\nThe CSV column grew_up: → the create a new attribute called option, and set the Label to “Country”\nThe CSV column grew_up_region: → the create a new attribute called option, and set the Label to “Region”\nThe CSV column grew_up_town: → the create a new attribute called option, and set the Label to “Town”\nThe CSV column languages_spoken: → the create a new attribute called option, and set the Label to “Languages”\n\nPress import.\nYou should see a page with information about the import, including the columns that were ignored, and the number of participants that were added.\n\nTo check the participant attributes really are now set:\n\nSelect the participants option on the menu.\nYou will see a list of speakers, and page links at the bottom.\nThe page also includes participant attribute values where they are known.\nPick a speaker (e.g. QB702_AnnaSoboleva) and click their name.\nYou will see the participant attributes page with their details filled in (e.g. QB702_AnnaSoboleva is a female English/Russian speaker between 18 and 25 years old).\n\nBy default, the new attributes are not flagged as searchable, so we will make a few of them searchable now.\n\n\nClick the participant attributes link on the menu.\nThis will display a list of the participant meta-data fields.\nEnsure that Searchability is set to Searchable for the following attributes:\n\ngender\nage_category\nlanguages_spoken\n\nPress the Save button at the bottom of the list.\nSelect participants on the top menu.\nYou will see that the searchable attributes are now listed with the participants.\n\nYou can filter the list using the attribute headers at the top of the list.\nUnder Gender select F\nNow the list only shows female participants.\n\n\nYou now have a small database with a number of speakers in it, so we can start doing some searches and creating some annotations."
  },
  {
    "objectID": "worksheets/express-tutorial/2-upload-data.html#uploading-transcripts-and-recordings",
    "href": "worksheets/express-tutorial/2-upload-data.html#uploading-transcripts-and-recordings",
    "title": "2 - Upload Data",
    "section": "",
    "text": "Transcripts can be uploaded manually, one at a time, using the upload transcripts option in the upload menu.\nHowever, if you already have a collection of transcripts and media files (which we have for these exercises - download QuakeStories.zip to get the workshop data), and they are systematically organised (which they are), you may be able to save some manual uploading work by uploading them using the ‘batch upload’ utility.\n\nIn LaBB-CAT, select the upload option on the menu.\nSelect the upload transcript batch link.\nThis shows a window with a large blank area in the middle with various buttons above it.\nOpen Windows Explorer or Finder, and navigate to the LaBB-CAT Workshop data folder.\nDrag the folder called “QuakeStories”, and drop it on to LaBB-CAT, on to the blank area below the buttons.\nThe previously blank area will contain a list of transcripts. Each transcript should have a value filled in for each column - Transcript, Media, Corpus, and Episode.\nMost of the transcripts are monologues, so set Type to monologue on the top left.\nClick the Upload button above the list.\nYou will see that in the Status column, the text changes to “Uploading…” for the first transcript. The progress bar progresses, and once it’s complete, the next transcript changes to “Transferring”, and so on.\nWhile the files are uploading, click  the online help link next to the upload transcript batch link you clicked above and read the conditions that must be met in order to use the batch uploader.\nOnce the uploader is finished, you can verify that all the transcripts are there by selecting the transcripts option on the menu in LaBB-CAT.\nYou should see a list of twenty transcripts.\nUse the “Transcript” box to find UC013AM_Dom.eaf\n(You can type just part of the name if you like)\nSelect the Attributes icon for UC013AM_Dom.eaf\n(the one with the spanner/wrench icon 🔧 on the right).\nChange type to interview and press Save.\nSimilarly, the following transcripts are interviews, so change their type accordingly\n\nUC215YW_DanielaMaoate-Cox.eaf\nUC226AD.eaf\n\nThe heading at the top of the transcript attributes page, which is the name of the transcript, is a link. Click the link.\n\nYou will now see LaBB-CAT’s ‘interactive transcript’ page for the transcript.\nAt the top there is a heading, a list of speakers, and then below this, the lines from the transcript, their speakers in the margin. This includes the words the participants utter, and also any noises, comments, and other annotations that were put in the transcript in ELAN.\n\nIn the top right corner are some playback controls; click the play button. You will see a shaded rectangle following the participant’s speech.\nTry the other controls to see what they do.\nNow click on any word in the transcript.\nYou will see a menu appear, with options for the ‘Utterance’ (the line), and the word.\nClick the play option in the menu to see what it does.\nClick on the formats link under the title.\nYou will see a menu, which includes various formats for exporting the transcript.\nSelect Plain Text Document\nSave the resulting file on your desktop, and then open it.\nYou will see the transcript in plain-text form.\nClick the formats link, and select the Praat Text Grid option.\nSave the resulting file on your desktop, and then open it with Praat.\n\nYou will see that the TextGrid has various tiers, two for utterances (one for each speaker), and two for individual words (one for each speaker).\n(You will see that each individual word has a ‘default’ alignment - i.e. the words are evenly spread out during the duration of the line they’re in. It is possible to make these word alignments actually line up with the words in the audio signal, using forced aligment, which is the subject of another tutorial.)\n\n\nYou can also open individual utterances in Praat directly from the transcript page, if you have Praat installed. But first, the LaBB-CAT/Praat integration has to be set up; this only has to be done once:\n\nOn the top-right of the page, above the playback controls, there’s a Praat icon - click it.\nFollow the instructions that appear (these vary depending on what web browser you use).\n\n\n\n\n\n\n\nTip\n\n\n\nYou may need to grant a browser extension permission to install, and it’s possible you will need a connection to the internet in order to download this extension.\nYou also may be asked where Praat is installed; Navigate to the location where Praat is installed, and double-click the “Praat.exe” file (on some systems the file may simply be called \"Praat\"). The Praat program may open, and then immediately close, as LaBB-CAT tests it can communicate with Praat.\n There are illustrated instructions for setting up Praat integration for each web browser in the online help for the transcript page; check there if you run into problems.\n\n\n\nNow Praat integration has been set up, and you should be able to access Praat options in the transcript page from now on…\n\nClick on a line in the transcript, and select the Open Text Grid in Praat option on the menu.\n\nPraat should open, and show you a spectrogram of the line’s audio, with a TextGrid below that includes a tier for the utterance, and another tier for individual word alignments. You could manually align them here, but it’s much more efficient to use HTK to force-align the utterances. Forced alignment is the subject of another tutorial…"
  },
  {
    "objectID": "worksheets/express-tutorial/2-upload-data.html#participant-data-import",
    "href": "worksheets/express-tutorial/2-upload-data.html#participant-data-import",
    "title": "2 - Upload Data",
    "section": "",
    "text": "The transcripts are now in the database, but the meta-data for the participants hasn’t been set yet (because it’s not contained in the ELAN files). We could manually add this for each speaker, but fortunately we have it stored in a spreadsheet (actually, a CSV text file) that we can upload in one go.\n\nIn LaBB-CAT, select the upload option on the menu.\nSelect the upload participant data option.\nClick Choose File, and select the file in the LaBB-CAT Exercises data folder called “participants.csv”.\nClick Upload\nYou will now see a list of the columns from the spreadsheet.\nFirstly, ensure that the Participant identity column is set to name. This ensures that the “name” column in the spreadsheet will be used to match names of participants in the LaBB-CAT database.\nBelow that is listed each column from the spreadsheet, with an arrow pointing to a drop-down box. The box contains various options, including each of the participant attributes set up in LaBB-CAT, an ignore option, and create a new attribute option.\nSelect the options as follows as follows:\n\n\nThe CSV column name: → ignore because it’s the Participant Identity Column identified above\nThe CSV column gender: → the Gender LaBB-CAT attribute\nThe CSV column age_category: → the create a new attribute called option, and set the Label to “Age”\nThe CSV column ethnicity: → the create a new attribute called option, and set the Label to “Ethnicity”\nThe CSV column grew_up: → the create a new attribute called option, and set the Label to “Country”\nThe CSV column grew_up_region: → the create a new attribute called option, and set the Label to “Region”\nThe CSV column grew_up_town: → the create a new attribute called option, and set the Label to “Town”\nThe CSV column languages_spoken: → the create a new attribute called option, and set the Label to “Languages”\n\nPress import.\nYou should see a page with information about the import, including the columns that were ignored, and the number of participants that were added.\n\nTo check the participant attributes really are now set:\n\nSelect the participants option on the menu.\nYou will see a list of speakers, and page links at the bottom.\nThe page also includes participant attribute values where they are known.\nPick a speaker (e.g. QB702_AnnaSoboleva) and click their name.\nYou will see the participant attributes page with their details filled in (e.g. QB702_AnnaSoboleva is a female English/Russian speaker between 18 and 25 years old).\n\nBy default, the new attributes are not flagged as searchable, so we will make a few of them searchable now.\n\n\nClick the participant attributes link on the menu.\nThis will display a list of the participant meta-data fields.\nEnsure that Searchability is set to Searchable for the following attributes:\n\ngender\nage_category\nlanguages_spoken\n\nPress the Save button at the bottom of the list.\nSelect participants on the top menu.\nYou will see that the searchable attributes are now listed with the participants.\n\nYou can filter the list using the attribute headers at the top of the list.\nUnder Gender select F\nNow the list only shows female participants.\n\n\nYou now have a small database with a number of speakers in it, so we can start doing some searches and creating some annotations."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\n\nReusehttps://creativecommons.org/licenses/by-sa/4.0/Copyright© 2023 NZILBB"
  }
]